
@article{karniadakis_physics-informed_2021,
	title = {Physics-informed machine learning},
	volume = {3},
	copyright = {2021 Springer Nature Limited},
	issn = {2522-5820},
	url = {https://www.nature.com/articles/s42254-021-00314-5},
	doi = {10.1038/s42254-021-00314-5},
	abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
	language = {en},
	number = {6},
	urldate = {2022-03-17},
	journal = {Nat. Rev. Phys.},
	author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
	month = jun,
	year = {2021},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Applied mathematics, Computational science},
	pages = {422--440},
	file = {Snapshot:/home/hell/Zotero/storage/IG6I2J2Q/s42254-021-00314-5.html:text/html;Snapshot:/home/hell/Zotero/storage/IK8N6IMR/s42254-021-00314-5.html:text/html},
}

@article{cuomo_scientific_2022,
	title = {Scientific {Machine} {Learning} through {Physics}-{Informed} {Neural} {Networks}: {Where} we are and {What}'s next},
	shorttitle = {Scientific {Machine} {Learning} through {Physics}-{Informed} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2201.05624},
	abstract = {Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, and integral-differential equations. This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages, the review also attempts to incorporate publications on a larger variety of issues, including physics-constrained neural networks (PCNN), where the initial or boundary conditions are directly embedded in the NN structure rather than in the loss functions. The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.},
	urldate = {2022-03-17},
	journal = {arXiv:2201.05624 [physics]},
	author = {Cuomo, Salvatore and di Cola, Vincenzo Schiano and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco},
	month = feb,
	year = {2022},
	note = {arXiv: 2201.05624},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Physics - Data Analysis, Statistics and Probability, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv Fulltext PDF:/home/hell/Zotero/storage/47ACBNAN/Cuomo et al. - 2022 - Scientific Machine Learning through Physics-Inform.pdf:application/pdf;arXiv.org Snapshot:/home/hell/Zotero/storage/E2YFJWNC/2201.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/RT3LL5LS/2201.html:text/html},
}

@article{raissi_numerical_2018,
	title = {Numerical {Gaussian} {Processes} for {Time}-{Dependent} and {Nonlinear} {Partial} {Differential} {Equations}},
	copyright = {© 2018, Society for Industrial and Applied Mathematics},
	url = {https://epubs.siam.org/doi/abs/10.1137/17M1120762},
	doi = {10.1137/17M1120762},
	abstract = {We introduce the concept of numerical Gaussian processes, which we define as Gaussian processes with covariance functions resulting from temporal discretization of time-dependent partial differential equations. Numerical Gaussian processes, by construction, are designed to deal with cases where (a) all we observe are noisy data on black-box initial conditions, and (b) we are interested in quantifying the uncertainty associated with such noisy data in our solutions to time-dependent partial differential equations. Our method circumvents the need for spatial discretization of the differential operators by proper placement of Gaussian process priors. This is an attempt to construct structured and data-efficient learning machines, which are explicitly informed by the underlying physics that possibly generated the observed data. The effectiveness of the proposed approach is demonstrated through several benchmark problems involving linear and nonlinear time-dependent operators. In all examples, we are able to recover accurate approximations of the latent solutions, and consistently propagate uncertainty, even in cases involving very long time integration.},
	language = {en},
	urldate = {2022-03-17},
	journal = {SIAM J. Sci. Comput.},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = jan,
	year = {2018},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	file = {Snapshot:/home/hell/Zotero/storage/EE4NX8I2/17M1120762.html:text/html;Snapshot:/home/hell/Zotero/storage/XHGP38Q7/17M1120762.html:text/html;Submitted Version:/home/hell/Zotero/storage/LSHVJTFZ/Raissi et al. - 2018 - Numerical Gaussian Processes for Time-Dependent an.pdf:application/pdf},
}

@article{raissi_deep_2019,
	title = {Deep learning of vortex-induced vibrations},
	volume = {861},
	issn = {0022-1120, 1469-7645},
	url = {https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/deep-learning-of-vortexinduced-vibrations/B7D9B152C42B7F1E895C1661F5A85881},
	doi = {10.1017/jfm.2018.872},
	abstract = {Vortex-induced vibrations of bluff bodies occur when the vortex shedding frequency is close to the natural frequency of the structure. Of interest is the prediction of the lift and drag forces on the structure given some limited and scattered information on the velocity field. This is an inverse problem that is not straightforward to solve using standard computational fluid dynamics methods, especially since no information is provided for the pressure. An even greater challenge is to infer the lift and drag forces given some dye or smoke visualizations of the flow field. Here we employ deep neural networks that are extended to encode the incompressible Navier–Stokes equations coupled with the structure’s dynamic motion equation. In the first case, given scattered data in space–time on the velocity field and the structure’s motion, we use four coupled deep neural networks to infer very accurately the structural parameters, the entire time-dependent pressure field (with no prior training data), and reconstruct the velocity vector field and the structure’s dynamic motion. In the second case, given scattered data in space–time on a concentration field only, we use five coupled deep neural networks to infer very accurately the vector velocity field and all other quantities of interest as before. This new paradigm of inference in fluid mechanics for coupled multi-physics problems enables velocity and pressure quantification from flow snapshots in small subdomains and can be exploited for flow control applications and also for system identification.},
	language = {en},
	urldate = {2022-03-17},
	journal = {J. Fluid Mech.},
	author = {Raissi, Maziar and Wang, Zhicheng and Triantafyllou, Michael S. and Karniadakis, George Em},
	month = feb,
	year = {2019},
	note = {Publisher: Cambridge University Press},
	keywords = {computational methods, flow–structure interactions, vortex interactions},
	pages = {119--137},
	file = {Full Text PDF:/home/hell/Zotero/storage/T8E622CZ/Raissi et al. - 2019 - Deep learning of vortex-induced vibrations.pdf:application/pdf},
}

@article{cai_physics-informed_2022,
	title = {Physics-informed neural networks ({PINNs}) for fluid mechanics: a review},
	issn = {1614-3116},
	shorttitle = {Physics-informed neural networks ({PINNs}) for fluid mechanics},
	url = {https://doi.org/10.1007/s10409-021-01148-1},
	doi = {10.1007/s10409-021-01148-1},
	abstract = {Despite the significant progress over the last 50 years in simulating flow problems using numerical discretization of the Navier–Stokes equations (NSE), we still cannot incorporate seamlessly noisy data into existing algorithms, mesh-generation is complex, and we cannot tackle high-dimensional problems governed by parametrized NSE. Moreover, solving inverse flow problems is often prohibitively expensive and requires complex and expensive formulations and new computer codes. Here, we review flow physics-informed learning, integrating seamlessly data and mathematical models, and implement them using physics-informed neural networks (PINNs). We demonstrate the effectiveness of PINNs for inverse problems related to three-dimensional wake flows, supersonic flows, and biomedical flows.},
	language = {en},
	urldate = {2022-03-17},
	journal = {Acta Mech. Sin.},
	author = {Cai, Shengze and Mao, Zhiping and Wang, Zhicheng and Yin, Minglang and Karniadakis, George Em},
	month = jan,
	year = {2022},
	file = {Springer Full Text PDF:/home/hell/Zotero/storage/E5TH8JRA/Cai et al. - 2022 - Physics-informed neural networks (PINNs) for fluid.pdf:application/pdf},
}

@article{haghighat_physics-informed_2021,
	title = {A physics-informed deep learning framework for inversion and surrogate modeling in solid mechanics},
	volume = {379},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782521000773},
	doi = {10.1016/j.cma.2021.113741},
	abstract = {We present the application of a class of deep learning, known as Physics Informed Neural Networks (PINN), to inversion and surrogate modeling in solid mechanics. We explain how to incorporate the momentum balance and constitutive relations into PINN, and explore in detail the application to linear elasticity, and illustrate its extension to nonlinear problems through an example that showcases von Mises elastoplasticity. While common PINN algorithms are based on training one deep neural network (DNN), we propose a multi-network model that results in more accurate representation of the field variables. To validate the model, we test the framework on synthetic data generated from analytical and numerical reference solutions. We study convergence of the PINN model, and show that Isogeometric Analysis (IGA) results in superior accuracy and convergence characteristics compared with classic low-order Finite Element Method (FEM). We also show the applicability of the framework for transfer learning, and find vastly accelerated convergence during network re-training. Finally, we find that honoring the physics leads to improved robustness: when trained only on a few parameters, we find that the PINN model can accurately predict the solution for a wide range of parameters new to the network—thus pointing to an important application of this framework to sensitivity analysis and surrogate modeling.},
	language = {en},
	urldate = {2022-03-17},
	journal = {Comput. Methods Appl. Mech. Eng.},
	author = {Haghighat, Ehsan and Raissi, Maziar and Moure, Adrian and Gomez, Hector and Juanes, Ruben},
	month = jun,
	year = {2021},
	keywords = {Artificial neural network, Elastoplasticity, Inversion, Linear elasticity, Physics-informed deep learning, Transfer learning},
	pages = {113741},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/HQBHSAF7/S0045782521000773.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/KX6HVWUQ/S0045782521000773.html:text/html},
}

@misc{noauthor_idrl-lab_nodate,
	title = {idrl-lab {PINNpapers}},
	url = {https://github.com/idrl-lab/PINNpapers},
}

@incollection{noauthor_notitle_nodate,
}

@article{chen_theory-guided_2021,
	title = {Theory-guided hard constraint projection ({HCP}): {A} knowledge-based data-driven scientific machine learning method},
	volume = {445},
	issn = {0021-9991},
	shorttitle = {Theory-guided hard constraint projection ({HCP})},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999121005192},
	doi = {10.1016/j.jcp.2021.110624},
	abstract = {Machine learning models have been successfully used in many scientific and engineering fields. However, it remains difficult for a model to simultaneously utilize domain knowledge and experimental observation data. The application of knowledge-based symbolic artificial intelligence (AI) represented by expert systems is limited by the expressive ability of the model, and data-driven connectionism AI represented by neural networks is prone to produce predictions that might violate physical principles. In order to fully integrate domain knowledge with observations and make full use of the strong fitting ability of neural networks, this study proposes theory-guided hard constraint projection (HCP). This deep learning model converts physical constraints, such as governing equations, into a form that is easy to handle through discretization, and then implements hard constraint optimization through projection in a patch. Based on rigorous mathematical proofs, theory-guided HCP can ensure that model predictions strictly conform to physical mechanisms in the constraint patch. The training process of theory-guided HCP only needs a small amount of labeled data (sparse observation), and it can supervise the model by combining the coordinates (label-free data) with domain knowledge. The performance of the theory-guided HCP is verified by experiments based on a published heterogeneous subsurface flow problem. The experiments show that theory-guided HCP requires fewer data, and achieves higher prediction accuracy and stronger robustness to noisy observations, than the fully connected neural networks and soft constraint models. Furthermore, due to the application of domain knowledge, theory-guided HCP possesses the ability to extrapolate and can accurately predict points outside of the range of the training dataset.},
	language = {en},
	urldate = {2022-03-17},
	journal = {J. Comput. Phys.},
	author = {Chen, Yuntian and Huang, Dou and Zhang, Dongxiao and Zeng, Junsheng and Wang, Nanzhe and Zhang, Haoran and Yan, Jinyue},
	month = nov,
	year = {2021},
	keywords = {Constraint patch, Hard constraint, Physics informed, Projection, Sparse observation, Theory guided},
	pages = {110624},
}

@article{chiu_can-pinn_2021,
	title = {{CAN}-{PINN}: {A} {Fast} {Physics}-{Informed} {Neural} {Network} {Based} on {Coupled}-{Automatic}-{Numerical} {Differentiation} {Method}},
	shorttitle = {{CAN}-{PINN}},
	url = {http://arxiv.org/abs/2110.15832},
	abstract = {In this study, novel physics-informed neural network (PINN) methods for coupling neighboring support points and automatic differentiation (AD) through Taylor series expansion are proposed to allow efficient training with improved accuracy. The computation of differential operators required for PINNs loss evaluation at collocation points are conventionally obtained via AD. Although AD has the advantage of being able to compute the exact gradients at any point, such PINNs can only achieve high accuracies with large numbers of collocation points, otherwise they are prone to optimizing towards unphysical solution. To make PINN training fast, the dual ideas of using numerical differentiation (ND)-inspired method and coupling it with AD are employed to define the loss function. The ND-based formulation for training loss can strongly link neighboring collocation points to enable efficient training in sparse sample regimes, but its accuracy is restricted by the interpolation scheme. The proposed coupled-automatic-numerical differentiation framework, labeled as can-PINN, unifies the advantages of AD and ND, providing more robust and efficient training than AD-based PINNs, while further improving accuracy by up to 1-2 orders of magnitude relative to ND-based PINNs. For a proof-of-concept demonstration of this can-scheme to fluid dynamic problems, two numerical-inspired instantiations of can-PINN schemes for the convection and pressure gradient terms were derived to solve the incompressible Navier-Stokes (N-S) equations. The superior performance of can-PINNs is demonstrated on several challenging problems, including the flow mixing phenomena, lid driven flow in a cavity, and channel flow over a backward facing step. The results reveal that for challenging problems like these, can-PINNs can consistently achieve very good accuracy whereas conventional AD-based PINNs fail.},
	urldate = {2022-03-17},
	journal = {arXiv:2110.15832 [physics]},
	author = {Chiu, Pao-Hsiung and Wong, Jian Cheng and Ooi, Chinchun and Dao, My Ha and Ong, Yew-Soon},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.15832},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Computer Science - Computational Engineering, Finance, and Science, Physics - Computational Physics, Physics - Fluid Dynamics, and Science, Computer Science - Computational Engineering, Finance},
}

@article{xiang_hybrid_2022,
	title = {Hybrid {Finite} {Difference} with the {Physics}-informed {Neural} {Network} for solving {PDE} in complex geometries},
	url = {http://arxiv.org/abs/2202.07926},
	abstract = {The physics-informed neural network (PINN) is effective in solving the partial differential equation (PDE) by capturing the physics constraints as a part of the training loss function through the Automatic Differentiation (AD). This study proposes the hybrid finite difference with the physics-informed neural network (HFD-PINN) to fully use the domain knowledge. The main idea is to use the finite difference method (FDM) locally instead of AD in the framework of PINN. In particular, we use AD at complex boundaries and the FDM in other domains. The hybrid learning model shows promising results in experiments. To use the FDM locally in the complex boundary domain and avoid the generation of background mesh, we propose the HFD-PINN-sdf method, which locally uses the finite difference scheme at random points. In addition, the signed distance function is used to avoid the difference scheme from crossing the domain boundary. In this paper, we demonstrate the performance of our proposed methods and compare the results with the different number of collocation points for the Poisson equation, Burgers equation. We also chose several different finite difference schemes, including the compact finite difference method (CDM) and crank-nicolson method (CNM), to verify the robustness of HFD-PINN. We take the heat conduction problem and the heat transfer problem on the irregular domain as examples to demonstrate the efficacy of our framework. In summary, HFD-PINN, especially HFD-PINN-sdf, are more instructive and efficient, significantly when solving PDEs in complex geometries.},
	urldate = {2022-03-17},
	journal = {arXiv:2202.07926 [physics]},
	author = {Xiang, Zixue and Peng, Wei and Zhou, Weien and Yao, Wen},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.07926},
	keywords = {Physics - Computational Physics},
}

@article{ramabathiran_spinn_2021,
	title = {{SPINN}: {Sparse}, {Physics}-based, and partially {Interpretable} {Neural} {Networks} for {PDEs}},
	volume = {445},
	issn = {0021-9991},
	shorttitle = {{SPINN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999121004952},
	doi = {10.1016/j.jcp.2021.110600},
	abstract = {We introduce a class of Sparse, Physics-based, and partially Interpretable Neural Networks (SPINN) for solving ordinary and partial differential equations (PDEs). By reinterpreting a traditional meshless representation of solutions of PDEs we develop a class of sparse neural network architectures that are partially interpretable. The SPINN model we propose here serves as a seamless bridge between two extreme modeling tools for PDEs, namely dense neural network based methods like Physics Informed Neural Networks (PINNs) and traditional mesh-free numerical methods, thereby providing a novel means to develop a new class of hybrid algorithms that build on the best of both these viewpoints. A unique feature of the SPINN model that distinguishes it from other neural network based approximations proposed earlier is that it is (i) interpretable, in a particular sense made precise in the work, and (ii) sparse in the sense that it has much fewer connections than typical dense neural networks used for PDEs. Further, the SPINN algorithm implicitly encodes mesh adaptivity and is able to handle discontinuities in the solutions. In addition, we demonstrate that Fourier series representations can also be expressed as a special class of SPINN and propose generalized neural network analogues of Fourier representations. We illustrate the utility of the proposed method with a variety of examples involving ordinary differential equations, elliptic, parabolic, hyperbolic and nonlinear partial differential equations, and an example in fluid dynamics.},
	language = {en},
	urldate = {2022-03-17},
	journal = {J. Comput. Phys.},
	author = {Ramabathiran, Amuthan A. and Ramachandran, Prabhu},
	month = nov,
	year = {2021},
	keywords = {Interpretable machine learning, Meshless methods, Numerical methods for PDEs, Partial differential equations, Physics-based neural networks, Sparse neural networks},
	pages = {110600},
}

@inproceedings{shi_non-fourier_2021,
	title = {Non-{Fourier} {Heat} {Conduction} based on {Self}-{Adaptive} {Weight} {Physics}-{Informed} {Neural} {Networks}},
	doi = {10.23919/CCC52363.2021.9550487},
	abstract = {This paper establishes the non-Fourier heat conduction model to describe the heat transfer process of mono-crystalline silicon under the condition of unstable thermal field and thermal shock in the Czochralski method. A novel differential equations solver called Physics-Informed Neural Networks (PINN) algorithm was introduced. Compared with finite element method (FEM), this method has some advantages like no grid requirement and easily solving. In order to deal with the unbalance of constraint condition and speed up the convergence, we propose a novel method called Self-Adaptive Weight Physics-Informed Neural Networks(SWPINN). The comparison of the experimental results of SWPINN and COMSOL verifies the effectiveness of SWPINN. By modifying the parameter of non-Fourier heat conduction model, this paper obtains the temperature distribution under different heat relaxation times. Finally, comparison between SWPINN and PINN shows that the proposed method has faster convergence speed and higher accuracy.},
	booktitle = {2021 40th Chin. Control Conf. (CCC)},
	author = {Shi, Shuyan and Liu, Ding and Zhao, Zhongdan},
	month = jul,
	year = {2021},
	note = {ISSN: 1934-1768},
	keywords = {Czochralski Method, Finite element analysis, Heating systems, Monocrystalline Silicon, Neural networks, Non-Fourier Heat Conduction, Partial Differential Equation, Physics-Informed Neural Networks, Silicon, Software, Temperature distribution, Thermal shock},
	pages = {8451--8456},
}

@article{zhao_solving_2020,
	title = {Solving {Allen}-{Cahn} and {Cahn}-{Hilliard} {Equations} using the {Adaptive} {Physics} {Informed} {Neural} {Networks}},
	volume = {29},
	url = {https://par.nsf.gov/biblio/10225320-solving-allen-cahn-cahn-hilliard-equations-using-adaptive-physics-informed-neural-networks},
	doi = {10.4208/cicp.OA-2020-0086},
	language = {en},
	number = {3},
	urldate = {2022-03-17},
	journal = {Comm. Comput. Phys.},
	author = {Zhao, Colby L.},
	month = jul,
	year = {2020},
}

@misc{noauthor_modulus_2021,
	title = {Modulus {User} {Guide}},
	url = {https://developer.nvidia.com/modulus-user-guide-v2106},
	publisher = {NVIDIA},
	month = nov,
	year = {2021},
	note = {release v21.06},
}

@inproceedings{hennigh_nvidia_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{NVIDIA} {SimNet}™: {An} {AI}-{Accelerated} {Multi}-{Physics} {Simulation} {Framework}},
	isbn = {978-3-030-77977-1},
	shorttitle = {{NVIDIA} {SimNet}™},
	doi = {10.1007/978-3-030-77977-1_36},
	abstract = {We present SimNet, an AI-driven multi-physics simulation framework, to accelerate simulations across a wide range of disciplines in science and engineering. Compared to traditional numerical solvers, SimNet addresses a wide range of use cases - coupled forward simulations without any training data, inverse and data assimilation problems. SimNet offers fast turnaround time by enabling parameterized system representation that solves for multiple configurations simultaneously, as opposed to the traditional solvers that solve for one configuration at a time. SimNet is integrated with parameterized constructive solid geometry as well as STL modules to generate point clouds. Furthermore, it is customizable with APIs that enable user extensions to geometry, physics and network architecture. It has advanced network architectures that are optimized for high-performance GPU computing, and offers scalable performance for multi-GPU and multi-Node implementation with accelerated linear algebra as well as FP32, FP64 and TF32 computations. In this paper we review the neural network solver methodology, the SimNet architecture, and the various features that are needed for effective solution of the PDEs. We present real-world use cases that range from challenging forward multi-physics simulations with turbulence and complex 3D geometries, to industrial design optimization and inverse problems that are not addressed efficiently by the traditional solvers. Extensive comparisons of SimNet results with open source and commercial solvers show good correlation. The SimNet source code is available at https://developer.nvidia.com/simnet.},
	language = {en},
	booktitle = {ICCS},
	publisher = {Springer Int. Pub.},
	author = {Hennigh, Oliver and Narasimhan, Susheela and Nabian, Mohammad Amin and Subramaniam, Akshay and Tangsali, Kaustubh and Fang, Zhiwei and Rietmann, Max and Byeon, Wonmin and Choudhry, Sanjay},
	editor = {Paszynski, Maciej and Kranzlmüller, Dieter and Krzhizhanovskaya, Valeria V. and Dongarra, Jack J. and Sloot, Peter M.A.},
	year = {2021},
	pages = {447--461},
}

@article{wang_when_2022,
	title = {When and why {PINNs} fail to train: {A} neural tangent kernel perspective},
	volume = {449},
	issn = {0021-9991},
	shorttitle = {When and why {PINNs} fail to train},
	url = {https://www.sciencedirect.com/science/article/pii/S002199912100663X},
	doi = {10.1016/j.jcp.2021.110768},
	abstract = {Physics-informed neural networks (PINNs) have lately received great attention thanks to their flexibility in tackling a wide range of forward and inverse problems involving partial differential equations. However, despite their noticeable empirical success, little is known about how such constrained neural networks behave during their training via gradient descent. More importantly, even less is known about why such models sometimes fail to train at all. In this work, we aim to investigate these questions through the lens of the Neural Tangent Kernel (NTK); a kernel that captures the behavior of fully-connected neural networks in the infinite width limit during training via gradient descent. Specifically, we derive the NTK of PINNs and prove that, under appropriate conditions, it converges to a deterministic kernel that stays constant during training in the infinite-width limit. This allows us to analyze the training dynamics of PINNs through the lens of their limiting NTK and find a remarkable discrepancy in the convergence rate of the different loss components contributing to the total training error. To address this fundamental pathology, we propose a novel gradient descent algorithm that utilizes the eigenvalues of the NTK to adaptively calibrate the convergence rate of the total training error. Finally, we perform a series of numerical experiments to verify the correctness of our theory and the practical effectiveness of the proposed algorithms. The data and code accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/PINNsNTK.},
	language = {en},
	urldate = {2022-03-17},
	journal = {J. Comput. Phys.},
	author = {Wang, Sifan and Yu, Xinling and Perdikaris, Paris},
	month = jan,
	year = {2022},
	keywords = {Gradient descent, Multi-task learning, Physics-informed neural networks, Scientific machine learning, Spectral bias},
	pages = {110768},
}

@article{yu_gradient-enhanced_2021,
	title = {Gradient-enhanced physics-informed neural networks for forward and inverse {PDE} problems},
	url = {http://arxiv.org/abs/2111.02801},
	abstract = {Deep learning has been shown to be an effective tool in solving partial differential equations (PDEs) through physics-informed neural networks (PINNs). PINNs embed the PDE residual into the loss function of the neural network, and have been successfully employed to solve diverse forward and inverse PDE problems. However, one disadvantage of the first generation of PINNs is that they usually have limited accuracy even with many training points. Here, we propose a new method, gradient-enhanced physics-informed neural networks (gPINNs), for improving the accuracy and training efficiency of PINNs. gPINNs leverage gradient information of the PDE residual and embed the gradient into the loss function. We tested gPINNs extensively and demonstrated the effectiveness of gPINNs in both forward and inverse PDE problems. Our numerical results show that gPINN performs better than PINN with fewer training points. Furthermore, we combined gPINN with the method of residual-based adaptive refinement (RAR), a method for improving the distribution of training points adaptively during training, to further improve the performance of gPINN, especially in PDEs with solutions that have steep gradients.},
	urldate = {2022-03-17},
	journal = {arXiv:2111.02801 [physics]},
	author = {Yu, Jeremy and Lu, Lu and Meng, Xuhui and Karniadakis, George Em},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.02801},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Partial differential equations, Physics-informed neural networks, Deep learning, Gradient-enhanced, Residual-based adaptive refinement},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/4G4DQPX5/S0045782522001438.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/EJZC2ZGR/S0045782522001438.html:text/html},
}

@article{lu_deepxde_2021,
	title = {{DeepXDE}: {A} {Deep} {Learning} {Library} for {Solving} {Differential} {Equations}},
	volume = {63},
	issn = {0036-1445},
	shorttitle = {{DeepXDE}},
	url = {https://epubs.siam.org/doi/10.1137/19M1274067},
	doi = {10.1137/19M1274067},
	abstract = {Deep learning has achieved remarkable success in diverse applications; however, its use in solving partial differential equations (PDEs) has emerged only recently. Here, we present an overview of physics-informed neural networks (PINNs), which embed a PDE into the loss of the neural network using automatic differentiation. The PINN algorithm is simple, and it can be applied to different types of PDEs, including integro-differential equations, fractional PDEs, and stochastic PDEs. Moreover, from an implementation point of view, PINNs solve inverse problems as easily as forward problems. We propose a new residual-based adaptive refinement (RAR) method to improve the training efficiency of PINNs. For pedagogical reasons, we compare the PINN algorithm to a standard finite element method. We also present a Python library for PINNs, DeepXDE, which is designed to serve both as an educational tool to be used in the classroom as well as a research tool for solving problems in computational science and engineering. Specifically, DeepXDE can solve forward problems given initial and boundary conditions, as well as inverse problems given some extra measurements. DeepXDE supports complex-geometry domains based on the technique of constructive solid geometry and enables the user code to be compact, resembling closely the mathematical formulation. We introduce the usage of DeepXDE and its customizability, and we also demonstrate the capability of PINNs and the user-friendliness of DeepXDE for five different examples. More broadly, DeepXDE contributes to the more rapid development of the emerging scientific machine learning field.},
	number = {1},
	urldate = {2022-03-17},
	journal = {SIAM Rev.},
	author = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
	month = jan,
	year = {2021},
	keywords = {65-01, 65-04, 65L99, 65M99, 65N99, deep learning, DeepXDE, differential equations, education software, physics-informed neural networks, scientific machine learning},
	pages = {208--228},
}

@techreport{mcclenny_self-adaptive_2019,
	title = {Self-{Adaptive} {Physics}-{Informed} {Neural} {Networks} using a {Soft} {Attention} {Mechanism}},
	shorttitle = {Workshop {Report} on {Basic} {Research} {Needs} for {Scientific} {Machine} {Learning}},
	url = {http://ceur-ws.org/Vol-2964/article_68.pdf},
	abstract = {Physics-Informed Neural Networks (PINNs) have emerged recently as a promising application of deep neural networks to the numerical solution of nonlinear partial differential equations (PDEs). However, the solution of more stiff or semi-linear PDEs can contain regions where the gradient and solution changes rapidly, creating difﬁculties in training the solution network. It has been recognized that adaptive procedures are needed to force the neural network to ﬁt accurately these “stubborn” spots in the solution. To accomplish that, previous approaches have used ﬁxed weights in the loss function hard-coded over regions of the solution deemed to be important. In this paper, we propose a new method to train PINNs adaptively, using fully-trainable weights that force the neural network to focus on regions of the solution are difﬁcult, in a way that is reminiscent of soft multiplicative attention masks used in Computer Vision. The key idea in Self-Adaptive PINNs is to make the weights increase as the corresponding losses increase, which is accomplished by training the network to simultaneously minimize the losses and maximize the weights, as in augmented Lagrangian and constraint-satisfaction methods in classical nonlinear optimization. We present numerical experiments with the Allen-Cahn PDE in which the Self-Adaptive PINN outperformed other state-of-the-art PINN algorithms in L2 error, while using a smaller number of training epochs.},
	language = {en},
	number = {68},
	urldate = {2022-01-31},
	institution = {AAAI-MLPS},
	author = {McClenny, Levi and Braga-Neto, Ulisses},
	month = feb,
	year = {2019},
	doi = {10.2172/1478744},
	pages = {1478744},
	file = {Baker et al. - 2019 - Workshop Report on Basic Research Needs for Scient.pdf:/home/hell/Zotero/storage/TFIYDRV2/Baker et al. - 2019 - Workshop Report on Basic Research Needs for Scient.pdf:application/pdf},
}

@article{wang_understanding_2021,
	title = {Understanding and {Mitigating} {Gradient} {Flow} {Pathologies} in {Physics}-{Informed} {Neural} {Networks}},
	volume = {43},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/20M1318043},
	doi = {10.1137/20M1318043},
	abstract = {The widespread use of neural networks across different scientific domains often involves constraining them to satisfy certain symmetries, conservation laws, or other domain knowledge. Such constraints are often imposed as soft penalties during model training and effectively act as domain-specific regularizers of the empirical risk loss. Physics-informed neural networks is an example of this philosophy in which the outputs of deep neural networks are constrained to approximately satisfy a given set of partial differential equations. In this work we review recent advances in scientific machine learning with a specific focus on the effectiveness of physics-informed neural networks in predicting outcomes of physical systems and discovering hidden physics from noisy data. We also identify and analyze a fundamental mode of failure of such approaches that is related to numerical stiffness leading to unbalanced back-propagated gradients during model training. To address this limitation we present a learning rate annealing algorithm that utilizes gradient statistics during model training to balance the interplay between different terms in composite loss functions. We also propose a novel neural network architecture that is more resilient to such gradient pathologies. Taken together, our developments provide new insights into the training of constrained neural networks and consistently improve the predictive accuracy of physics-informed neural networks by a factor of 50--100\${\textbackslash}times\$ across a range of problems in computational physics. All code and data accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/GradientPathologiesPINNs.},
	number = {5},
	urldate = {2022-01-31},
	journal = {SIAM J. Sci. Comput.},
	author = {Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
	month = jan,
	year = {2021},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65M99, deep learning, differential equations, 68T99, 68U20, computational physics, optimization, stiff dynamics},
	pages = {A3055--A3081},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	shorttitle = {Physics-informed neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	language = {en},
	urldate = {2022-01-31},
	journal = {J. Comput. Phys.},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	month = feb,
	year = {2019},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	pages = {686--707},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/62WUGXLX/S0021999118307125.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/JBYV97MM/S0021999118307125.html:text/html},
}

@misc{noauthor_partial_nodate,
	title = {Partial {Differential} {Equation} {Toolbox} ({R2022a})},
	url = {https://uk.mathworks.com/products/pde.html},
	abstract = {Partial Differential Equation Toolbox provides functions for solving partial differential equations (PDEs) in 2D, 3D, and time using finite element analysis.},
	language = {en},
	urldate = {2022-05-27},
	file = {Snapshot:/home/hell/Zotero/storage/NML4EG6V/pde.html:text/html},
}

@misc{noauthor_torchsin_nodate,
	title = {torch.sin — {PyTorch} 1.11.0 documentation},
	url = {https://pytorch.org/docs/stable/generated/torch.sin.html},
	urldate = {2022-06-03},
	file = {torch.sin — PyTorch 1.11.0 documentation:/home/hell/Zotero/storage/XPTIKXZW/torch.sin.html:text/html},
}

@techreport{elfwing_sigmoid-weighted_2017,
	title = {Sigmoid-{Weighted} {Linear} {Units} for {Neural} {Network} {Function} {Approximation} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1702.03118},
	abstract = {In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Tesauro's TD-Gammon achieved near top-level human performance in backgammon, the deep reinforcement learning algorithm DQN achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection with simple annealing can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10\${\textbackslash}times\$10 board, using TD(\${\textbackslash}lambda\$) learning and shallow dSiLU network agents, and, then, by outperforming DQN in the Atari 2600 domain by using a deep Sarsa(\${\textbackslash}lambda\$) agent with SiLU and dSiLU hidden units.},
	number = {arXiv:1702.03118},
	urldate = {2022-06-03},
	institution = {arXiv},
	author = {Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
	month = nov,
	year = {2017},
	doi = {10.48550/arXiv.1702.03118},
	note = {arXiv:1702.03118 [cs]
type: article},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/DXV6MULS/1702.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/9J3T32WC/1702.html:text/html},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-05-27},
	booktitle = {Adv. Neural Inf. Process. Syst.},
	publisher = {Curran Assoc., Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@inproceedings{ledig_photo-realistic_2017,
	title = {Photo-realistic single image super-resolution using a generative adversarial network},
	booktitle = {Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
	author = {Ledig, Christian and Theis, Lucas and Huszár, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan},
	year = {2017},
	pages = {4681--4690},
}

@inproceedings{wang_high-resolution_2018,
	title = {High-resolution image synthesis and semantic manipulation with conditional gans},
	booktitle = {Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
	author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
	year = {2018},
	pages = {8798--8807},
}

@inproceedings{isola_image--image_2017,
	title = {Image-to-image translation with conditional adversarial networks},
	booktitle = {Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	year = {2017},
	pages = {1125--1134},
}

@article{li_physics-informed_2021,
	title = {Physics-informed neural operator for learning partial differential equations},
	journal = {arxiv preprint arxiv:2111.03794},
	author = {Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
	year = {2021},
}

@techreport{guibas_adaptive_2022,
	title = {Adaptive {Fourier} {Neural} {Operators}: {Efficient} {Token} {Mixers} for {Transformers}},
	shorttitle = {Adaptive {Fourier} {Neural} {Operators}},
	url = {http://arxiv.org/abs/2111.13587},
	abstract = {Vision transformers have delivered tremendous success in representation learning. This is primarily due to effective token mixing through self attention. However, this scales quadratically with the number of pixels, which becomes infeasible for high-resolution inputs. To cope with this challenge, we propose Adaptive Fourier Neural Operator (AFNO) as an efficient token mixer that learns to mix in the Fourier domain. AFNO is based on a principled foundation of operator learning which allows us to frame token mixing as a continuous global convolution without any dependence on the input resolution. This principle was previously used to design FNO, which solves global convolution efficiently in the Fourier domain and has shown promise in learning challenging PDEs. To handle challenges in visual representation learning such as discontinuities in images and high resolution inputs, we propose principled architectural modifications to FNO which results in memory and computational efficiency. This includes imposing a block-diagonal structure on the channel mixing weights, adaptively sharing weights across tokens, and sparsifying the frequency modes via soft-thresholding and shrinkage. The resulting model is highly parallel with a quasi-linear complexity and has linear memory in the sequence size. AFNO outperforms self-attention mechanisms for few-shot segmentation in terms of both efficiency and accuracy. For Cityscapes segmentation with the Segformer-B3 backbone, AFNO can handle a sequence size of 65k and outperforms other efficient self-attention mechanisms.},
	number = {arXiv:2111.13587},
	urldate = {2022-05-27},
	institution = {arXiv},
	author = {Guibas, John and Mardani, Morteza and Li, Zongyi and Tao, Andrew and Anandkumar, Anima and Catanzaro, Bryan},
	month = mar,
	year = {2022},
	note = {arXiv:2111.13587 [cs]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/B7CG76A9/2111.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/BPTT36NA/2111.html:text/html},
}

@inproceedings{fathony_multiplicative_2020,
	title = {Multiplicative filter networks},
	booktitle = {ICLR},
	author = {Fathony, Rizal and Sahu, Anit Kumar and Willmott, Devin and Kolter, J. Zico},
	year = {2020},
}

@inproceedings{srivastava_training_2015,
	title = {Training {Very} {Deep} {Networks}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/215a71a12769b056c3c32e7299f1c5ed-Abstract.html},
	abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
	urldate = {2022-05-27},
	booktitle = {Adv. Neural Inf. Process. Syst.},
	publisher = {Curran Assoc., Inc.},
	author = {Srivastava, Rupesh K and Greff, Klaus and Schmidhuber, Jürgen},
	year = {2015},
}

@article{lu_extraction_2020,
	title = {Extraction of mechanical properties of materials through deep learning from instrumented indentation},
	volume = {117},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1922210117},
	doi = {10.1073/pnas.1922210117},
	number = {13},
	urldate = {2022-05-27},
	journal = {Proc. Natl. Acad. Sci.},
	author = {Lu, Lu and Dao, Ming and Kumar, Punit and Ramamurty, Upadrasta and Karniadakis, George Em and Suresh, Subra},
	month = mar,
	year = {2020},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {7052--7062},
}

@article{meng_composite_2020,
	title = {A composite neural network that learns from multi-fidelity data: {Application} to function approximation and inverse {PDE} problems},
	volume = {401},
	issn = {0021-9991},
	shorttitle = {A composite neural network that learns from multi-fidelity data},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999119307260},
	doi = {10.1016/j.jcp.2019.109020},
	abstract = {Currently the training of neural networks relies on data of comparable accuracy but in real applications only a very small set of high-fidelity data is available while inexpensive lower fidelity data may be plentiful. We propose a new composite neural network (NN) that can be trained based on multi-fidelity data. It is comprised of three NNs, with the first NN trained using the low-fidelity data and coupled to two high-fidelity NNs, one with activation functions and another one without, in order to discover and exploit nonlinear and linear correlations, respectively, between the low-fidelity and the high-fidelity data. We first demonstrate the accuracy of the new multi-fidelity NN for approximating some standard benchmark functions but also a 20-dimensional function that is not easy to approximate with other methods, e.g. Gaussian process regression. Subsequently, we extend the recently developed physics-informed neural networks (PINNs) to be trained with multi-fidelity data sets (MPINNs). MPINNs contain four fully-connected neural networks, where the first one approximates the low-fidelity data, while the second and third construct the correlation between the low- and high-fidelity data and produce the multi-fidelity approximation, which is then used in the last NN that encodes the partial differential equations (PDEs). Specifically, by decomposing the correlation into a linear and nonlinear part, the present model is capable of learning both the linear and complex nonlinear correlations between the low- and high-fidelity data adaptively. By training the MPINNs, we can: (1) obtain the correlation between the low- and high-fidelity data, (2) infer the quantities of interest based on a few scattered data, and (3) identify the unknown parameters in the PDEs. In particular, we employ the MPINNs to learn the hydraulic conductivity field for unsaturated flows as well as the reactive models for reactive transport. The results demonstrate that MPINNs can achieve relatively high accuracy based on a very small set of high-fidelity data. Despite the relatively low dimension and limited number of fidelities (two-fidelity levels) for the benchmark problems in the present study, the proposed model can be readily extended to very high-dimensional regression and classification problems involving multi-fidelity data.},
	language = {en},
	urldate = {2022-05-27},
	journal = {J. Comput. Phys.},
	author = {Meng, Xuhui and Karniadakis, George Em},
	month = jan,
	year = {2020},
	keywords = {Physics-informed neural networks, Adversarial data, Multi-fidelity, Porous media, Reactive transport},
	pages = {109020},
}

@techreport{lu_multifidelity_2022,
	title = {Multifidelity deep neural operators for efficient learning of partial differential equations with application to fast inverse design of nanoscale heat transport},
	url = {http://arxiv.org/abs/2204.06684},
	abstract = {Deep neural operators can learn operators mapping between infinite-dimensional function spaces via deep neural networks and have become an emerging paradigm of scientific machine learning. However, training neural operators usually requires a large amount of high-fidelity data, which is often difficult to obtain in real engineering problems. Here, we address this challenge by using multifidelity learning, i.e., learning from multifidelity datasets. We develop a multifidelity neural operator based on a deep operator network (DeepONet). A multifidelity DeepONet includes two standard DeepONets coupled by residual learning and input augmentation. Multifidelity DeepONet significantly reduces the required amount of high-fidelity data and achieves one order of magnitude smaller error when using the same amount of high-fidelity data. We apply a multifidelity DeepONet to learn the phonon Boltzmann transport equation (BTE), a framework to compute nanoscale heat transport. By combining a trained multifidelity DeepONet with genetic algorithm or topology optimization, we demonstrate a fast solver for the inverse design of BTE problems.},
	number = {arXiv:2204.06684},
	urldate = {2022-05-27},
	institution = {arXiv},
	author = {Lu, Lu and Pestourie, Raphael and Johnson, Steven G. and Romano, Giuseppe},
	month = apr,
	year = {2022},
	doi = {10.48550/arXiv.2204.06684},
	note = {arXiv:2204.06684 [physics]
type: article},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/TDQB8P8E/2204.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/973GCGS3/2204.html:text/html},
}

@article{mao_deepmmnet_2021,
	title = {{DeepM}\&{Mnet} for hypersonics: {Predicting} the coupled flow and finite-rate chemistry behind a normal shock using neural-network approximation of operators},
	volume = {447},
	issn = {0021-9991},
	shorttitle = {{DeepM}\&{Mnet} for hypersonics},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999121005933},
	doi = {10.1016/j.jcp.2021.110698},
	abstract = {In high-speed flow past a normal shock, the fluid temperature rises rapidly triggering downstream chemical dissociation reactions. The chemical changes lead to appreciable changes in fluid properties, and these coupled multiphysics and the resulting multiscale dynamics are challenging to resolve numerically. Using conventional computational fluid dynamics (CFD) requires excessive computing cost. Here, we propose a totally new efficient approach, assuming that some sparse measurements of the state variables are available that can be seamlessly integrated in the simulation algorithm. We employ a special neural network for approximating nonlinear operators, the DeepONet [23], which is used to predict separately each individual field, given inputs from the rest of the fields of the coupled multiphysics system. We demonstrate the effectiveness of DeepONet for a benchmark hypersonic flow involving seven field variables. Specifically we predict five species in the non-equilibrium chemistry downstream of a normal shock at high Mach numbers as well as the velocity and temperature fields. We show that upon training, DeepONets can be over five orders of magnitude faster than the CFD solver employed to generate the training data and yield good accuracy for unseen Mach numbers within the range of training. Outside this range, DeepONet can still predict accurately and fast if a few sparse measurements are available. We then propose a composite supervised neural network, DeepM\&Mnet, that uses multiple pre-trained DeepONets as building blocks and scattered measurements to infer the set of all seven fields in the entire domain of interest. Two DeepM\&Mnet architectures are tested, and we demonstrate the accuracy and capacity for efficient data assimilation. DeepM\&Mnet is simple and general: it can be employed to construct complex multiphysics and multiscale models and assimilate sparse measurements using pre-trained DeepONets in a “plug-and-play” mode.},
	language = {en},
	urldate = {2022-05-27},
	journal = {J. Comput. Phys.},
	author = {Mao, Zhiping and Lu, Lu and Marxen, Olaf and Zaki, Tamer A. and Karniadakis, George Em},
	month = dec,
	year = {2021},
	keywords = {Chemically reacting flow, Data assimilation, Deep learning, DeepONet, Hypersonics, Operator approximation},
	pages = {110698},
}

@article{cai_deepmmnet_2021,
	title = {{DeepM}\&{Mnet}: {Inferring} the electroconvection multiphysics fields based on operator approximation by neural networks},
	volume = {436},
	issn = {0021-9991},
	shorttitle = {{DeepM}\&{Mnet}},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999121001911},
	doi = {10.1016/j.jcp.2021.110296},
	abstract = {Electroconvection is a multiphysics problem involving coupling of the flow field with the electric field as well as the cation and anion concentration fields. Here, we use electroconvection as a benchmark problem to put forward a new data assimilation framework, the DeepM\&Mnet, for simulating multiphysics and multiscale problems at speeds much faster than standard numerical methods using pre-trained neural networks. We first pre-train DeepONets that can predict independently each field, given general inputs from the rest of the fields of the coupled system. DeepONets can approximate nonlinear operators and are composed of two sub-networks, a branch net for the input fields and a trunk net for the locations of the output field. DeepONets, which are extremely fast, are used as building blocks in the DeepM\&Mnet and form constraints for the multiphysics solution along with some sparse available measurements of any of the fields. We demonstrate the new methodology and document the accuracy of each individual DeepONet, and subsequently we present two different DeepM\&Mnet architectures that infer accurately and efficiently 2D electroconvection fields for unseen electric potentials. The DeepM\&Mnet framework is general and can be applied for building any complex multiphysics and multiscale models based on very few measurements using pre-trained DeepONets in a “plug-and-play” mode.},
	language = {en},
	urldate = {2022-05-27},
	journal = {J. Comput. Phys.},
	author = {Cai, Shengze and Wang, Zhicheng and Lu, Lu and Zaki, Tamer A. and Karniadakis, George Em},
	month = jul,
	year = {2021},
	keywords = {Data assimilation, Deep learning, DeepONet, Operator approximation, Multiscale modeling, Mutiphysics},
	pages = {110296},
}

@techreport{jin_mionet_2022,
	title = {{MIONet}: {Learning} multiple-input operators via tensor product},
	shorttitle = {{MIONet}},
	url = {http://arxiv.org/abs/2202.06137},
	abstract = {As an emerging paradigm in scientific machine learning, neural operators aim to learn operators, via neural networks, that map between infinite-dimensional function spaces. Several neural operators have been recently developed. However, all the existing neural operators are only designed to learn operators defined on a single Banach space, i.e., the input of the operator is a single function. Here, for the first time, we study the operator regression via neural networks for multiple-input operators defined on the product of Banach spaces. We first prove a universal approximation theorem of continuous multiple-input operators. We also provide detailed theoretical analysis including the approximation error, which provides a guidance of the design of the network architecture. Based on our theory and a low-rank approximation, we propose a novel neural operator, MIONet, to learn multiple-input operators. MIONet consists of several branch nets for encoding the input functions and a trunk net for encoding the domain of the output function. We demonstrate that MIONet can learn solution operators involving systems governed by ordinary and partial differential equations. In our computational examples, we also show that we can endow MIONet with prior knowledge of the underlying system, such as linearity and periodicity, to further improve the accuracy.},
	number = {arXiv:2202.06137},
	urldate = {2022-05-27},
	institution = {arXiv},
	author = {Jin, Pengzhan and Meng, Shuai and Lu, Lu},
	month = feb,
	year = {2022},
	doi = {10.48550/arXiv.2202.06137},
	note = {arXiv:2202.06137 [physics]
type: article},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/P6PDPFFP/2202.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/UEYBZR3G/2202.html:text/html},
}

@article{wang_learning_2021,
	title = {Learning the solution operator of parametric partial differential equations with physics-informed {DeepONets}},
	volume = {7},
	url = {https://www.science.org/doi/10.1126/sciadv.abi8605},
	doi = {10.1126/sciadv.abi8605},
	number = {40},
	urldate = {2022-05-27},
	journal = {Sci. Adv.},
	author = {Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
	month = sep,
	year = {2021},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabi8605},
}

@article{lu_comprehensive_2022,
	title = {A comprehensive and fair comparison of two neural operators (with practical extensions) based on {FAIR} data},
	volume = {393},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782522001207},
	doi = {10.1016/j.cma.2022.114778},
	abstract = {Neural operators can learn nonlinear mappings between function spaces and offer a new simulation paradigm for real-time prediction of complex dynamics for realistic diverse applications as well as for system identification in science and engineering. Herein, we investigate the performance of two neural operators, which have shown promising results so far, and we develop new practical extensions that will make them more accurate and robust and importantly more suitable for industrial-complexity applications. The first neural operator, DeepONet, was published in 2019 (Lu et al., 2019), and its original architecture was based on the universal approximation theorem of Chen \& Chen (1995). The second one, named Fourier Neural Operator or FNO, was published in 2020 (Li et al., 2020), and it is based on parameterizing the integral kernel in the Fourier space. DeepONet is represented by a summation of products of neural networks (NNs), corresponding to the branch NN for the input function and the trunk NN for the output function; both NNs are general architectures, e.g., the branch NN can be replaced with a CNN or a ResNet. According to Kovachki et al. (2021), FNO in its continuous form can be viewed conceptually as a DeepONet with a specific architecture of the branch NN and a trunk NN represented by a trigonometric basis. In order to compare FNO with DeepONet computationally for realistic setups, we develop several extensions of FNO that can deal with complex geometric domains as well as mappings where the input and output function spaces are of different dimensions. We also develop an extended DeepONet with special features that provide inductive bias and accelerate training, and we present a faster implementation of DeepONet with cost comparable to the computational cost of FNO, which is based on the Fast Fourier Transform. We consider 16 different benchmarks to demonstrate the relative performance of the two neural operators, including instability wave analysis in hypersonic boundary layers, prediction of the vorticity field of a flapping airfoil, porous media simulations in complex-geometry domains, etc. We follow the guiding principles of FAIR (Findability, Accessibility, Interoperability, and Reusability) for scientific data management and stewardship. The performance of DeepONet and FNO is comparable for relatively simple settings, but for complex geometries the performance of FNO deteriorates greatly. We also compare theoretically the two neural operators and obtain similar error estimates for DeepONet and FNO under the same regularity assumptions.},
	language = {en},
	urldate = {2022-05-27},
	journal = {Comput. Methods Appl. Mech. Eng.},
	author = {Lu, Lu and Meng, Xuhui and Cai, Shengze and Mao, Zhiping and Goswami, Somdatta and Zhang, Zhongqiang and Karniadakis, George Em},
	month = apr,
	year = {2022},
	keywords = {Scientific machine learning, Deep learning, DeepONet, FNO, Nonlinear mappings, Operator regression},
	pages = {114778},
}

@article{lu_learning_2021,
	title = {Learning nonlinear operators via {DeepONet} based on the universal approximation theorem of operators},
	volume = {3},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-021-00302-5},
	doi = {10.1038/s42256-021-00302-5},
	abstract = {It is widely known that neural networks (NNs) are universal approximators of continuous functions. However, a less known but powerful result is that a NN with a single hidden layer can accurately approximate any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the structure and potential of deep neural networks (DNNs) in learning continuous operators or complex systems from streams of scattered data. Here, we thus extend this theorem to DNNs. We design a new network with small generalization error, the deep operator network (DeepONet), which consists of a DNN for encoding the discrete input function space (branch net) and another DNN for encoding the domain of the output functions (trunk net). We demonstrate that DeepONet can learn various explicit operators, such as integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations. We study different formulations of the input function space and its effect on the generalization error for 16 different diverse applications.},
	language = {en},
	number = {3},
	urldate = {2022-05-27},
	journal = {Nat. Mach. Intell.},
	author = {Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
	month = mar,
	year = {2021},
	note = {Number: 3
Publisher: Nature Publishing Group},
	keywords = {Applied mathematics, Computational science},
	pages = {218--229},
	file = {Snapshot:/home/hell/Zotero/storage/7CIFH2QX/s42256-021-00302-5.html:text/html;Snapshot:/home/hell/Zotero/storage/9TSPYEZW/s42256-021-00302-5.html:text/html},
}

@article{wang_eigenvector_2021,
	title = {On the eigenvector bias of {Fourier} feature networks: {From} regression to solving multi-scale {PDEs} with physics-informed neural networks},
	volume = {384},
	issn = {0045-7825},
	shorttitle = {On the eigenvector bias of {Fourier} feature networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782521002759},
	doi = {10.1016/j.cma.2021.113938},
	abstract = {Physics-informed neural networks (PINNs) are demonstrating remarkable promise in integrating physical models with gappy and noisy observational data, but they still struggle in cases where the target functions to be approximated exhibit high-frequency or multi-scale features. In this work we investigate this limitation through the lens of Neural Tangent Kernel (NTK) theory and elucidate how PINNs are biased towards learning functions along the dominant eigen-directions of their limiting NTK. Using this observation, we construct novel architectures that employ spatio-temporal and multi-scale random Fourier features, and justify how such coordinate embedding layers can lead to robust and accurate PINN models. Numerical examples are presented for several challenging cases where conventional PINN models fail, including wave propagation and reaction–diffusion dynamics, illustrating how the proposed methods can be used to effectively tackle both forward and inverse problems involving partial differential equations with multi-scale behavior. All code an data accompanying this manuscript will be made publicly available at https://github.com/PredictiveIntelligenceLab/MultiscalePINNs.},
	language = {en},
	urldate = {2022-05-27},
	journal = {Comput. Methods Appl. Mech. Eng.},
	author = {Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
	month = oct,
	year = {2021},
	keywords = {Partial differential equations, Scientific machine learning, Spectral bias, Deep learning, Neural Tangent Kernel},
	pages = {113938},
}

@article{lu_physics-informed_2021,
	title = {Physics-{Informed} {Neural} {Networks} with {Hard} {Constraints} for {Inverse} {Design}},
	volume = {43},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/21M1397908},
	doi = {10.1137/21M1397908},
	abstract = {Inverse design arises in a variety of areas in engineering such as acoustic, mechanics, thermal/electronic transport, electromagnetism, and optics. Topology optimization is an important form of inverse design, where one optimizes a designed geometry to achieve targeted properties parameterized by the materials at every point in a design region. This optimization is challenging, because it has a very high dimensionality and is usually constrained by partial differential equations (PDEs) and additional inequalities. Here, we propose a new deep learning method---physics-informed neural networks with hard constraints (hPINNs)---for solving topology optimization. hPINN leverages the recent development of PINNs for solving PDEs, and thus does not require a large dataset (generated by numerical PDE solvers) for training. However, all the constraints in PINNs are soft constraints, and hence we impose hard constraints by using the penalty method and the augmented Lagrangian method. We demonstrate the effectiveness of hPINN for a holography problem in optics and a fluid problem of Stokes flow. We achieve the same objective as conventional PDE-constrained optimization methods based on adjoint methods and numerical PDE solvers, but find that the design obtained from hPINN is often smoother for problems whose solution is not unique. Moreover, the implementation of inverse design with hPINN can be easier than that of conventional methods because it exploits the extensive deep-learning software infrastructure.},
	number = {6},
	urldate = {2022-05-27},
	journal = {SIAM J. Sci. Comput.},
	author = {Lu, Lu and Pestourie, Raphaël and Yao, Wenjie and Wang, Zhicheng and Verdugo, Francesc and Johnson, Steven G.},
	month = jan,
	year = {2021},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {physics-informed neural networks, 35R30, 65K10, 68T20, augmented Lagrangian method, inverse design, partial differential equations, penalty method, topology optimization},
	pages = {B1105--B1132},
}

@article{zhang_learning_2020,
	title = {Learning in {Modal} {Space}: {Solving} {Time}-{Dependent} {Stochastic} {PDEs} {Using} {Physics}-{Informed} {Neural} {Networks}},
	volume = {42},
	issn = {1064-8275},
	shorttitle = {Learning in {Modal} {Space}},
	url = {https://epubs.siam.org/doi/10.1137/19M1260141},
	doi = {10.1137/19M1260141},
	abstract = {One of the open problems in scientific computing is the long-time integration of nonlinear stochastic partial differential equations (SPDEs), especially with arbitrary initial data. We address this problem by taking advantage of recent advances in scientific machine learning and the spectral dynamically orthogonal (DO) and borthogonal (BO) methods for representing stochastic processes. The recently introduced DO/BO methods reduce the SPDE to solving a system of deterministic PDEs and a system of stochastic ordinary differential equations. Specifically, we propose two new physics-informed neural networks (PINNs) for solving time-dependent SPDEs, namely the neural network (NN)-DO/BO methods. The proposed methods incorporate the DO/BO constraints into the loss function (along with the modal decomposition of the SPDE) with an implicit form instead of generating explicit expressions for the temporal derivatives of the DO/BO modes. Hence, the NN-DO/BO methods can overcome some of the drawbacks of the original DO/BO methods. For example, we do not need the assumption that the covariance matrix of the random coefficients is invertible as in the original DO method, and we can remove the assumption of no eigenvalue crossing as in the original BO method. Moreover, the NN-DO/BO methods can be used to solve time-dependent stochastic inverse problems with the same formulation and same computational complexity as for forward problems. We demonstrate the capability of the proposed methods via several numerical examples, namely: (1) A linear stochastic advection equation with deterministic initial condition: we obtain good results with the proposed methods, while the original DO/BO methods cannot be applied directly in this case. (2) Long-time integration of the stochastic Burgers' equation: we show the good performance of NN-DO/BO methods, especially the effectiveness of the NN-BO approach for such problems with many eigenvalue crossings during the whole time evolution, while the original BO method fails. (3) Nonlinear reaction diffusion equation: we consider both the forward problem and the inverse problems, including very noisy initial point values, to investigate the flexibility of the NN-DO/BO methods in handling inverse and mixed type problems. Taken together, these simulation results demonstrate that the NN-DO/BO methods can be employed to effectively quantify uncertainty propagation in a wide range of physical problems, but future work should address the efficiency issue of PINNs for forward problems.},
	number = {2},
	urldate = {2022-05-27},
	journal = {SIAM J. Sci. Comput.},
	author = {Zhang, Dongkun and Guo, Ling and Karniadakis, George Em},
	month = jan,
	year = {2020},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {scientific machine learning, 34F05, 60H35, 62M45, biorthogonality, data-driven modeling, dynamical orthogonality, inverse problems, uncertainty quantification},
	pages = {A639--A665},
}

@article{pang_fpinns_2019,
	title = {{fPINNs}: {Fractional} {Physics}-{Informed} {Neural} {Networks}},
	volume = {41},
	issn = {1064-8275},
	shorttitle = {{fPINNs}},
	url = {https://epubs.siam.org/doi/10.1137/18M1229845},
	doi = {10.1137/18M1229845},
	abstract = {Physics-informed neural networks (PINNs), introduced in [M. Raissi, P. Perdikaris, and G. Karniadakis, J. Comput. Phys., 378 (2019), pp. 686--707], are effective in solving integer-order partial differential equations (PDEs) based on scattered and noisy data. PINNs employ standard feedforward neural networks (NNs) with the PDEs explicitly encoded into the NN using automatic differentiation, while the sum of the mean-squared PDE residuals and the mean-squared error in initial-boundary conditions is minimized with respect to the NN parameters. Here we extend PINNs to fractional PINNs (fPINNs) to solve space-time fractional advection-diffusion equations (fractional ADEs), and we study systematically their convergence, hence explaining both fPINNs and PINNs for the first time. Specifically, we demonstrate their accuracy and effectiveness in solving multidimensional forward and inverse problems with forcing terms whose values are only known at randomly scattered spatio-temporal coordinates (black-box (BB) forcing terms). A novel element of the fPINNs is the hybrid approach that we introduce for constructing the residual in the loss function using both automatic differentiation for the integer-order operators and numerical discretization for the fractional operators. This approach bypasses the difficulties stemming from the fact that automatic differentiation is not applicable to fractional operators because the standard chain rule in integer calculus is not valid in fractional calculus. To discretize the fractional operators, we employ the Grünwald--Letnikov (GL) formula in one-dimensional fractional ADEs and the vector GL formula in conjunction with the directional fractional Laplacian in two- and three-dimensional fractional ADEs. We first consider the one-dimensional fractional Poisson equation and compare the convergence of the fPINNs against the finite difference method (FDM). We present the solution convergence using both the mean \$L{\textasciicircum}2\$ error as well as the standard deviation due to sensitivity to NN parameter initializations. Using different GL formulas we observe first-, second-, and third-order convergence rates for small size training sets but the error saturates for larger training sets. We explain these results by analyzing the four sources of numerical errors due to discretization, sampling, NN approximation, and optimization. The total error decays monotonically (below \$10{\textasciicircum}\{-5\}\$ for a third-order GL formula) but it saturates beyond that point due to the optimization error. We also analyze the relative balance between discretization and sampling errors and observe that the sampling size and the number of discretization points (auxiliary points) should be comparable to achieve the highest accuracy. As we increase the depth of the NN up to certain value, the mean error decreases and the standard deviation increases, whereas the width has essentially no effect unless its value is either too small or too large. We next consider time-dependent fractional ADEs and compare white-box (WB) and BB forcing. We observe that for the WB forcing, our results are similar to the aforementioned cases; however, for the BB forcing fPINNs outperform FDM. Subsequently, we consider multidimensional time-, space-, and space-time-fractional ADEs using the directional fractional Laplacian and we observe relative errors of \$10{\textasciicircum}\{-3\}{\textbackslash}sim10{\textasciicircum}\{-4\}\$. Finally, we solve several inverse problems in one, two, and three dimensions to identify the fractional orders, diffusion coefficients, and transport velocities and obtain accurate results given proper initializations even in the presence of significant noise.},
	number = {4},
	urldate = {2022-05-27},
	journal = {SIAM J. Sci. Comput.},
	author = {Pang, Guofei and Lu, Lu and Karniadakis, George Em},
	month = jan,
	year = {2019},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {35R30, 35R11, 65Mxx, fractional advection-diffusion, fractional inverse problem, numerical error analysis, parameter identification, physics-informed learning machines},
	pages = {A2603--A2626},
}

@incollection{lemieux_chapter_2006,
	series = {Simulation},
	title = {Chapter 12 {Quasi}-{Random} {Number} {Techniques}},
	volume = {13},
	url = {https://www.sciencedirect.com/science/article/pii/S0927050706130121},
	abstract = {Over the last decade, quasi-Monte Carlo methods have been used as an efficient estimation tool in various high-dimensional applications, particularly in the field of finance. These methods can be seen as a deterministic version of the Monte Carlo method for multidimensional integration, in which quasi-random numbers are used to construct highly-uniform point sets over which the integrand is sampled. This chapter discusses the use of these techniques in simulation.},
	language = {en},
	urldate = {2022-05-27},
	booktitle = {Handb. Oper. Res. Manage. Sci.},
	publisher = {elsevier},
	author = {Lemieux, C.},
	editor = {Henderson, Shane G. and Nelson, Barry L.},
	month = jan,
	year = {2006},
	doi = {10.1016/S0927-0507(06)13012-1},
	pages = {351--379},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/TFWAU376/S0927050706130121.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/6FN5YMNL/S0927050706130121.html:text/html},
}

@article{shaw_quasirandom_1988,
	title = {A {Quasirandom} {Approach} to {Integration} in {Bayesian} {Statistics}},
	volume = {16},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/2241763},
	abstract = {Practical Bayesian statistics with realistic models usually gives posterior distributions that are analytically intractable, and inferences must be made via numerical integration. In many cases, the integrands can be transformed into periodic functions on the unit d-dimensional cube, for which quasirandom sequences are known to give efficient numerical integration rules. This paper reviews some relevant theory, defines new criteria for identifying suitable quasirandom sequences and suggests some extensions to the basic integration rules. Various quasirandom methods are then compared on the sort of integrals that arise in Bayesian inference and are shown to be much more efficient than Monte Carlo methods.},
	number = {2},
	urldate = {2022-05-27},
	journal = {Ann. Stat.},
	author = {Shaw, J. E. H.},
	year = {1988},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {895--914},
}

@article{hammersley_monte_2013,
	title = {Monte {Carlo} methods: {Springer} {Science} and {Business} {Media}},
	author = {Hammersley, J.},
	year = {2013},
}

@article{hammersley_monte_1960,
	title = {Monte {Carlo} {Methods} for {Solving} {Multivariable} {Problems}},
	volume = {86},
	issn = {1749-6632},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-6632.1960.tb42846.x},
	doi = {10.1111/j.1749-6632.1960.tb42846.x},
	language = {en},
	number = {3},
	urldate = {2022-05-27},
	journal = {Ann. N.Y. Acad. Sci.},
	author = {Hammersley, J. M.},
	year = {1960},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1749-6632.1960.tb42846.x},
	pages = {844--874},
	file = {Snapshot:/home/hell/Zotero/storage/I6EXMB5V/j.1749-6632.1960.tb42846.html:text/html;Snapshot:/home/hell/Zotero/storage/EGYSKW7A/j.1749-6632.1960.tb42846.html:text/html},
}

@article{joe_constructing_2008,
	title = {Constructing {Sobol} {Sequences} with {Better} {Two}-{Dimensional} {Projections}},
	volume = {30},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/070709359},
	doi = {10.1137/070709359},
	abstract = {Direction numbers for generating Sobol\$'\$ sequences that satisfy the so-called Property A in up to 1111 dimensions have previously been given in Joe and Kuo [ACM Trans. Math. Software, 29 (2003), pp. 49–57]. However, these Sobol\$'\$ sequences may have poor two-dimensional projections. Here we provide a new set of direction numbers alleviating this problem. These are obtained by treating Sobol\$'\$ sequences in d dimensions as \$(t,d)\$-sequences and then optimizing the t-values of the two-dimensional projections. Our target dimension is 21201.},
	number = {5},
	urldate = {2022-05-27},
	journal = {SIAM J. Sci. Comput.},
	author = {Joe, Stephen and Kuo, Frances Y.},
	month = jan,
	year = {2008},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65D30, 65D32, digital nets and sequences, numerical integration, quasi-Monte Carlo methods, Sobol\$'\$ sequences, two-dimensional projections},
	pages = {2635--2654},
}

@article{sobol_distribution_1967,
	title = {On the distribution of points in a cube and the approximate evaluation of integrals},
	volume = {7},
	issn = {00415553},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0041555367901449},
	doi = {10.1016/0041-5553(67)90144-9},
	language = {en},
	number = {4},
	urldate = {2022-05-27},
	journal = {USSR Comput. Math. Math. Phys.},
	author = {Sobol', I.M},
	month = jan,
	year = {1967},
	pages = {86--112},
}

@article{faure_generalized_2009,
	title = {Generalized {Halton} sequences in 2008: {A} comparative study},
	volume = {19},
	issn = {1049-3301},
	shorttitle = {Generalized {Halton} sequences in 2008},
	url = {https://doi.org/10.1145/1596519.1596520},
	doi = {10.1145/1596519.1596520},
	abstract = {Halton sequences have always been quite popular with practitioners, in part because of their intuitive definition and ease of implementation. However, in their original form, these sequences have also been known for their inadequacy to integrate functions in moderate to large dimensions, in which case (t,s)-sequences such as the Sobol' sequence are usually preferred. To overcome this problem, one possible approach is to include permutations in the definition of Halton sequences—thereby obtaining generalized Halton sequences—an idea that goes back to almost thirty years ago, and that has been studied by many researchers in the last few years. In parallel to these efforts, an important improvement in the upper bounds for the discrepancy of Halton sequences has been made by Atanassov in 2004. Together, these two lines of research have revived the interest in Halton sequences. In this article, we review different generalized Halton sequences that have been proposed recently, and compare them by means of numerical experiments. We also propose a new generalized Halton sequence which, we believe, offers a practical advantage over the surveyed constructions, and that should be of interest to practitioners.},
	number = {4},
	urldate = {2022-05-27},
	journal = {ACM Trans. Model. Comput. Simul.},
	author = {Faure, Henri and Lemieux, Christiane},
	month = nov,
	year = {2009},
	keywords = {discrepancy, Halton sequences, permutations, scrambling},
	pages = {15:1--15:31},
}

@article{halton_efficiency_1960,
	title = {On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals},
	volume = {2},
	issn = {0945-3245},
	url = {https://doi.org/10.1007/BF01386213},
	doi = {10.1007/BF01386213},
	language = {en},
	number = {1},
	urldate = {2022-05-27},
	journal = {Numer. Math.},
	author = {Halton, J. H.},
	month = dec,
	year = {1960},
	keywords = {Mathematical Method},
	pages = {84--90},
}

@article{viana_tutorial_2016,
	title = {A {Tutorial} on {Latin} {Hypercube} {Design} of {Experiments}},
	volume = {32},
	issn = {1099-1638},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qre.1924},
	doi = {10.1002/qre.1924},
	abstract = {The growing power of computers enabled techniques created for design and analysis of simulations to be applied to a large spectrum of problems and to reach high level of acceptance among practitioners. Generally, when simulations are time consuming, a surrogate model replaces the computer code in further studies (e.g., optimization, sensitivity analysis, etc.). The first step for a successful surrogate modeling and statistical analysis is the planning of the input configuration that is used to exercise the simulation code. Among the strategies devised for computer experiments, Latin hypercube designs have become particularly popular. This paper provides a tutorial on Latin hypercube design of experiments, highlighting potential reasons of its widespread use. The discussion starts with the early developments in optimization of the point selection and goes all the way to the pitfalls of the indiscriminate use of Latin hypercube designs. Final thoughts are given on opportunities for future research. Copyright © 2015 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2022-05-27},
	journal = {Qual. Reliab. Eng. Int.},
	author = {Viana, Felipe A. C.},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.1924},
	keywords = {design and analysis of computer experiments, Latin hypercube sampling, sequential sampling, space-filling designs},
	pages = {1975--1985},
}

@article{arulkumaran_deep_2017,
	title = {Deep {Reinforcement} {Learning}: {A} {Brief} {Survey}},
	volume = {34},
	issn = {1558-0792},
	shorttitle = {Deep {Reinforcement} {Learning}},
	doi = {10.1109/MSP.2017.2743240},
	abstract = {Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higher-level understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.},
	number = {6},
	journal = {IEEE Signal Process. Mag.},
	author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
	month = nov,
	year = {2017},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Neural networks, Machine learning, Artificial intelligence, Learning (artificial intelligence), Signal processing algorithms, Visualization},
	pages = {26--38},
	file = {IEEE Xplore Abstract Record:/home/hell/Zotero/storage/ICYNUKUA/8103164.html:text/html;IEEE Xplore Abstract Record:/home/hell/Zotero/storage/N2LX75FI/8103164.html:text/html},
}

@techreport{samsami_distributed_2020,
	title = {Distributed {Deep} {Reinforcement} {Learning}: {An} {Overview}},
	shorttitle = {Distributed {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2011.11012},
	abstract = {Deep reinforcement learning (DRL) is a very active research area. However, several technical and scientific issues require to be addressed, amongst which we can mention data inefficiency, exploration-exploitation trade-off, and multi-task learning. Therefore, distributed modifications of DRL were introduced; agents that could be run on many machines simultaneously. In this article, we provide a survey of the role of the distributed approaches in DRL. We overview the state of the field, by studying the key research works that have a significant impact on how we can use distributed methods in DRL. We choose to overview these papers, from the perspective of distributed learning, and not the aspect of innovations in reinforcement learning algorithms. Also, we evaluate these methods on different tasks and compare their performance with each other and with single actor and learner agents.},
	number = {arXiv:2011.11012},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Samsami, Mohammad Reza and Alimadad, Hossein},
	month = nov,
	year = {2020},
	doi = {10.48550/arXiv.2011.11012},
	note = {arXiv:2011.11012 [cs]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, and Cluster Computing, Computer Science - Distributed, Parallel},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/JF4E23XB/2011.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/NBYQUBED/2011.html:text/html},
}

@article{martino_effective_2017,
	title = {Effective sample size for importance sampling based on discrepancy measures},
	volume = {131},
	issn = {0165-1684},
	url = {https://www.sciencedirect.com/science/article/pii/S0165168416302110},
	doi = {10.1016/j.sigpro.2016.08.025},
	abstract = {The Effective Sample Size (ESS) is an important measure of efficiency of Monte Carlo methods such as Markov Chain Monte Carlo (MCMC) and Importance Sampling (IS) techniques. In the IS context, an approximation ESS{\textasciicircum} of the theoretical ESS definition is widely applied, involving the inverse of the sum of the squares of the normalized importance weights. This formula, ESS{\textasciicircum}, has become an essential piece within Sequential Monte Carlo (SMC) methods, to assess the convenience of a resampling step. From another perspective, the expression ESS{\textasciicircum} is related to the Euclidean distance between the probability mass described by the normalized weights and the discrete uniform probability mass function (pmf). In this work, we derive other possible ESS functions based on different discrepancy measures between these two pmfs. Several examples are provided involving, for instance, the geometric mean of the weights, the discrete entropy (including the perplexity measure, already proposed in literature) and the Gini coefficient among others. We list five theoretical requirements which a generic ESS function should satisfy, allowing us to classify different ESS measures. We also compare the most promising ones by means of numerical simulations.},
	language = {en},
	urldate = {2022-05-26},
	journal = {Signal Process.},
	author = {Martino, Luca and Elvira, Víctor and Louzada, Francisco},
	month = feb,
	year = {2017},
	keywords = {Bayesian Inference, Effective Sample Size, Importance Sampling, Particle Filtering, Perplexity, Sequential Monte Carlo},
	pages = {386--401},
}

@article{nabian_efficient_2021,
	title = {Efficient training of physics-informed neural networks via importance sampling},
	volume = {36},
	issn = {1467-8667},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mice.12685},
	doi = {10.1111/mice.12685},
	abstract = {Physics-informed neural networks (PINNs) are a class of deep neural networks that are trained, using automatic differentiation, to compute the response of systems governed by partial differential equations (PDEs). The training of PINNs is simulation free, and does not require any training data set to be obtained from numerical PDE solvers. Instead, it only requires the physical problem description, including the governing laws of physics, domain geometry, initial/boundary conditions, and the material properties. This training usually involves solving a nonconvex optimization problem using variants of the stochastic gradient descent method, with the gradient of the loss function approximated on a batch of collocation points, selected randomly in each iteration according to a uniform distribution. Despite the success of PINNs in accurately solving a wide variety of PDEs, the method still requires improvements in terms of computational efficiency. To this end, in this paper, we study the performance of an importance sampling approach for efficient training of PINNs. Using numerical examples together with theoretical evidences, we show that in each training iteration, sampling the collocation points according to a distribution proportional to the loss function will improve the convergence behavior of the PINNs training. Additionally, we show that providing a piecewise constant approximation to the loss function for faster importance sampling can further improve the training efficiency. This importance sampling approach is straightforward and easy to implement in the existing PINN codes, and also does not introduce any new hyperparameter to calibrate. The numerical examples include elasticity, diffusion, and plane stress problems, through which we numerically verify the accuracy and efficiency of the importance sampling approach compared to the predominant uniform sampling approach.},
	language = {en},
	number = {8},
	urldate = {2022-05-26},
	journal = {Comput.-Aided Civ. Infrastruct. Eng.},
	author = {Nabian, Mohammad Amin and Gladstone, Rini Jasmine and Meidani, Hadi},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/mice.12685},
	pages = {962--977},
	file = {Snapshot:/home/hell/Zotero/storage/3CRN3SH3/mice.html:text/html;Snapshot:/home/hell/Zotero/storage/L3WZ9Y3R/mice.html:text/html},
}

@inproceedings{chan_level_2005,
	title = {Level set based shape prior segmentation},
	volume = {2},
	doi = {10.1109/CVPR.2005.212},
	abstract = {We propose a level set based variational approach that incorporates shape priors into Chan-Vese's model for the shape prior segmentation problem. In our model, besides the level set function for segmentation, as in Cremers' work, we introduce another labelling level set function to indicate the regions on which the prior shape should be compared. Our model can segment an object, whose shape is similar to the given prior shape, from a background where there are several objects. Moreover, we provide a proof for a fast solution principle, which was mentioned by F. Gibou et al., and similar to the one proposed in [B. Song et al., (2002)], for minimizing Chan-Vese's segmentation model without length term. We extend the principle to the minimization of our prescribed functionals.},
	booktitle = {CVPR},
	author = {Chan, T. and Zhu, Wei},
	month = jun,
	year = {2005},
	note = {ISSN: 1063-6919},
	keywords = {Active contours, Convergence, Image processing, Image segmentation, Labeling, Level set, Mathematical model, Mathematics, Shape, Solid modeling},
	pages = {1164--1170 vol. 2},
	file = {IEEE Xplore Abstract Record:/home/hell/Zotero/storage/8KB9P5Z7/1467575.html:text/html;IEEE Xplore Abstract Record:/home/hell/Zotero/storage/M2P9U5DE/1467575.html:text/html},
}

@techreport{son_sobolev_2021,
	title = {Sobolev {Training} for {Physics} {Informed} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2101.08932},
	abstract = {Physics Informed Neural Networks (PINNs) is a promising application of deep learning. The smooth architecture of a fully connected neural network is appropriate for finding the solutions of PDEs; the corresponding loss function can also be intuitively designed and guarantees the convergence for various kinds of PDEs. However, the rate of convergence has been considered as a weakness of this approach. This paper proposes Sobolev-PINNs, a novel loss function for the training of PINNs, making the training substantially efficient. Inspired by the recent studies that incorporate derivative information for the training of neural networks, we develop a loss function that guides a neural network to reduce the error in the corresponding Sobolev space. Surprisingly, a simple modification of the loss function can make the training process similar to {\textbackslash}textit\{Sobolev Training\} although PINNs is not a fully supervised learning task. We provide several theoretical justifications that the proposed loss functions upper bound the error in the corresponding Sobolev spaces for the viscous Burgers equation and the kinetic Fokker--Planck equation. We also present several simulation results, which show that compared with the traditional \$L{\textasciicircum}2\$ loss function, the proposed loss function guides the neural network to a significantly faster convergence. Moreover, we provide the empirical evidence that shows that the proposed loss function, together with the iterative sampling techniques, performs better in solving high dimensional PDEs.},
	number = {arXiv:2101.08932},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Son, Hwijae and Jang, Jin Woo and Han, Woo Jin and Hwang, Hyung Ju},
	month = dec,
	year = {2021},
	doi = {10.48550/arXiv.2101.08932},
	note = {arXiv:2101.08932 [cs, math]
type: article},
	keywords = {Mathematics - Numerical Analysis},
}

@techreport{heger_investigation_2022,
	title = {Investigation of {Physics}-{Informed} {Deep} {Learning} for the {Prediction} of {Parametric}, {Three}-{Dimensional} {Flow} {Based} on {Boundary} {Data}},
	url = {http://arxiv.org/abs/2203.09204},
	abstract = {The placement of temperature sensitive and safety-critical components is crucial in the automotive industry. It is therefore inevitable, even at the design stage of new vehicles that these components are assessed for potential safety issues. However, with increasing number of design proposals, risk assessment quickly becomes expensive. We therefore present a parameterized surrogate model for the prediction of three-dimensional flow fields in aerothermal vehicle simulations. The proposed physics-informed neural network (PINN) design is aimed at learning families of flow solutions according to a geometric variation. In scope of this work, we could show that our nondimensional, multivariate scheme can be efficiently trained to predict the velocity and pressure distribution for different design scenarios and geometric scales. The proposed algorithm is based on a parametric minibatch training which enables the utilization of large datasets necessary for the three-dimensional flow modeling. Further, we introduce a continuous resampling algorithm that allows to operate on one static dataset. Every feature of our methodology is tested individually and verified against conventional CFD simulations. Finally, we apply our proposed method in context of an exemplary real-world automotive application.},
	number = {arXiv:2203.09204},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Heger, Philip and Full, Markus and Hilger, Daniel and Hosters, Norbert},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2203.09204},
	note = {arXiv:2203.09204 [cs, math]
type: article},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Computer Science - Computational Engineering, Finance, and Science, 76.01, I.2.8, and Science, Computer Science - Computational Engineering, Finance},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/JEEYEYUV/2203.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/YILDZ5FQ/2203.html:text/html},
}

@techreport{raj_physics-informed_2022,
	title = {Physics-informed neural networks for solving thermo-mechanics problems of functionally graded material},
	url = {http://arxiv.org/abs/2111.10751},
	abstract = {Differential equations are indispensable to engineering and hence to innovation. In recent years, physics-informed neural networks (PINN) have emerged as a novel method for solving differential equations. PINN method has the advantage of being meshless, scalable, and can potentially be intelligent in terms of transferring the knowledge learned from solving one differential equation to the other. The exploration in this field has majorly been limited to solving linear-elasticity problems, crack propagation problems. This study uses PINNs to solve coupled thermo-mechanics problems of materials with functionally graded properties. An in-depth analysis of the PINN framework has been carried out by understanding the training datasets, model architecture, and loss functions. The efficacy of the PINN models in solving thermo-mechanics differential equations has been measured by comparing the obtained solutions either with analytical solutions or finite element method-based solutions. While R2 score of more than 99\% has been achieved in predicting primary variables such as displacement and temperature fields, achieving the same for secondary variables such as stress turns out to be more challenging. This study is the first to implement the PINN framework for solving coupled thermo-mechanics problems on composite materials. This study is expected to enhance the understanding of the novel PINN framework and will be seminal for further research on PINNs.},
	number = {arXiv:2111.10751},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Raj, Mayank and Kumbhar, Pramod and Annabattula, Ratna Kumar},
	month = jan,
	year = {2022},
	doi = {10.48550/arXiv.2111.10751},
	note = {arXiv:2111.10751 [cs]
type: article},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, and Science, Computer Science - Computational Engineering, Finance},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/34M7R6WP/2111.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/TYB3BGCI/2111.html:text/html},
}

@techreport{demo_extended_2021,
	title = {An extended physics informed neural network for preliminary analysis of parametric optimal control problems},
	url = {http://arxiv.org/abs/2110.13530},
	abstract = {In this work we propose an extension of physics informed supervised learning strategies to parametric partial differential equations. Indeed, even if the latter are indisputably useful in many applications, they can be computationally expensive most of all in a real-time and many-query setting. Thus, our main goal is to provide a physics informed learning paradigm to simulate parametrized phenomena in a small amount of time. The physics information will be exploited in many ways, in the loss function (standard physics informed neural networks), as an augmented input (extra feature employment) and as a guideline to build an effective structure for the neural network (physics informed architecture). These three aspects, combined together, will lead to a faster training phase and to a more accurate parametric prediction. The methodology has been tested for several equations and also in an optimal control framework.},
	number = {arXiv:2110.13530},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Demo, Nicola and Strazzullo, Maria and Rozza, Gianluigi},
	month = oct,
	year = {2021},
	doi = {10.48550/arXiv.2110.13530},
	note = {arXiv:2110.13530 [cs, math]
type: article},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/3ATL7DL3/2110.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/ASBB5R29/2110.html:text/html},
}

@techreport{schiassi_extreme_2020,
	title = {Extreme {Theory} of {Functional} {Connections}: {A} {Physics}-{Informed} {Neural} {Network} {Method} for {Solving} {Parametric} {Differential} {Equations}},
	shorttitle = {Extreme {Theory} of {Functional} {Connections}},
	url = {http://arxiv.org/abs/2005.10632},
	abstract = {In this work we present a novel, accurate, and robust physics-informed method for solving problems involving parametric differential equations (DEs) called the Extreme Theory of Functional Connections, or X-TFC. The proposed method is a synergy of two recently developed frameworks for solving problems involving parametric DEs, 1) the Theory of Functional Connections, TFC, and the Physics-Informed Neural Networks, PINN. Although this paper focuses on the solution of exact problems involving parametric DEs (i.e. problems where the modeling error is negligible) with known parameters, X-TFC can also be used for data-driven solutions and data-driven discovery of parametric DEs. In the proposed method, the latent solution of the parametric DEs is approximated by a TFC constrained expression that uses a Neural Network (NN) as the free-function. This approximate solution form always analytically satisfies the constraints of the DE, while maintaining a NN with unconstrained parameters, like the Deep-TFC method. X-TFC differs from PINN and Deep-TFC; whereas PINN and Deep-TFC use a deep-NN, X-TFC uses a single-layer NN, or more precisely, an Extreme Learning Machine, ELM. This choice is based on the properties of the ELM algorithm. In order to numerically validate the method, it was tested over a range of problems including the approximation of solutions to linear and non-linear ordinary DEs (ODEs), systems of ODEs (SODEs), and partial DEs (PDEs). Furthermore, a few of these problems are of interest in physics and engineering such as the Classic Emden-Fowler equation, the Radiative Transfer (RT) equation, and the Heat-Transfer (HT) equation. The results show that X-TFC achieves high accuracy with low computational time and thus it is comparable with the other state-of-the-art methods.},
	number = {arXiv:2005.10632},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Schiassi, Enrico and Leake, Carl and De Florio, Mario and Johnston, Hunter and Furfaro, Roberto and Mortari, Daniele},
	month = may,
	year = {2020},
	doi = {10.48550/arXiv.2005.10632},
	note = {arXiv:2005.10632 [physics, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Physics - Computational Physics, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/7NURXFU4/2005.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/7C8M6H2G/2005.html:text/html},
}

@article{sukumar_exact_2022,
	title = {Exact imposition of boundary conditions with distance functions in physics-informed deep neural networks},
	volume = {389},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782521006186},
	doi = {10.1016/j.cma.2021.114333},
	abstract = {In this paper, we introduce a new approach based on distance fields to exactly impose boundary conditions in physics-informed deep neural networks. The challenges in satisfying Dirichlet boundary conditions in meshfree and particle methods are well-known. This issue is also pertinent in the development of physics informed neural networks (PINN) for the solution of partial differential equations. We introduce geometry-aware trial functions in artificial neural networks to improve the training in deep learning for partial differential equations. To this end, we use concepts from constructive solid geometry (R-functions) and generalized barycentric coordinates (mean value potential fields) to construct ϕ(x), an approximate distance function to the boundary of a domain in Rd. To exactly impose homogeneous Dirichlet boundary conditions, the trial function is taken as ϕ(x) multiplied by the PINN approximation, and its generalization via transfinite interpolation is used to a priori satisfy inhomogeneous Dirichlet (essential), Neumann (natural), and Robin boundary conditions on complex geometries. In doing so, we eliminate modeling error associated with the satisfaction of boundary conditions in a collocation method and ensure that kinematic admissibility is met pointwise in a Ritz method. With this new ansatz, the training for the neural network is simplified: sole contribution to the loss function is from the residual error at interior collocation points where the governing equation is required to be satisfied. Numerical solutions are computed using strong form collocation and Ritz minimization. To convey the main ideas and to assess the accuracy of the approach, we present numerical solutions for linear and nonlinear boundary-value problems over convex and nonconvex polygonal domains as well as over domains with curved boundaries. Benchmark problems in one dimension for linear elasticity, advection-diffusion, and beam bending; and in two dimensions for the steady-state heat equation, Laplace equation, biharmonic equation (Kirchhoff plate bending), and the nonlinear Eikonal equation are considered. The construction of approximate distance functions using R-functions extends to higher dimensions, and we showcase its use by solving a Poisson problem with homogeneous Dirichlet boundary conditions over the four-dimensional hypercube. The proposed approach consistently outperforms a standard PINN-based collocation method, which underscores the importance of exactly (a priori) satisfying the boundary condition when constructing a loss function in PINN. This study provides a pathway for meshfree analysis to be conducted on the exact geometry without domain discretization.},
	language = {en},
	urldate = {2022-05-26},
	journal = {Comput. Methods Appl. Mech. Eng.},
	author = {Sukumar, N. and Srivastava, Ankit},
	month = feb,
	year = {2022},
	keywords = {Deep learning, Distance function, Exact geometry, Meshfree method, R-function, Transfinite interpolation},
	pages = {114333},
}

@article{tato_improving_2018,
	title = {{IMPROVING} {ADAM} {OPTIMIZER}},
	abstract = {We present a modiﬁed version of the Adam (Adaptive moment estimation) optimization algorithm, able to improve the speed of convergence and ﬁnds a better minimum for the loss function com-pared to the original algorithm. The proposed solution borrows some ideas from the momentum based optimizer and the exponential decay technique. The current step size made by Adam to update the parameters is modify in such a way that the new step takes in consideration the direction of the gradient and the previous steps update. We conducted several tests with deep Convolutional Neural Networks in the MNIST data. The results showed that AAdam(Accelerated Adam) outperforms Adam and NAdam (Nesterov ac-celerated Adam). The preliminary evidence suggests that making such a change improves the speed of convergence and the quality of the learned models.},
	language = {en},
	author = {Tato, Ange and Nkambou, Roger},
	year = {2018},
	pages = {4},
	file = {Tato and Nkambou - 2018 - IMPROVING ADAM OPTIMIZER.pdf:/home/hell/Zotero/storage/WFR5P2RU/Tato and Nkambou - 2018 - IMPROVING ADAM OPTIMIZER.pdf:application/pdf},
}

@techreport{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	number = {arXiv:1412.6980},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	doi = {10.48550/arXiv.1412.6980},
	note = {arXiv:1412.6980 [cs]
type: article},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/3XD3F6KF/1412.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/8KJ6ACX9/1412.html:text/html},
}

@incollection{robert_monte_1999,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {Monte {Carlo} {Integration}},
	isbn = {978-1-4757-3071-5},
	url = {https://doi.org/10.1007/978-1-4757-3071-5_3},
	abstract = {Two major classes of numerical problems that arise in statistical inference are optimization problems and integration problems. (An associated problem, that of implicit equations, can often be reformulated as an optimization problem.) Although optimization is generally associated with the likelihood approach, and integration with the Bayesian approach, these are not strict classifications, as shown by Examples 1.2.2 and 1.3.5, and Examples 3.1.1, 3.1.2 and 3.1.3, respectively.},
	language = {en},
	urldate = {2022-05-26},
	booktitle = {Monte {Carlo} {Statistical} {Methods}},
	publisher = {springer},
	author = {Robert, Christian P. and Casella, George},
	editor = {Robert, Christian P. and Casella, George},
	year = {1999},
	doi = {10.1007/978-1-4757-3071-5_3},
	keywords = {Importance Sampling, Edgeworth Expansion, Laplace Approximation, Monte CARLO Integration, Saddlepoint Approximation},
	pages = {71--138},
}

@article{morokoff_quasi-monte_1995,
	title = {Quasi-{Monte} {Carlo} {Integration}},
	volume = {122},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999185712090},
	doi = {10.1006/jcph.1995.1209},
	abstract = {The standard Monte Carlo approach to evaluating multidimensional integrals using (pseudo)-random integration nodes is frequently used when quadrature methods are too difficult or expensive to implement. As an alternative to the random methods, it has been suggested that lower error and improved convergence may be obtained by replacing the pseudo-random sequences with more uniformly distributed sequences known as quasi-random. In this paper quasi-random (Halton, Sobol', and Faure) and pseudo-random sequences are compared in computational experiments designed to determine the effects on convergence of certain properties of the integrand, including variance, variation, smoothness, and dimension. The results show that variation, which plays an important role in the theoretical upper bound given by the Koksma-Hlawka inequality, does not affect convergence, while variance, the determining factor in random Monte Carlo, is shown to provide a rough upper bound, but does not accurately predict performance. In general, quasi-Monte Carlo methods are superior to random Monte Carlo, but the advantage may be slight, particularly in high dimensions or for integrands that are not smooth. For discontinuous integrands, we derive a bound which shows that the exponent for algebraic decay of the integration error from quasi-Monte Carlo is only slightly larger than 12 in high dimensions.},
	language = {en},
	number = {2},
	urldate = {2022-05-26},
	journal = {J. Comput. Phys.},
	author = {Morokoff, William J. and Caflisch, Russel E.},
	month = dec,
	year = {1995},
	pages = {218--230},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/BHEWZ9JW/S0021999185712090.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/2DXQHDCK/S0021999185712090.html:text/html},
}

@techreport{bajaj_robust_2021,
	title = {Robust {Learning} of {Physics} {Informed} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2110.13330},
	abstract = {Physics-informed Neural Networks (PINNs) have been shown to be effective in solving partial differential equations by capturing the physics induced constraints as a part of the training loss function. This paper shows that a PINN can be sensitive to errors in training data and overfit itself in dynamically propagating these errors over the domain of the solution of the PDE. It also shows how physical regularizations based on continuity criteria and conservation laws fail to address this issue and rather introduce problems of their own causing the deep network to converge to a physics-obeying local minimum instead of the global minimum. We introduce Gaussian Process (GP) based smoothing that recovers the performance of a PINN and promises a robust architecture against noise/errors in measurements. Additionally, we illustrate an inexpensive method of quantifying the evolution of uncertainty based on the variance estimation of GPs on boundary data. Robust PINN performance is also shown to be achievable by choice of sparse sets of inducing points based on sparsely induced GPs. We demonstrate the performance of our proposed methods and compare the results from existing benchmark models in literature for time-dependent Schr{\textbackslash}"odinger and Burgers' equations.},
	number = {arXiv:2110.13330},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Bajaj, Chandrajit and McLennan, Luke and Andeen, Timothy and Roy, Avik},
	month = oct,
	year = {2021},
	doi = {10.48550/arXiv.2110.13330},
	note = {arXiv:2110.13330 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/I26KZRFY/2110.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/QDZ85MI4/2110.html:text/html},
}

@article{arzani_uncovering_2021,
	title = {Uncovering near-wall blood flow from sparse data with physics-informed neural networks},
	volume = {33},
	issn = {1070-6631},
	url = {https://aip.scitation.org/doi/abs/10.1063/5.0055600},
	doi = {10.1063/5.0055600},
	number = {7},
	urldate = {2022-05-26},
	journal = {Phys. Fluids},
	author = {Arzani, Amirhossein and Wang, Jian-Xun and D'Souza, Roshan M.},
	month = jul,
	year = {2021},
	note = {Publisher: American Institute of Physics},
	pages = {071905},
}

@techreport{gopakumar_loss_2022,
	title = {Loss {Landscape} {Engineering} via {Data} {Regulation} on {PINNs}},
	url = {http://arxiv.org/abs/2205.07843},
	abstract = {Physics-Informed Neural Networks have shown unique utility in parameterising the solution of a well-defined partial differential equation using automatic differentiation and residual losses. Though they provide theoretical guarantees of convergence, in practice the required training regimes tend to be exacting and demanding. Through the course of this paper, we take a deep dive into understanding the loss landscapes associated with a PINN and how that offers some insight as to why PINNs are fundamentally hard to optimise for. We demonstrate how PINNs can be forced to converge better towards the solution, by way of feeding in sparse or coarse data as a regulator. The data regulates and morphs the topology of the loss landscape associated with the PINN to make it easily traversable for the minimiser. Data regulation of PINNs helps ease the optimisation required for convergence by invoking a hybrid unsupervised-supervised training approach, where the labelled data pushes the network towards the vicinity of the solution, and the unlabelled regime fine-tunes it to the solution.},
	number = {arXiv:2205.07843},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Gopakumar, Vignesh and Pamela, Stanislas and Samaddar, Debasmita},
	month = may,
	year = {2022},
	note = {arXiv:2205.07843 [physics]
type: article},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/JGBKEL8X/2205.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/5BUAL58U/2205.html:text/html},
}

@article{andonie_hyperparameter_2019,
	title = {Hyperparameter optimization in learning systems},
	volume = {1},
	issn = {2523-8914},
	url = {https://doi.org/10.1007/s41965-019-00023-0},
	doi = {10.1007/s41965-019-00023-0},
	abstract = {While the training parameters of machine learning models are adapted during the training phase, the values of the hyperparameters (or meta-parameters) have to be specified before the learning phase. The goal is to find a set of hyperparameter values which gives us the best model for our data in a reasonable amount of time. We present an integrated view of methods used in hyperparameter optimization of learning systems, with an emphasis on computational complexity aspects. Our thesis is that we should solve a hyperparameter optimization problem using a combination of techniques for: optimization, search space and training time reduction. Case studies from real-world applications illustrate the practical aspects. We create the framework for a future separation between parameters and hyperparameters in adaptive P systems.},
	language = {en},
	number = {4},
	urldate = {2022-05-25},
	journal = {J. Membrane Comput.},
	author = {Andonie, Răzvan},
	month = dec,
	year = {2019},
	keywords = {Machine learning, Hyperparameters, Membrane computing, Neural computing, P system, Spiking neural P system},
	pages = {279--291},
}

@article{raschka_machine_2020,
	title = {Machine {Learning} in {Python}: {Main} {Developments} and {Technology} {Trends} in {Data} {Science}, {Machine} {Learning}, and {Artificial} {Intelligence}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	shorttitle = {Machine {Learning} in {Python}},
	url = {https://www.mdpi.com/2078-2489/11/4/193},
	doi = {10.3390/info11040193},
	abstract = {Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.},
	language = {en},
	number = {4},
	urldate = {2022-05-25},
	journal = {Inf.},
	author = {Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
	month = apr,
	year = {2020},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, data science, GPU computing, machine learning, neural networks, Python},
	pages = {193},
	file = {Snapshot:/home/hell/Zotero/storage/AU7U9RVZ/htm.html:text/html;Snapshot:/home/hell/Zotero/storage/RLJDAMJT/htm.html:text/html},
}

@article{chen_universal_1995,
	title = {Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems},
	volume = {6},
	issn = {1941-0093},
	doi = {10.1109/72.392253},
	abstract = {The purpose of this paper is to investigate neural network capability systematically. The main results are: 1) every Tauber-Wiener function is qualified as an activation function in the hidden layer of a three-layered neural network; 2) for a continuous function in S'(R/sup 1/) to be a Tauber-Wiener function, the necessary and sufficient condition is that it is not a polynomial; 3) the capability of approximating nonlinear functionals defined on some compact set of a Banach space and nonlinear operators has been shown; and 4) the possibility by neural computation to approximate the output as a whole (not at a fixed point) of a dynamical system, thus identifying the system.{\textless}{\textgreater}},
	number = {4},
	journal = {IEEE Trans. Neural Netw.},
	author = {Chen, Tianping and Chen, Hong},
	month = jul,
	year = {1995},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Neural networks, Mathematics, Computer networks, H infinity control, Integral equations, Kernel, Nonlinear dynamical systems, Polynomials, Sufficient conditions, Sun},
	pages = {911--917},
	file = {IEEE Xplore Abstract Record:/home/hell/Zotero/storage/XKM28H4V/392253.html:text/html;IEEE Xplore Abstract Record:/home/hell/Zotero/storage/9PQ8RYL7/392253.html:text/html},
}

@misc{noauthor_stiff_nodate,
	title = {Stiff {Differential} {Equations}},
	url = {https://uk.mathworks.com/company/newsletters/articles/stiff-differential-equations.html},
	abstract = {Stiffness is a subtle, difficult, and important - concept in the numerical solution of ordinary differential equations.},
	language = {en},
	urldate = {2022-05-25},
	file = {Snapshot:/home/hell/Zotero/storage/IRE4I7XU/stiff-differential-equations.html:text/html},
}

@techreport{zubov_neuralpde_2021,
	title = {{NeuralPDE}: {Automating} {Physics}-{Informed} {Neural} {Networks} ({PINNs}) with {Error} {Approximations}},
	shorttitle = {{NeuralPDE}},
	url = {http://arxiv.org/abs/2107.09443},
	abstract = {Physics-informed neural networks (PINNs) are an increasingly powerful way to solve partial differential equations, generate digital twins, and create neural surrogates of physical models. In this manuscript we detail the inner workings of NeuralPDE.jl and show how a formulation structured around numerical quadrature gives rise to new loss functions which allow for adaptivity towards bounded error tolerances. We describe the various ways one can use the tool, detailing mathematical techniques like using extended loss functions for parameter estimation and operator discovery, to help potential users adopt these PINN-based techniques into their workflow. We showcase how NeuralPDE uses a purely symbolic formulation so that all of the underlying training code is generated from an abstract formulation, and show how to make use of GPUs and solve systems of PDEs. Afterwards we give a detailed performance analysis which showcases the trade-off between training techniques on a large set of PDEs. We end by focusing on a complex multiphysics example, the Doyle-Fuller-Newman (DFN) Model, and showcase how this PDE can be formulated and solved with NeuralPDE. Together this manuscript is meant to be a detailed and approachable technical report to help potential users of the technique quickly get a sense of the real-world performance trade-offs and use cases of the PINN techniques.},
	number = {arXiv:2107.09443},
	urldate = {2022-05-24},
	institution = {arXiv},
	author = {Zubov, Kirill and McCarthy, Zoe and Ma, Yingbo and Calisto, Francesco and Pagliarino, Valerio and Azeglio, Simone and Bottero, Luca and Luján, Emmanuel and Sulzer, Valentin and Bharambe, Ashutosh and Vinchhi, Nand and Balakrishnan, Kaushik and Upadhyay, Devesh and Rackauckas, Chris},
	month = jul,
	year = {2021},
	note = {arXiv:2107.09443 [cs]
type: article},
	keywords = {Computer Science - Mathematical Software, Computer Science - Symbolic Computation},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/EJNTE3Y7/2107.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/PH69QAAI/2107.html:text/html},
}

@techreport{mcclenny_tensordiffeq_2021,
	title = {{TensorDiffEq}: {Scalable} {Multi}-{GPU} {Forward} and {Inverse} {Solvers} for {Physics} {Informed} {Neural} {Networks}},
	shorttitle = {{TensorDiffEq}},
	url = {http://arxiv.org/abs/2103.16034},
	abstract = {Physics-Informed Neural Networks promise to revolutionize science and engineering practice, by introducing domain-aware deep machine learning models into scientific computation. Several software suites have emerged to make the implementation and usage of these architectures available to the research and industry communities. Here we introduce{\textbackslash}linebreak TensorDiffEq, built on Tensorflow 2.x, which presents an intuitive Keras-like interface for problem domain definition, model definition, and solution of forward and inverse problems using physics-aware deep learning methods. TensorDiffEq takes full advantage of Tensorflow 2.x infrastructure for deployment on multiple GPUs, allowing the implementation of large high-dimensional and complex models. Simultaneously, TensorDiffEq supports the Keras API for custom neural network architecture definitions. In the case of smaller or simpler models, the package allows for rapid deployment on smaller-scale CPU platforms with negligible changes to the implementation scripts. We demonstrate the basic usage and capabilities of TensorDiffEq in solving forward, inverse, and data assimilation problems of varying sizes and levels of complexity. The source code is available at https://github.com/tensordiffeq.},
	number = {arXiv:2103.16034},
	urldate = {2022-05-24},
	institution = {arXiv},
	author = {McClenny, Levi D. and Haile, Mulugeta A. and Braga-Neto, Ulisses M.},
	month = mar,
	year = {2021},
	note = {arXiv:2103.16034 [physics]
type: article},
	keywords = {Physics - Computational Physics, Computer Science - Mathematical Software},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/KSR9L4NP/2103.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/ZJQJUG9S/2103.html:text/html},
}

@article{haghighat_sciann_2021,
	title = {{SciANN}: {A} {Keras}/{TensorFlow} wrapper for scientific computations and physics-informed deep learning using artificial neural networks},
	volume = {373},
	issn = {0045-7825},
	shorttitle = {{SciANN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782520307374},
	doi = {10.1016/j.cma.2020.113552},
	abstract = {In this paper, we introduce SciANN, a Python package for scientific computing and physics-informed deep learning using artificial neural networks. SciANN uses the widely used deep-learning packages TensorFlow and Keras to build deep neural networks and optimization models, thus inheriting many of Keras’s functionalities, such as batch optimization and model reuse for transfer learning. SciANN is designed to abstract neural network construction for scientific computations and solution and discovery of partial differential equations (PDE) using the physics-informed neural networks (PINN) architecture, therefore providing the flexibility to set up complex functional forms. We illustrate, in a series of examples, how the framework can be used for curve fitting on discrete data, and for solution and discovery of PDEs in strong and weak forms. We summarize the features currently available in SciANN, and also outline ongoing and future developments.},
	language = {en},
	urldate = {2022-05-24},
	journal = {Comput. Methods Appl. Mech. Eng.},
	author = {Haghighat, Ehsan and Juanes, Ruben},
	month = jan,
	year = {2021},
	keywords = {Deep neural networks, PINN, SciANN, Scientific computations, vPINN},
	pages = {113552},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/D58EHQLG/S0045782520307374.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/LRB46DB8/S0045782520307374.html:text/html},
}

@article{bonkile_systematic_2018,
	title = {A systematic literature review of {Burgers}’ equation with recent advances},
	volume = {90},
	issn = {0973-7111},
	url = {https://doi.org/10.1007/s12043-018-1559-4},
	doi = {10.1007/s12043-018-1559-4},
	abstract = {Even if numerical simulation of the Burgers’ equation is well documented in the literature, a detailed literature survey indicates that gaps still exist for comparative discussion regarding the physical and mathematical significance of the Burgers’ equation. Recently, an increasing interest has been developed within the scientific community, for studying non-linear convective–diffusive partial differential equations partly due to the tremendous improvement in computational capacity. Burgers’ equation whose exact solution is well known, is one of the famous non-linear partial differential equations which is suitable for the analysis of various important areas. A brief historical review of not only the mathematical, but also the physical significance of the solution of Burgers’ equation is presented, emphasising current research strategies, and the challenges that remain regarding the accuracy, stability and convergence of various schemes are discussed. One of the objectives of this paper is to discuss the recent developments in mathematical modelling of Burgers’ equation and thus open doors for improvement. No claim is made that the content of the paper is new. However, it is a sincere effort to outline the physical and mathematical importance of Burgers’ equation in the most simplified ways. We throw some light on the plethora of challenges which need to be overcome in the research areas and give motivation for the next breakthrough to take place in a numerical simulation of ordinary / partial differential equations.},
	language = {en},
	number = {6},
	urldate = {2022-05-24},
	journal = {Pramana - J Phys},
	author = {Bonkile, Mayur P. and Awasthi, Ashish and Lakshmi, C. and Mukundan, Vijitha and Aswin, V. S.},
	month = apr,
	year = {2018},
	keywords = {11.30.Hv, 12.10.Dm, 12.60.Jv, 98.80.Cq, Burgers’ equation, Hopf–Cole transformation, non-linear convection–diffusion equation, numerical solutions},
	pages = {69},
}

@article{tan_review_2019,
	title = {Review of second-order optimization techniques in artificial neural networks backpropagation},
	volume = {495},
	issn = {1757-899X},
	url = {https://doi.org/10.1088/1757-899x/495/1/012003},
	doi = {10.1088/1757-899X/495/1/012003},
	abstract = {Second-order optimization technique is the advances of first-order optimization in neural networks. It provides an addition curvature information of an objective function that adaptively estimate the step-length of optimization trajectory in training phase of neural network. With the additional information, it reduces training iteration and achieves fast convergence with less tuning of hyper-parameter. The current improved memory allocation and computing power further motivates machine learning practitioners to revisit the benefits of second-order optimization techniques. This paper covers the review on second-order optimization techniques that involve Hessian calculation for neural network training. It reviews the basic theory of Newton method, quasi-Newton, Gauss-Newton, Levenberg-Marquardt, Approximate Greatest Descent and Hessian-Free optimization. This paper summarizes the feasibility and performance of optimization techniques in deep neural network training. Comments and suggestions are highlighted for second-order optimization techniques in artificial neural network training in term of advantages and limitations.},
	language = {en},
	urldate = {2022-05-24},
	journal = {IOP Conf. Ser. Mater. Sci. Eng.},
	author = {Tan, Hong Hui and Lim, King Hann},
	month = jun,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {012003},
}

@incollection{fletcher_overview_1994,
	address = {Dordrecht},
	series = {{NATO} {ASI} {Series}},
	title = {An {Overview} of {Unconstrained} {Optimization}},
	isbn = {978-94-009-0369-2},
	url = {https://doi.org/10.1007/978-94-009-0369-2_5},
	abstract = {Developments in the theory and practice of unconstrained optimization are described. Both line search and trust region prototypes are explained and various algorithms based on the use of a quadratic model are outlined. The properties and implementation of the BFGS method are described in some detail and the current preference for this approach is discussed. Various conjugate gradient methods for large unstructured systems are given, but it is argued that limited memory methods are considerably more effective without a great increase in storage or time. Further developments in this area are described. For structured problems the possibilities for updates that retain sparsity are described, including a recent proposal which maintains positive definite matrices and reduces to the BFGS update in the dense case. The alternative use of structure in partially separable optimization is also discussed},
	language = {en},
	urldate = {2022-05-24},
	booktitle = {Algorithms for {Continuous} {Optimization}: {The} {State} of the {Art}},
	publisher = {Springer Netherlands},
	author = {Fletcher, R.},
	editor = {Spedicato, Emilio},
	year = {1994},
	doi = {10.1007/978-94-009-0369-2_5},
	keywords = {Conjugate Gradient Method, Line Search, Trust Region, Trust Region Method, Unconstrained Optimization},
	pages = {109--143},
}

@article{sun_optimization_2020,
	title = {Optimization for {Deep} {Learning}: {An} {Overview}},
	volume = {8},
	issn = {2194-6698},
	shorttitle = {Optimization for {Deep} {Learning}},
	url = {https://doi.org/10.1007/s40305-020-00309-6},
	doi = {10.1007/s40305-020-00309-6},
	abstract = {Optimization is a critical component in deep learning. We think optimization for neural networks is an interesting topic for theoretical research due to various reasons. First, its tractability despite non-convexity is an intriguing question and may greatly expand our understanding of tractable problems. Second, classical optimization theory is far from enough to explain many phenomena. Therefore, we would like to understand the challenges and opportunities from a theoretical perspective and review the existing research in this field. First, we discuss the issue of gradient explosion/vanishing and the more general issue of undesirable spectrum and then discuss practical solutions including careful initialization, normalization methods and skip connections. Second, we review generic optimization methods used in training neural networks, such as stochastic gradient descent and adaptive gradient methods, and existing theoretical results. Third, we review existing research on the global issues of neural network training, including results on global landscape, mode connectivity, lottery ticket hypothesis and neural tangent kernel.},
	language = {en},
	number = {2},
	urldate = {2022-05-24},
	journal = {J. Oper. Res. Soc. China},
	author = {Sun, Ruo-Yu},
	month = jun,
	year = {2020},
	keywords = {Neural networks, Deep learning, Convergence, 68Q32, 90C30, Landscape, Non-convex optimization},
	pages = {249--294},
}

@inproceedings{sitzmann_implicit_2020,
	title = {Implicit {Neural} {Representations} with {Periodic} {Activation} {Functions}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/53c04118df112c13a8c34b38343b9c10-Abstract.html},
	abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or SIRENs, are ideally suited for representing complex natural signals and their derivatives. We analyze SIREN activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how SIRENs can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions.},
	urldate = {2022-05-23},
	booktitle = {Adv. Neural Inf. Process. Syst.},
	publisher = {Curran Assoc., Inc.},
	author = {Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
	year = {2020},
	pages = {7462--7473},
}

@inproceedings{tancik_fourier_2020,
	title = {Fourier {Features} {Let} {Networks} {Learn} {High} {Frequency} {Functions} in {Low} {Dimensional} {Domains}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/55053683268957697aa39fba6f231c68-Abstract.html},
	abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP has impractically slow convergence to high frequency signal components. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
	urldate = {2022-05-23},
	booktitle = {Adv. Neural Inf. Process. Syst.},
	publisher = {Curran Assoc., Inc.},
	author = {Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
	year = {2020},
	pages = {7537--7547},
}

@inproceedings{rahaman_spectral_2019,
	title = {On the {Spectral} {Bias} of {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v97/rahaman19a.html},
	abstract = {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100\% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we highlight a learning bias of deep networks towards low frequency functions – i.e. functions that vary globally without local fluctuations – which manifests itself as a frequency-dependent learning speed. Intuitively, this property is in line with the observation that over-parameterized networks prioritize learning simple patterns that generalize across data samples. We also investigate the role of the shape of the data manifold by presenting empirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold complexity.},
	language = {en},
	urldate = {2022-05-23},
	booktitle = {Proc. 36th Int. Conf. Mach. Learn.},
	publisher = {PMLR},
	author = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5301--5310},
}

@article{yu_review_2019,
	title = {A {Review} of {Recurrent} {Neural} {Networks}: {LSTM} {Cells} and {Network} {Architectures}},
	volume = {31},
	issn = {0899-7667},
	shorttitle = {A {Review} of {Recurrent} {Neural} {Networks}},
	url = {https://doi.org/10.1162/neco_a_01199},
	doi = {10.1162/neco_a_01199},
	abstract = {Recurrent neural networks (RNNs) have been widely adopted in research areas concerned with sequential data, such as text, audio, and video. However, RNNs consisting of sigma cells or tanh cells are unable to learn the relevant information of input data when the input gap is large. By introducing gate functions into the cell structure, the long short-term memory (LSTM) could handle the problem of long-term dependencies well. Since its introduction, almost all the exciting results based on RNNs have been achieved by the LSTM. The LSTM has become the focus of deep learning. We review the LSTM cell and its variants to explore the learning capacity of the LSTM cell. Furthermore, the LSTM networks are divided into two broad categories: LSTM-dominated networks and integrated LSTM networks. In addition, their various applications are discussed. Finally, future research directions are presented for LSTM networks.},
	number = {7},
	urldate = {2022-05-23},
	journal = {Neural Comput.},
	author = {Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
	month = jul,
	year = {2019},
	pages = {1235--1270},
	file = {Snapshot:/home/hell/Zotero/storage/XRBCDDHY/A-Review-of-Recurrent-Neural-Networks-LSTM-Cells.html:text/html;Snapshot:/home/hell/Zotero/storage/4A7VJSC7/A-Review-of-Recurrent-Neural-Networks-LSTM-Cells.html:text/html},
}

@article{malek_numerical_2006,
	title = {Numerical solution for high order differential equations using a hybrid neural network—{Optimization} method},
	volume = {183},
	issn = {0096-3003},
	url = {https://www.sciencedirect.com/science/article/pii/S0096300306005583},
	doi = {10.1016/j.amc.2006.05.068},
	abstract = {This paper reports a novel hybrid method based on optimization techniques and neural networks methods for the solution of high order ordinary differential equations. Here neural networks is considered as a part of large field called neural computing or soft computing. This means that we propose a new solution method for the approximated solution of high order ordinary differential equations using innovative mathematical tools and neural-like systems of computation. This hybrid method can result in improved numerical methods for solving initial/boundary value problems, without using preassigned discretisation points. The mixture of feed forward neural networks and optimization techniques, based on Nelder–Mead method is used to introduce the close analytic form of the solution for the differential equation. Excellent test results are obtained for the solution of lower and higher order differential equations. The model finds approximation solution for the differential equation inside and outside the domain of consideration for the close enough neighborhood of initial/boundary points. Numerical examples are described to demonstrate the method.},
	language = {en},
	number = {1},
	urldate = {2022-05-23},
	journal = {Appl. Math. Comput.},
	author = {Malek, A. and Shekari Beidokhti, R.},
	month = dec,
	year = {2006},
	keywords = {Feed forward artificial neural networks, Multidimensional optimization, Nelder–Mead method, Ordinary differential equations},
	pages = {260--271},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/N23IBHMN/S0096300306005583.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/NLWSQH6P/S0096300306005583.html:text/html},
}

@article{lee_neural_1990,
	title = {Neural algorithm for solving differential equations},
	volume = {91},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/002199919090007N},
	doi = {10.1016/0021-9991(90)90007-N},
	abstract = {Finite difference equations are considered to solve differential equations numerically by utilizing minimization algorithms. Neural minimization algorithms for solving the finite difference equations are presented. Results of numerical simulation are described to demonstrate the method. Methods of implementing the algorithms are discussed. General features of the neural algorithms are discussed.},
	language = {en},
	number = {1},
	urldate = {2022-05-23},
	journal = {J. Comput. Phys.},
	author = {Lee, Hyuk and Kang, In Seok},
	month = nov,
	year = {1990},
	pages = {110--131},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/GJ4LTHPP/002199919090007N.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/5CISLRNL/002199919090007N.html:text/html},
}

@article{rudd_constrained_2015,
	title = {A constrained integration ({CINT}) approach to solving partial differential equations using artificial neural networks},
	volume = {155},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S092523121401652X},
	doi = {10.1016/j.neucom.2014.11.058},
	abstract = {This paper presents a novel constrained integration (CINT) method for solving initial boundary value partial differential equations (PDEs). The CINT method combines classical Galerkin methods with a constrained backpropogation training approach to obtain an artificial neural network representation of the PDE solution that approximately satisfies the boundary conditions at every integration step. The advantage of CINT over existing methods is that it is readily applicable to solving PDEs on irregular domains, and requires no special modification for domains with complex geometries. Furthermore, the CINT method provides a semi-analytical solution that is infinitely differentiable. In this paper the CINT method is demonstrated on two hyperbolic and one parabolic initial boundary value problems with a known analytical solutions that can be used for performance comparison. The numerical results show that, when compared to the most efficient finite element methods, the CINT method achieves significant improvements both in terms of computational time and accuracy.},
	language = {en},
	urldate = {2022-05-23},
	journal = {Neurocomputing},
	author = {Rudd, Keith and Ferrari, Silvia},
	month = may,
	year = {2015},
	keywords = {Partial differential equations, Neural networks, Galerkin methods, Initial-boundary value problem, Irregular domains, Spectral methods},
	pages = {277--285},
}

@article{lagaris_neural-network_2000,
	title = {Neural-network methods for boundary value problems with irregular boundaries},
	volume = {11},
	issn = {1941-0093},
	doi = {10.1109/72.870037},
	abstract = {Partial differential equations (PDEs) with boundary conditions (Dirichlet or Neumann) defined on boundaries with simple geometry have been successfully treated using sigmoidal multilayer perceptrons in previous works. The article deals with the case of complex boundary geometry, where the boundary is determined by a number of points that belong to it and are closely located, so as to offer a reasonable representation. Two networks are employed: a multilayer perceptron and a radial basis function network. The later is used to account for the exact satisfaction of the boundary conditions. The method has been successfully tested on two-dimensional and three-dimensional PDEs and has yielded accurate results.},
	number = {5},
	journal = {IEEE Trans. Neural Netw.},
	author = {Lagaris, I.E. and Likas, A.C. and Papageorgiou, D.G.},
	month = sep,
	year = {2000},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Neural networks, Shape, Boundary conditions, Boundary value problems, Concurrent computing, Differential equations, Geometry, Multilayer perceptrons, Radial basis function networks, Testing},
	pages = {1041--1049},
	file = {IEEE Xplore Abstract Record:/home/hell/Zotero/storage/ZYZEAQ5P/870037.html:text/html;IEEE Xplore Abstract Record:/home/hell/Zotero/storage/64IX44ML/870037.html:text/html},
}

@article{sirignano_dgm_2018,
	title = {{DGM}: {A} deep learning algorithm for solving partial differential equations},
	volume = {375},
	issn = {0021-9991},
	shorttitle = {{DGM}},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118305527},
	doi = {10.1016/j.jcp.2018.08.029},
	abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve high-dimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the differential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton–Jacobi–Bellman PDE and Burgers' equation. The deep learning algorithm approximates the general solution to the Burgers' equation for a continuum of different boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method (DGM)” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.},
	language = {en},
	urldate = {2022-05-23},
	journal = {J. Comput. Phys.},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	month = dec,
	year = {2018},
	keywords = {Partial differential equations, Machine learning, Deep learning, High-dimensional partial differential equations},
	pages = {1339--1364},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/33J7JLFR/S0021999118305527.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/N2Z97EEQ/S0021999118305527.html:text/html},
}

@incollection{kollmannsberger_physics-informed_2021,
	address = {Cham},
	series = {Studies in {Computational} {Intelligence}},
	title = {Physics-{Informed} {Neural} {Networks}},
	isbn = {978-3-030-76587-3},
	url = {https://doi.org/10.1007/978-3-030-76587-3_5},
	abstract = {Physics-informed neural networks (PINNs) are used for problems where data are scarce. The underlying physics is enforced via the governing differential equation, including the residual in the cost function. PINNs can be used for both solving and discovering differential equations. In this chapter, PINNs are illustrated with three one-dimensional examples. The first example shows how the displacement of a static bar can be computed. The temperature evolution in a one-dimensional spatial domain is determined using the non-linear heat equation, using both a continuous and a discrete approach. Finally, the data-driven identification is illustrated with the static bar, where the cross-sectional stiffness is estimated from the displacement.},
	language = {en},
	urldate = {2022-05-23},
	booktitle = {Deep {Learning} in {Computational} {Mechanics}: {An} {Introductory} {Course}},
	publisher = {Springer Int. Pub.},
	author = {Kollmannsberger, Stefan and D’Angella, Davide and Jokeit, Moritz and Herrmann, Leon},
	editor = {Kollmannsberger, Stefan and D'Angella, Davide and Jokeit, Moritz and Herrmann, Leon},
	year = {2021},
	doi = {10.1007/978-3-030-76587-3_5},
	pages = {55--84},
}

@article{owhadi_bayesian_2015,
	title = {Bayesian {Numerical} {Homogenization}},
	volume = {13},
	issn = {1540-3459},
	url = {https://epubs.siam.org/doi/abs/10.1137/140974596},
	doi = {10.1137/140974596},
	abstract = {Numerical homogenization, i.e., the finite-dimensional approximation of solution spaces of PDEs with arbitrary rough coefficients, requires the identification of accurate basis elements. These basis elements are oftentimes found after a laborious process of scientific investigation and plain guesswork. Can this identification problem be facilitated? Is there a general recipe/decision framework for guiding the design of basis elements? We suggest that the answer to the above questions could be positive based on the reformulation of numerical homogenization as a Bayesian inference problem in which a given PDE with rough coefficients (or multiscale operator) is excited with noise (random right-hand side/source term) and one tries to estimate the value of the solution at a given point based on a finite number of observations. We apply this reformulation to the identification of bases for the numerical homogenization of arbitrary integro-differential equations and show that these bases have optimal recovery properties. In particular we show how rough polyharmonic splines can be rediscovered as the optimal solution of a Gaussian filtering problem.},
	number = {3},
	urldate = {2022-05-23},
	journal = {Multiscale Model. Simul.},
	author = {Owhadi, Houman},
	month = jan,
	year = {2015},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {34E13, 41A15, 60H30, 62C10, Bayesian inference, Bayesian numerical analysis, coarse graining, Gaussian filtering, numerical homogenization, polyharmonic splines},
	pages = {812--828},
}

@inproceedings{pascanu_difficulty_2013,
	title = {On the difficulty of training recurrent neural networks},
	url = {https://proceedings.mlr.press/v28/pascanu13.html},
	abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
	language = {en},
	urldate = {2022-05-23},
	booktitle = {Proc. 30th Int. Conf. Mach. Learn.},
	publisher = {PMLR},
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {1310--1318},
}

@article{margossian_review_2019,
	title = {A review of automatic differentiation and its efficient implementation},
	volume = {9},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1305},
	doi = {10.1002/widm.1305},
	abstract = {Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation (AD) is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of AD, however, requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region-based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open and practical problems include the extension of current packages to provide more specialized routines, and finding optimal methods to perform higher-order differentiation. This article is categorized under: Algorithmic Development {\textgreater} Scalable Statistical Methods},
	language = {en},
	number = {4},
	urldate = {2022-05-23},
	journal = {WIREs Data Min. Knowl. Discovery},
	author = {Margossian, Charles C.},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1305},
	keywords = {automatic differentiation, computational statistics, numerical methods},
	pages = {e1305},
	file = {Snapshot:/home/hell/Zotero/storage/72H64383/widm.html:text/html;Snapshot:/home/hell/Zotero/storage/B3B75MXS/widm.html:text/html},
}

@article{lagaris_artificial_1998,
	title = {Artificial neural networks for solving ordinary and partial differential equations},
	volume = {9},
	issn = {1941-0093},
	doi = {10.1109/72.712178},
	abstract = {We present a method to solve initial and boundary value problems using artificial neural networks. A trial solution of the differential equation is written as a sum of two parts. The first part satisfies the initial/boundary conditions and contains no adjustable parameters. The second part is constructed so as not to affect the initial/boundary conditions. This part involves a feedforward neural network containing adjustable parameters (the weights). Hence by construction the initial/boundary conditions are satisfied and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single ordinary differential equations (ODE), to systems of coupled ODE and also to partial differential equations (PDE). In this article, we illustrate the method by solving a variety of model problems and present comparisons with solutions obtained using the Galerkin finite element method for several cases of partial differential equations. With the advent of neuroprocessors and digital signal processors the method becomes particularly interesting due to the expected essential gains in the execution speed.},
	number = {5},
	journal = {IEEE Trans. Neural Netw.},
	author = {Lagaris, I.E. and Likas, A. and Fotiadis, D.I.},
	month = sep,
	year = {1998},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Partial differential equations, Neural networks, Boundary conditions, Boundary value problems, Differential equations, Artificial neural networks, Digital signal processors, Feedforward neural networks, Finite element methods, Moment methods},
	pages = {987--1000},
	file = {IEEE Xplore Abstract Record:/home/hell/Zotero/storage/UVZM9MGA/712178.html:text/html;IEEE Xplore Abstract Record:/home/hell/Zotero/storage/U4ZVZBP4/712178.html:text/html},
}

@article{whytock_dynamic_2014,
	title = {Dynamic {Distance}-{Based} {Shape} {Features} for {Gait} {Recognition}},
	volume = {50},
	issn = {0924-9907, 1573-7683},
	url = {http://link.springer.com/10.1007/s10851-014-0501-8},
	doi = {10.1007/s10851-014-0501-8},
	language = {en},
	number = {3},
	urldate = {2022-04-22},
	journal = {J. Math. Imaging Vision},
	author = {Whytock, Tenika and Belyaev, Alexander and Robertson, Neil M.},
	month = nov,
	year = {2014},
	pages = {314--326},
}

@article{liu_improve_2022,
	title = {Improve the discontinuity capturing ability of physics-informed neural network},
	url = {https://www.techrxiv.org/articles/preprint/Improve_the_discontinuity_capturing_ability_of_physics-informed_neural_network/19391279/1},
	doi = {10.36227/techrxiv.19391279.v1},
	abstract = {In this paper, we proposed a method to improve the discontinuities (especially shock waves) capturing ability of physics-informed-neural-networks (PINN) in simulating hyperbolic equations. The main idea of the method is to weaken the influence of the points inside discontinuities which can not be expressed directly by the differential equations theoretically, and may let the trainings fall into a confrontation with the physics compressible effect. In this work, we add a weight to each point which is related to the gradient locally, then the network can focus on training the ‘differential equations expressed points’(smooth points). Automatically by the physical compressible effect, all the nearby points will move out of the discontinuously regions, and gain a sharp and exact result automatically with the physical process inside the training},
	language = {en},
	urldate = {2022-04-07},
	author = {Liu, Li},
	month = mar,
	year = {2022},
	note = {Publisher: TechRxiv},
	file = {Snapshot:/home/hell/Zotero/storage/MDC3NALB/1.html:text/html;Snapshot:/home/hell/Zotero/storage/GWP77RLX/1.html:text/html},
}

@article{bard_neural_2021,
	title = {Neural {Network} {Reconstruction} of {Plasma} {Space}-{Time}},
	volume = {8},
	issn = {2296-987X},
	url = {https://www.frontiersin.org/article/10.3389/fspas.2021.732275},
	abstract = {We explore the use of Physics-Informed Neural Networks (PINNs) for reconstructing full magnetohydrodynamic solutions from partial samples, mimicking the recreation of space-time environments around spacecraft observations. We use one-dimensional magneto- and hydrodynamic benchmarks, namely the Sod, Ryu-Jones, and Brio-Wu shock tubes, to obtain the plasma state variables along linear trajectories in space-time. These simulated spacecraft measurements are used as constraining boundary data for a PINN which incorporates the full set of one-dimensional (magneto) hydrodynamics equations in its loss function. We find that the PINN is able to reconstruct the full 1D solution of these shock tubes even in the presence of Gaussian noise. However, our chosen PINN transformer architecture does not appear to scale well to higher dimensions. Nonetheless, PINNs in general could turn out to be a promising mechanism for reconstructing simple magnetic structures and dynamics from satellite observations in geospace.},
	urldate = {2022-04-07},
	journal = {Front. Astron. Space Sci.},
	author = {Bard, C. and Dorelli, J.C.},
	year = {2021},
}

@article{sun_physics-based_2021,
	title = {Physics-{Based} {Deep} {Learning} for {Flow} {Problems}},
	volume = {14},
	issn = {1996-1073},
	url = {https://www.mdpi.com/1996-1073/14/22/7760},
	doi = {10.3390/en14227760},
	abstract = {It is the tradition for the fluid community to study fluid dynamics problems via numerical simulations such as finite-element, finite-difference and finite-volume methods. These approaches use various mesh techniques to discretize a complicated geometry and eventually convert governing equations into finite-dimensional algebraic systems. To date, many attempts have been made by exploiting machine learning to solve flow problems. However, conventional data-driven machine learning algorithms require heavy inputs of large labeled data, which is computationally expensive for complex and multi-physics problems. In this paper, we proposed a data-free, physics-driven deep learning approach to solve various low-speed flow problems and demonstrated its robustness in generating reliable solutions. Instead of feeding neural networks large labeled data, we exploited the known physical laws and incorporated this physics into a neural network to relax the strict requirement of big data and improve prediction accuracy. The employed physics-informed neural networks (PINNs) provide a feasible and cheap alternative to approximate the solution of differential equations with specified initial and boundary conditions. Approximate solutions of physical equations can be obtained via the minimization of the customized objective function, which consists of residuals satisfying differential operators, the initial/boundary conditions as well as the mean-squared errors between predictions and target values. This new approach is data efficient and can greatly lower the computational cost for large and complex geometries. The capacity and generality of the proposed method have been assessed by solving various flow and transport problems, including the flow past cylinder, linear Poisson, heat conduction and the Taylor–Green vortex problem.},
	language = {en},
	number = {22},
	urldate = {2022-04-06},
	journal = {Energies},
	author = {Sun, Yubiao and Sun, Qiankun and Qin, Kan},
	month = nov,
	year = {2021},
	pages = {7760},
	file = {Full Text:/home/hell/Zotero/storage/6P9VMUY5/Sun et al. - 2021 - Physics-Based Deep Learning for Flow Problems.pdf:application/pdf},
}

@article{benson_over-parametrized_2020,
	title = {Over-parametrized neural networks as under-determined linear systems},
	url = {http://arxiv.org/abs/2010.15959},
	abstract = {We draw connections between simple neural networks and under-determined linear systems to comprehensively explore several interesting theoretical questions in the study of neural networks. First, we emphatically show that it is unsurprising such networks can achieve zero training loss. More specifically, we provide lower bounds on the width of a single hidden layer neural network such that only training the last linear layer suffices to reach zero training loss. Our lower bounds grow more slowly with data set size than existing work that trains the hidden layer weights. Second, we show that kernels typically associated with the ReLU activation function have fundamental flaws -- there are simple data sets where it is impossible for widely studied bias-free models to achieve zero training loss irrespective of how the parameters are chosen or trained. Lastly, our analysis of gradient descent clearly illustrates how spectral properties of certain matrices impact both the early iteration and long-term training behavior. We propose new activation functions that avoid the pitfalls of ReLU in that they admit zero training loss solutions for any set of distinct data points and experimentally exhibit favorable spectral properties.},
	urldate = {2022-03-28},
	journal = {arXiv:2010.15959 [cs, math, stat]},
	author = {Benson, Austin R. and Damle, Anil and Townsend, Alex},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.15959},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning, 68Q32, 68T05, 68Q32, 46E22, 46E22, 68T05},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	urldate = {2022-03-28},
	booktitle = {Adv. Neural Inf. Process. Syst.},
	publisher = {Curran Assoc., Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	file = {Full Text PDF:/home/hell/Zotero/storage/AW27CWZN/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf},
}

@book{minsky_perceptrons_2017,
	title = {Perceptrons: {An} {Introduction} to {Computational} {Geometry}},
	isbn = {978-0-262-34393-0},
	shorttitle = {Perceptrons},
	url = {https://direct.mit.edu/books/book/3132/perceptronsan-introduction-to-computational},
	language = {en},
	urldate = {2022-03-28},
	publisher = {MIT Press},
	author = {Minsky, Marvin and Papert, Seymour A.},
	year = {2017},
	doi = {10.7551/mitpress/11301.001.0001},
	file = {Minsky and Papert - 2017 - Perceptrons An Introduction to Computational Geom.pdf:/home/hell/Zotero/storage/W3G45Q8R/Minsky and Papert - 2017 - Perceptrons An Introduction to Computational Geom.pdf:application/pdf},
}

@article{werbos_beyond_1974,
	title = {Beyond {Regression} : {New} {Tools} for {Prediction} and {Analysis} in the {Behavior} {Science}},
	shorttitle = {Beyond {Regression}},
	url = {https://ci.nii.ac.jp/naid/10012540025/},
	urldate = {2022-03-28},
	journal = {Doctoral Dissertation, Harvard University},
	author = {WERBOS, P.},
	year = {1974},
	file = {Beyond Regression \: New Tools for Prediction and Analysis in the Behavior Science Snapshot:/home/hell/Zotero/storage/TI67S2WZ/10012540025.html:text/html},
}

@article{diaconis_nonlinear_1984,
	title = {On {Nonlinear} {Functions} of {Linear} {Combinations}},
	volume = {5},
	issn = {0196-5204},
	url = {https://epubs.siam.org/doi/abs/10.1137/0905013},
	doi = {10.1137/0905013},
	abstract = {Projection pursuit algorithms approximate a function of p variables by a sum of nonlinear functions of linear combinations: {\textbackslash}[ (1){\textbackslash}qquad f{\textbackslash}left( \{x\_1 , {\textbackslash}cdots ,x\_p \} {\textbackslash}right) {\textbackslash}doteq {\textbackslash}sum\_\{i = 1\}{\textasciicircum}n \{g\_i {\textbackslash}left( \{a\_\{i1\} x\_1 + {\textbackslash}cdots + a\_\{ip\} x\_p \} {\textbackslash}right)\} . {\textbackslash}] We develop some approximation theory, give a necessary and sufficient condition for equality in (1), and discuss nonuniqueness of the representation.},
	number = {1},
	urldate = {2022-03-28},
	journal = {SIAM J. Sci. Stat. Comput.},
	author = {Diaconis, Persi and Shahshahani, Mehrdad},
	month = mar,
	year = {1984},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {approximation theory, nonlinear high-dimensional nonparametric regression, polynomials, Schwartz distributions},
	pages = {175--191},
	file = {Submitted Version:/home/hell/Zotero/storage/BQHCT2TB/Diaconis and Shahshahani - 1984 - On Nonlinear Functions of Linear Combinations.pdf:application/pdf},
}

@article{thacker_role_1989,
	title = {The role of the {Hessian} matrix in fitting models to measurements},
	volume = {94},
	issn = {2156-2202},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/JC094iC05p06177},
	doi = {10.1029/JC094iC05p06177},
	abstract = {A numerical model can be fit to data by minimizing a positive quadratic function of the differences between the data and their model counterparts. The rate at which algorithms for computing the best fit to data converge depends on the size of the condition number and the distribution of eigenvalues of the Hessian matrix, which contains the second derivatives of this quadratic function. The inverse of the Hessian can be identified as the covariance matrix that establishes the accuracy to which the model state is determined by the data; the reciprocals of the Hessian's eigenvalues represent the variances of linear combinations of variables determined by its eigenvectors. The aspect of the model state that are most difficult to compute are those about which the data provide the least information. A unified formalism is presented in which the model may be treated as providing either strong or weak constraints, and methods for computing and inverting the Hessian matrix are discussed. Examples are given of the uncertainties resulting from fitting an oceanographic model to several different sets of hypothetical data.},
	language = {en},
	number = {C5},
	urldate = {2022-03-28},
	journal = {J. Geophys. Res.: Oceans},
	author = {Thacker, William Carlisle},
	year = {1989},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/JC094iC05p06177},
	pages = {6177--6196},
	file = {Snapshot:/home/hell/Zotero/storage/RSG9CZA2/JC094iC05p06177.html:text/html;Snapshot:/home/hell/Zotero/storage/2H93T886/JC094iC05p06177.html:text/html},
}

@book{freedman_statistical_2009,
	title = {Statistical {Models}: {Theory} and {Practice}},
	isbn = {978-0-521-11243-7},
	shorttitle = {Statistical {Models}},
	abstract = {This lively and engaging textbook explains the things you have to know in order to read empirical papers in the social and health sciences, as well as the techniques you need to build statistical models of your own. The author, David A. Freedman, explains the basic ideas of association and regression, and takes you through the current models that link these ideas to causality. The focus is on applications of linear models, including generalized least squares and two-stage least squares, with probits and logits for binary variables. The bootstrap is developed as a technique for estimating bias and computing standard errors. Careful attention is paid to the principles of statistical inference. There is background material on study design, bivariate regression, and matrix algebra. To develop technique, there are computer labs with sample computer programs. The book is rich in exercises, most with answers. Target audiences include advanced undergraduates and beginning graduate students in statistics, as well as students and professionals in the social and health sciences. The discussion in the book is organized around published studies, as are many of the exercises. Relevant journal articles are reprinted at the back of the book. Freedman makes a thorough appraisal of the statistical methods in these papers and in a variety of other examples. He illustrates the principles of modeling, and the pitfalls. The discussion shows you how to think about the critical issues - including the connection (or lack of it) between the statistical models and the real phenomena. Features of the book: • authoritative guidance from a well-known author with wide experience in teaching, research, and consulting • careful analysis of statistical issues in substantive applications • no-nonsense, direct style • versatile structure, enabling the text to be used as a text in a course, or read on its own • text that has been thoroughly class-tested at Berkeley • background material on regression and matrix algebra • plenty of exercises, most with solutions • extra material for instructors, including data sets and code for lab projects (available from Cambridge University Press) • many new exercises and examples • reorganized, restructured, and revised chapters to aid teaching and understanding},
	language = {en},
	publisher = {Cambridge Press},
	author = {Freedman, David},
	month = apr,
	year = {2009},
	note = {Google-Books-ID: 4N3KOEitRe8C},
	keywords = {Mathematics / Probability \& Statistics / General},
}

@article{ren_phycrnet_2022,
	title = {{PhyCRNet}: {Physics}-informed convolutional-recurrent network for solving spatiotemporal {PDEs}},
	volume = {389},
	issn = {0045-7825},
	shorttitle = {{PhyCRNet}},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782521006514},
	doi = {10.1016/j.cma.2021.114399},
	abstract = {Partial differential equations (PDEs) play a fundamental role in modeling and simulating problems across a wide range of disciplines. Recent advances in deep learning have shown the great potential of physics-informed neural networks (PINNs) to solve PDEs as a basis for data-driven modeling and inverse analysis. However, the majority of existing PINN methods, based on fully-connected NNs, pose intrinsic limitations to low-dimensional spatiotemporal parameterizations. Moreover, since the initial/boundary conditions (I/BCs) are softly imposed via penalty, the solution quality heavily relies on hyperparameter tuning. To this end, we propose the novel physics-informed convolutional-recurrent learning architectures (PhyCRNet and PhyCRNet-s) for solving PDEs without any labeled data. Specifically, an encoder–decoder convolutional long short-term memory network is proposed for low-dimensional spatial feature extraction and temporal evolution learning. The loss function is defined as the aggregated discretized PDE residuals, while the I/BCs are hard-encoded in the network to ensure forcible satisfaction (e.g., periodic boundary padding). The networks are further enhanced by autoregressive and residual connections that explicitly simulate time marching. The performance of our proposed methods has been assessed by solving three nonlinear PDEs (e.g., 2D Burgers’ equations, the λ-ω and FitzHugh Nagumo reaction–diffusion equations), and compared against the start-of-the-art baseline algorithms. The numerical results demonstrate the superiority of our proposed methodology in the context of solution accuracy, extrapolability and generalizability.},
	language = {en},
	urldate = {2022-03-24},
	journal = {Comput. Methods Appl. Mech. Eng.},
	author = {Ren, Pu and Rao, Chengping and Liu, Yang and Wang, Jian-Xun and Sun, Hao},
	month = feb,
	year = {2022},
	keywords = {Physics-informed deep learning, Partial differential equations, Convolutional-recurrent learning, Encoder–decoder, Hard-encoding of I/BCs, Residual connection},
	pages = {114399},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/2AQHMVQY/S0045782521006514.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/WGRFN7DI/S0045782521006514.html:text/html},
}

@article{jagtap_adaptive_2020,
	title = {Adaptive activation functions accelerate convergence in deep and physics-informed neural networks},
	volume = {404},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999119308411},
	doi = {10.1016/j.jcp.2019.109136},
	abstract = {We employ adaptive activation functions for regression in deep and physics-informed neural networks (PINNs) to approximate smooth and discontinuous functions as well as solutions of linear and nonlinear partial differential equations. In particular, we solve the nonlinear Klein-Gordon equation, which has smooth solutions, the nonlinear Burgers equation, which can admit high gradient solutions, and the Helmholtz equation. We introduce a scalable hyper-parameter in the activation function, which can be optimized to achieve best performance of the network as it changes dynamically the topology of the loss function involved in the optimization process. The adaptive activation function has better learning capabilities than the traditional one (fixed activation) as it improves greatly the convergence rate, especially at early training, as well as the solution accuracy. To better understand the learning process, we plot the neural network solution in the frequency domain to examine how the network captures successively different frequency bands present in the solution. We consider both forward problems, where the approximate solutions are obtained, as well as inverse problems, where parameters involved in the governing equation are identified. Our simulation results show that the proposed method is a very simple and effective approach to increase the efficiency, robustness and accuracy of the neural network approximation of nonlinear functions as well as solutions of partial differential equations, especially for forward problems. We theoretically prove that in the proposed method, gradient descent algorithms are not attracted to suboptimal critical points or local minima. Furthermore, the proposed adaptive activation functions are shown to accelerate the minimization process of the loss values in standard deep learning benchmarks using CIFAR-10, CIFAR-100, SVHN, MNIST, KMNIST, Fashion-MNIST, and Semeion datasets with and without data augmentation.},
	language = {en},
	urldate = {2022-03-18},
	journal = {J. Comput. Phys.},
	author = {Jagtap, Ameya D. and Kawaguchi, Kenji and Karniadakis, George Em},
	month = mar,
	year = {2020},
	keywords = {Partial differential equations, Physics-informed neural networks, Machine learning, Bad minima, Deep learning benchmarks, Inverse problems},
	pages = {109136},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/XJRDWD2U/S0021999119308411.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/7USF6G7U/S0021999119308411.html:text/html},
}

@article{psaros_meta-learning_2022,
	title = {Meta-learning {PINN} loss functions},
	volume = {458},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999122001838},
	doi = {10.1016/j.jcp.2022.111121},
	abstract = {We propose a meta-learning technique for offline discovery of physics-informed neural network (PINN) loss functions. We extend earlier works on meta-learning, and develop a gradient-based meta-learning algorithm for addressing diverse task distributions based on parametrized partial differential equations (PDEs) that are solved with PINNs. Furthermore, based on new theory we identify two desirable properties of meta-learned losses in PINN problems, which we enforce by proposing a new regularization method or using a specific parametrization of the loss function. In the computational examples, the meta-learned losses are employed at test time for addressing regression and PDE task distributions. Our results indicate that significant performance improvement can be achieved by using a shared-among-tasks offline-learned loss function even for out-of-distribution meta-testing. In this case, we solve for test tasks that do not belong to the task distribution used in meta-training, and we also employ PINN architectures that are different from the PINN architecture used in meta-training. To better understand the capabilities and limitations of the proposed method, we consider various parametrizations of the loss function and describe different algorithm design options and how they may affect meta-learning performance.},
	language = {en},
	urldate = {2022-03-17},
	journal = {J. Comput. Phys.},
	author = {Psaros, Apostolos F and Kawaguchi, Kenji and Karniadakis, George Em},
	month = jun,
	year = {2022},
	keywords = {Physics-informed neural networks, Meta-learned loss function, Meta-learning},
	pages = {111121},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/8R6GHMUL/S0021999122001838.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/93IWDVG5/S0021999122001838.html:text/html},
}

@article{huang_parallel_2021,
	title = {Parallel {Physics}-{Informed} {Neural} {Networks} with {Bidirectional} {Balance}},
	url = {http://arxiv.org/abs/2111.05641},
	abstract = {As an emerging technology in deep learning, physics-informed neural networks (PINNs) have been widely used to solve various partial differential equations (PDEs) in engineering. However, PDEs based on practical considerations contain multiple physical quantities and complex initial boundary conditions, thus PINNs often returns incorrect results. Here we take heat transfer problem in multilayer fabrics as a typical example. It is coupled by multiple temperature fields with strong correlation, and the values of variables are extremely unbalanced among different dimensions. We clarify the potential difficulties of solving such problems by classic PINNs, and propose a parallel physics-informed neural networks with bidirectional balance. In detail, our parallel solving framework synchronously fits coupled equations through several multilayer perceptions. Moreover, we design two modules to balance forward process of data and back-propagation process of loss gradient. This bidirectional balance not only enables the whole network to converge stably, but also helps to fully learn various physical conditions in PDEs. We provide a series of ablation experiments to verify the effectiveness of the proposed methods. The results show that our approach makes the PINNs unsolvable problem solvable, and achieves excellent solving accuracy.},
	urldate = {2022-03-17},
	journal = {arXiv:2111.05641 [cs, math]},
	author = {Huang, Yuhao},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.05641},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/UHJ7A8QI/2111.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/WI2LL5QH/2111.html:text/html},
}

@article{hu_when_2021,
	title = {When {Do} {Extended} {Physics}-{Informed} {Neural} {Networks} ({XPINNs}) {Improve} {Generalization}?},
	url = {http://arxiv.org/abs/2109.09444},
	abstract = {Physics-informed neural networks (PINNs) have become a popular choice for solving high-dimensional partial differential equations (PDEs) due to their excellent approximation power and generalization ability. Recently, Extended PINNs (XPINNs) based on domain decomposition methods have attracted considerable attention due to their effectiveness in modeling multiscale and multiphysics problems and their parallelization. However, theoretical understanding on their convergence and generalization properties remains unexplored. In this study, we take an initial step towards understanding how and when XPINNs outperform PINNs. Specifically, for general multi-layer PINNs and XPINNs, we first provide a prior generalization bound via the complexity of the target functions in the PDE problem, and a posterior generalization bound via the posterior matrix norms of the networks after optimization. Moreover, based on our bounds, we analyze the conditions under which XPINNs improve generalization. Concretely, our theory shows that the key building block of XPINN, namely the domain decomposition, introduces a tradeoff for generalization. On the one hand, XPINNs decompose the complex PDE solution into several simple parts, which decreases the complexity needed to learn each part and boosts generalization. On the other hand, decomposition leads to less training data being available in each subdomain, and hence such model is typically prone to overfitting and may become less generalizable. Empirically, we choose five PDEs to show when XPINNs perform better than, similar to, or worse than PINNs, hence demonstrating and justifying our new theory.},
	urldate = {2022-03-17},
	journal = {arXiv:2109.09444 [cs, math, stat]},
	author = {Hu, Zheyuan and Jagtap, Ameya D. and Karniadakis, George Em and Kawaguchi, Kenji},
	month = dec,
	year = {2021},
	note = {arXiv: 2109.09444},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning, Mathematics - Dynamical Systems},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/DY268U5E/2109.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/6PE5542G/2109.html:text/html},
}

@article{jagtap_conservative_2020,
	title = {Conservative physics-informed neural networks on discrete domains for conservation laws: {Applications} to forward and inverse problems},
	volume = {365},
	issn = {0045-7825},
	shorttitle = {Conservative physics-informed neural networks on discrete domains for conservation laws},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782520302127},
	doi = {10.1016/j.cma.2020.113028},
	abstract = {We propose a conservative physics-informed neural network (cPINN) on discrete domains for nonlinear conservation laws. Here, the term discrete domain represents the discrete sub-domains obtained after division of the computational domain, where PINN is applied and the conservation property of cPINN is obtained by enforcing the flux continuity in the strong form along the sub-domain interfaces. In case of hyperbolic conservation laws, the convective flux contributes at the interfaces, whereas in case of viscous conservation laws, both convective and diffusive fluxes contribute. Apart from the flux continuity condition, an average solution (given by two different neural networks) is also enforced at the common interface between two sub-domains. One can also employ a deep neural network in the domain, where the solution may have complex structure, whereas a shallow neural network can be used in the sub-domains with relatively simple and smooth solutions. Another advantage of the proposed method is the additional freedom it gives in terms of the choice of optimization algorithm and the various training parameters like residual points, activation function, width and depth of the network etc. Various forms of errors involved in cPINN such as optimization, generalization and approximation errors and their sources are discussed briefly. In cPINN, locally adaptive activation functions are used, hence training the model faster compared to its fixed counterparts. Both, forward and inverse problems are solved using the proposed method. Various test cases ranging from scalar nonlinear conservation laws like Burgers, Korteweg–de Vries (KdV) equations to systems of conservation laws, like compressible Euler equations are solved. The lid-driven cavity test case governed by incompressible Navier–Stokes equation is also solved and the results are compared against a benchmark solution. The proposed method enjoys the property of domain decomposition with separate neural networks in each sub-domain, and it efficiently lends itself to parallelized computation, where each sub-domain can be assigned to a different computational node.},
	language = {en},
	urldate = {2022-03-17},
	journal = {Comput. Methods Appl. Mech. Eng.},
	author = {Jagtap, Ameya D. and Kharazmi, Ehsan and Karniadakis, George Em},
	month = jun,
	year = {2020},
	keywords = {Machine learning, Inverse problems, Conservation laws, cPINN, Domain decomposition, Mortar PINN},
	pages = {113028},
}

@article{kharazmi_hp-vpinns_2021,
	title = {hp-{VPINNs}: {Variational} physics-informed neural networks with domain decomposition},
	volume = {374},
	issn = {0045-7825},
	shorttitle = {hp-{VPINNs}},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782520307325},
	doi = {10.1016/j.cma.2020.113547},
	abstract = {We formulate a general framework for hp-variational physics-informed neural networks (hp-VPINNs) based on the nonlinear approximation of shallow and deep neural networks and hp-refinement via domain decomposition and projection onto the space of high-order polynomials. The trial space is the space of neural network, which is defined globally over the entire computational domain, while the test space contains piecewise polynomials. Specifically in this study, the hp-refinement corresponds to a global approximation with a local learning algorithm that can efficiently localize the network parameter optimization. We demonstrate the advantages of hp-VPINNs in both accuracy and training cost for several numerical examples of function approximation and in solving differential equations.},
	language = {en},
	urldate = {2022-03-17},
	journal = {Comput. Methods Appl. Mech. Eng.},
	author = {Kharazmi, Ehsan and Zhang, Zhongqiang and Karniadakis, George E. M.},
	month = feb,
	year = {2021},
	keywords = {Partial differential equations, Domain decomposition, -refinement, Automatic differentiation, Physics-informed learning, Variational neural network, VPINNs},
	pages = {113547},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/HDY5WYQ6/S0045782520307325.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/K3W4H6GJ/S0045782520307325.html:text/html},
}

@article{cai_physics-informed_2021,
	title = {Physics-{Informed} {Neural} {Networks} for {Heat} {Transfer} {Problems}},
	volume = {143},
	issn = {0022-1481},
	url = {https://doi.org/10.1115/1.4050542},
	doi = {10.1115/1.4050542},
	abstract = {Physics-informed neural networks (PINNs) have gained popularity across different engineering fields due to their effectiveness in solving realistic problems with noisy data and often partially missing physics. In PINNs, automatic differentiation is leveraged to evaluate differential operators without discretization errors, and a multitask learning problem is defined in order to simultaneously fit observed data while respecting the underlying governing laws of physics. Here, we present applications of PINNs to various prototype heat transfer problems, targeting in particular realistic conditions not readily tackled with traditional computational methods. To this end, we first consider forced and mixed convection with unknown thermal boundary conditions on the heated surfaces and aim to obtain the temperature and velocity fields everywhere in the domain, including the boundaries, given some sparse temperature measurements. We also consider the prototype Stefan problem for two-phase flow, aiming to infer the moving interface, the velocity and temperature fields everywhere as well as the different conductivities of a solid and a liquid phase, given a few temperature measurements inside the domain. Finally, we present some realistic industrial applications related to power electronics to highlight the practicality of PINNs as well as the effective use of neural networks in solving general heat transfer problems of industrial complexity. Taken together, the results presented herein demonstrate that PINNs not only can solve ill-posed problems, which are beyond the reach of traditional computational methods, but they can also bridge the gap between computational and experimental heat transfer.},
	number = {6},
	urldate = {2022-03-17},
	journal = {J. Heat Transfer},
	author = {Cai, Shengze and Wang, Zhicheng and Wang, Sifan and Perdikaris, Paris and Karniadakis, George Em},
	month = apr,
	year = {2021},
}

@article{chen_solving_2021,
	title = {Solving {Inverse} {Stochastic} {Problems} from {Discrete} {Particle} {Observations} {Using} the {Fokker}--{Planck} {Equation} and {Physics}-{Informed} {Neural} {Networks}},
	volume = {43},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/20M1360153},
	doi = {10.1137/20M1360153},
	abstract = {The Fokker--Planck (FP) equation governing the evolution of the probability density function (PDF) is applicable to many disciplines, but it requires specification of the coefficients for each case, which can be functions of space-time and not just constants and hence require the development of a data-driven modeling approach. When the data available is directly on the PDF, there exist methods for inverse problems that can be employed to infer the coefficients and thus determine the FP equation and subsequently obtain its solution. Herein, we address a more realistic scenario, where only sparse data are given on the particles' positions at a few time instants, which are not sufficient to accurately construct directly the PDF even at those times from existing methods, e.g., kernel estimation algorithms. To this end, we develop a general framework based on physics-informed neural networks (PINNs) that introduces a new loss function using the Kullback--Leibler divergence to connect the stochastic samples with the FP equation to simultaneously learn the equation and infer the multidimensional PDF at all times. In particular, we consider two types of inverse problems, type I, where the FP equation is known but the initial PDF is unknown, and type II, in which, in addition to the unknown initial PDF, the drift and diffusion terms are also unknown. In both cases, we investigate problems with either Brownian or Lévy noise or a combination of both. We demonstrate the new PINN framework in detail in the one-dimensional (1D) case, but we also provide results for up to five dimensions demonstrating that we can infer both the FP equation and dynamics simultaneously at all times with high accuracy using only very few discrete observations of the particles.},
	number = {3},
	urldate = {2022-06-10},
	journal = {SIAM J. Sci. Comput.},
	author = {Chen, Xiaoli and Yang, Liu and Duan, Jinqiao and Karniadakis, George Em},
	month = jan,
	year = {2021},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {60H35, 62M45, small data, 35Q84, deep neural networks, Kullback--Leibler divergence, Lévy noise, nonlocality, physics-informed learning, Kullback–Leibler divergence},
	pages = {B811--B830},
}

@article{chen_physics-informed_2020,
	title = {Physics-informed neural networks for inverse problems in nano-optics and metamaterials},
	volume = {28},
	copyright = {\&\#169; 2020 Optical Society of America},
	issn = {1094-4087},
	url = {https://opg.optica.org/oe/abstract.cfm?uri=oe-28-8-11618},
	doi = {10.1364/OE.384875},
	abstract = {In this paper, we employ the emerging paradigm of physics-informed neural networks (PINNs) for the solution of representative inverse scattering problems in photonic metamaterials and nano-optics technologies. In particular, we successfully apply mesh-free PINNs to the difficult task of retrieving the effective permittivity parameters of a number of finite-size scattering systems that involve many interacting nanostructures as well as multi-component nanoparticles. Our methodology is fully validated by numerical simulations based on the finite element method (FEM). The development of physics-informed deep learning techniques for inverse scattering can enable the design of novel functional nanostructures and significantly broaden the design space of metamaterials by naturally accounting for radiation and finite-size effects beyond the limitations of traditional effective medium theories.},
	language = {EN},
	number = {8},
	urldate = {2022-06-10},
	journal = {Opt. Express},
	author = {Chen, Yuyao and Lu, Lu and Karniadakis, George Em and Negro, Luca Dal and Negro, Luca Dal and Negro, Luca Dal and Negro, Luca Dal},
	month = apr,
	year = {2020},
	note = {Publisher: Optica Publishing Group},
	pages = {11618--11633},
}

@techreport{zapf_investigating_2022,
	title = {Investigating molecular transport in the human brain from {MRI} with physics-informed neural networks},
	url = {http://arxiv.org/abs/2205.02592},
	abstract = {In recent years, a plethora of methods combining deep neural networks and partial differential equations have been developed. A widely known and popular example are physics-informed neural networks. They solve forward and inverse problems involving partial differential equations in terms of a neural network training problem. We apply physics-informed neural networks as well as the finite element method to estimate the diffusion coefficient governing the long term, i.e. over days, spread of molecules in the human brain from a novel magnetic resonance imaging technique. Synthetic testcases are created to demonstrate that the standard formulation of the physics-informed neural network faces challenges with noisy measurements in our application. Our numerical results demonstrate that the residual of the partial differential equation after training needs to be small in order to obtain accurate recovery of the diffusion coefficient. To achieve this, we apply several strategies such as tuning the weights and the norms used in the loss function as well as residual based adaptive refinement and exchange of residual training points. We find that the diffusion coefficient estimated with PINNs from magnetic resonance images becomes consistent with results from a finite element based approach when the residuum after training becomes small. The observations presented in this work are an important first step towards solving inverse problems on observations from large cohorts of patients in a semi-automated fashion with physics-informed neural networks.},
	number = {arXiv:2205.02592},
	urldate = {2022-06-10},
	institution = {arXiv},
	author = {Zapf, Bastian and Haubner, Johannes and Kuchta, Miroslav and Ringstad, Geir and Eide, Per Kristian and Mardal, Kent-Andre},
	month = may,
	year = {2022},
	doi = {10.48550/arXiv.2205.02592},
	note = {arXiv:2205.02592 [cs, math]
type: article},
	keywords = {Mathematics - Numerical Analysis, 49M41, 65L09, Mathematics - Optimization and Control, 49M41, 65L09},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/8H9RRXTN/2205.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/A9ZMV2L5/2205.html:text/html},
}

@techreport{xu_transfer_2022,
	title = {Transfer learning based physics-informed neural networks for solving inverse problems in tunneling},
	url = {http://arxiv.org/abs/2205.07731},
	abstract = {Recently, a class of machine learning methods called physics-informed neural networks (PINNs) has been proposed and gained great prevalence in solving various scientific computing problems. This approach enables the solution of partial differential equations (PDEs) via embedding physical laws into the loss function of neural networks. Many inverse problems can also be tackled by simply combining the observational data from real life scenarios with existing PINN algorithms. In this paper, we present a multi-task learning method to improve the training stability of PINNs for linear elastic problems, and the homoscedastic uncertainty is introduced as a basis for weighting losses. Furthermore, we demonstrate an application of PINNs to a practical inverse problem in tunnel engineering: prediction of external loading distributions of tunnel rings based on a limited number of displacement monitoring points. To this end, we first determine a simplified tunneling scenario at the offline stage. By setting unknown boundary conditions as learnable parameters, PINNs can predict the external loads applied on the tunnel lining with the support of enough measurement data. When it comes to the online stage in real tunnel projects, the Kriging method is adopted to reconstruct the whole displacement field based on very limited measurements. Then transfer learning is employed to fine-tune the pre-trained model from offline stage. Our results show that, although the reconstructed displacement field generated from gappy measurements is accompanied by errors, satisfactory results can still be obtained from the PINN model due to the dual regularization of physics laws and prior knowledge, which exhibits better robustness compared to traditional analysis methods. The convergence of training is also accelerated, thus making it possible for PINNs to be applied in actual tunnel projects.},
	number = {arXiv:2205.07731},
	urldate = {2022-06-10},
	institution = {arXiv},
	author = {Xu, Chen and Cao, Ba Trung and Yuan, Yong and Meschke, Günther},
	month = may,
	year = {2022},
	doi = {10.48550/arXiv.2205.07731},
	note = {arXiv:2205.07731 [cs]
type: article},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, and Science, Computer Science - Computational Engineering, Finance},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/BI9HVHAY/2205.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/AHI6S5DP/2205.html:text/html},
}

@techreport{abueidda_enhanced_2022,
	title = {Enhanced physics-informed neural networks for hyperelasticity},
	url = {http://arxiv.org/abs/2205.14148},
	abstract = {Physics-informed neural networks have gained growing interest. Specifically, they are used to solve partial differential equations governing several physical phenomena. However, physics-informed neural network models suffer from several issues and can fail to provide accurate solutions in many scenarios. We discuss a few of these challenges and the techniques, such as the use of Fourier transform, that can be used to resolve these issues. This paper proposes and develops a physics-informed neural network model that combines the residuals of the strong form and the potential energy, yielding many loss terms contributing to the definition of the loss function to be minimized. Hence, we propose using the coefficient of variation weighting scheme to dynamically and adaptively assign the weight for each loss term in the loss function. The developed PINN model is standalone and meshfree. In other words, it can accurately capture the mechanical response without requiring any labeled data. Although the framework can be used for many solid mechanics problems, we focus on three-dimensional (3D) hyperelasticity, where we consider two hyperelastic models. Once the model is trained, the response can be obtained almost instantly at any point in the physical domain, given its spatial coordinates. We demonstrate the framework's performance by solving different problems with various boundary conditions.},
	number = {arXiv:2205.14148},
	urldate = {2022-06-10},
	institution = {arXiv},
	author = {Abueidda, Diab W. and Koric, Seid and Guleryuz, Erman and Sobh, Nahil A.},
	month = may,
	year = {2022},
	doi = {10.48550/arXiv.2205.14148},
	note = {arXiv:2205.14148 [cs]
type: article},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, and Science, Computer Science - Computational Engineering, Finance},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/CUISVYTX/2205.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/U6VR7PUW/2205.html:text/html},
}

@article{aliakbari_predicting_2022,
	title = {Predicting high-fidelity multiphysics data from low-fidelity fluid flow and transport solvers using physics-informed neural networks},
	volume = {96},
	issn = {0142-727X},
	url = {https://www.sciencedirect.com/science/article/pii/S0142727X22000777},
	doi = {10.1016/j.ijheatfluidflow.2022.109002},
	abstract = {High-fidelity models of multiphysics fluid flow processes are often computationally expensive. On the other hand, less accurate low-fidelity models could be efficiently executed to provide an approximation to the solution. Multi-fidelity approaches combine high-fidelity and low-fidelity data and/or models to obtain a desirable balance between computational efficiency and accuracy. In this manuscript, we propose a multi-fidelity approach where we combine data generated by a low-fidelity computational fluid dynamics (CFD) solution strategy (solver settings and resolution) and data-free physics-informed neural networks (PINN) to obtain improved accuracy. Specifically, transfer learning based on low-fidelity CFD data is used to initialize PINN. Subsequently, PINN with this physics-guided initialization is used to obtain the final results without any high-fidelity training data. The accuracy of the final results relies on the governing equations encoded in PINN together with the low-fidelity CFD data initialization. To investigate the accuracy of this approach, several partial differential equations were solved to predict velocity and temperature in different fluid flow, heat transfer, and porous media transport problems. Comparison with reference high-fidelity CFD data revealed that the proposed approach not only significantly improves the accuracy of low-fidelity CFD data but also improves the convergence speed and accuracy of PINN.},
	language = {en},
	urldate = {2022-06-10},
	journal = {Int. J. Heat Fluid Flow},
	author = {Aliakbari, Maryam and Mahmoudi, Mostafa and Vadasz, Peter and Arzani, Amirhossein},
	month = aug,
	year = {2022},
	keywords = {Partial differential equations, Scientific machine learning, Porous media, Deep learning, CFD, Multi-fidelity modeling},
	pages = {109002},
}

@article{de_florio_physics-informed_2022,
	title = {Physics-{Informed} {Neural} {Networks} for rarefied-gas dynamics: {Poiseuille} flow in the {BGK} approximation},
	volume = {73},
	issn = {1420-9039},
	shorttitle = {Physics-{Informed} {Neural} {Networks} for rarefied-gas dynamics},
	url = {https://doi.org/10.1007/s00033-022-01767-z},
	doi = {10.1007/s00033-022-01767-z},
	abstract = {We present a new accurate approach to solving a class of problems in the theory of rarefied–gas dynamics using a Physics-Informed Neural Networks framework, where the solution of the problem is approximated by the constrained expressions introduced by the Theory of Functional Connections. The constrained expressions are made by a sum of a free function and a functional that always analytically satisfies the equation constraints. The free function used in this work is a Chebyshev neural network trained via the extreme learning machine algorithm. The method is designed to accurately and efficiently solve the linear one-point boundary value problem that arises from the Bhatnagar–Gross–Krook model of the Poiseuille flow between two parallel plates for a wide range of Knudsen numbers. The accuracy of our results is validated via the comparison with the published benchmarks.},
	language = {en},
	number = {3},
	urldate = {2022-06-10},
	journal = {Z. Angew. Math. Phys.},
	author = {De Florio, Mario and Schiassi, Enrico and Ganapol, Barry D. and Furfaro, Roberto},
	month = may,
	year = {2022},
	keywords = {Physics-Informed Neural Networks, 35Q20, 68T07, 76P05, Boltzmann equation, Extreme learning machine, Functional interpolation, Poiseuille flow, Rarefied gas dynamics},
	pages = {126},
}

@article{de_florio_physics-informed_2022-1,
	title = {Physics-informed neural networks and functional interpolation for stiff chemical kinetics},
	volume = {32},
	issn = {1054-1500},
	url = {https://aip.scitation.org/doi/full/10.1063/5.0086649},
	doi = {10.1063/5.0086649},
	abstract = {This work presents a recently developed approach based on physics-informed neural networks (PINNs) for the solution of initial value problems (IVPs), focusing on stiff chemical kinetic problems with governing equations of stiff ordinary differential equations (ODEs). The framework developed by the authors combines PINNs with the theory of functional connections and extreme learning machines in the so-called extreme theory of functional connections (X-TFC). While regular PINN methodologies appear to fail in solving stiff systems of ODEs easily, we show how our method, with a single-layer neural network (NN) is efficient and robust to solve such challenging problems without using artifacts to reduce the stiffness of problems. The accuracy of X-TFC is tested against several state-of-the-art methods, showing its performance both in terms of computational time and accuracy. A rigorous upper bound on the generalization error of X-TFC frameworks in learning the solutions of IVPs for ODEs is provided here for the first time. A significant advantage of this framework is its flexibility to adapt to various problems with minimal changes in coding. Also, once the NN is trained, it gives us an analytical representation of the solution at any desired instant in time outside the initial discretization. Learning stiff ODEs opens up possibilities of using X-TFC in applications with large time ranges, such as chemical dynamics in energy conversion, nuclear dynamics systems, life sciences, and environmental engineering.},
	number = {6},
	urldate = {2022-06-10},
	journal = {Chaos},
	author = {De Florio, Mario and Schiassi, Enrico and Furfaro, Roberto},
	month = jun,
	year = {2022},
	note = {Publisher: American Institute of Physics},
	pages = {063107},
}

@techreport{krishnapriyan_characterizing_2021,
	title = {Characterizing possible failure modes in physics-informed neural networks},
	url = {http://arxiv.org/abs/2109.01050},
	abstract = {Recent work in scientific machine learning has developed so-called physics-informed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN's setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN's loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training.},
	number = {arXiv:2109.01050},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Krishnapriyan, Aditi S. and Gholami, Amir and Zhe, Shandian and Kirby, Robert M. and Mahoney, Michael W.},
	month = nov,
	year = {2021},
	doi = {10.48550/arXiv.2109.01050},
	note = {arXiv:2109.01050 [physics]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/home/hell/Zotero/storage/C7Q3SG66/Krishnapriyan et al. - 2021 - Characterizing possible failure modes in physics-i.pdf:application/pdf;arXiv.org Snapshot:/home/hell/Zotero/storage/RV8FPXHY/2109.html:text/html;arXiv.org Snapshot:/home/hell/Zotero/storage/63NFSWII/2109.html:text/html},
}

@inproceedings{najafi_application_2020,
	title = {Application of {Artificial} {Neural} {Network} {As} a {Near}-{Real} {Time} {Technique} for {Solving} {Non}-{Linear} {Inverse} {Heat} {Conduction} {Problems} in a {One}-{Dimensional} {Medium} {With} {Moving} {Boundary}},
	url = {https://asmedigitalcollection.asme.org/HT/proceedings/HT2020/83709/V001T02A013/1087362},
	doi = {10.1115/HT2020-9054},
	language = {en},
	urldate = {2022-06-28},
	publisher = {ASME Digital Collection},
	author = {Najafi, Hamidreza and Uyanna, Obinna and Zhang, Jian},
	month = sep,
	year = {2020},
	file = {Snapshot:/home/hell/Zotero/storage/J62Z8VYM/1087362.html:text/html;Snapshot:/home/hell/Zotero/storage/NXXV96IW/1087362.html:text/html},
}

@article{tamaddon-jahromi_data-driven_2020,
	title = {Data-driven inverse modelling through neural network (deep learning) and computational heat transfer},
	volume = {369},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782520304023},
	doi = {10.1016/j.cma.2020.113217},
	abstract = {In this work, the potential of carrying out inverse problems with linear and non-linear behaviour is investigated using deep learning methods. In inverse problems, the boundary conditions are determined using sparse measurement of a variable such as velocity or temperature. Although this is mathematically tractable for simple problems, it can be extremely challenging for complex problems. To overcome the non-linear and complex effects, a brute force approach was used on a trial and error basis to find an approximate solution. With the advent of machine learning algorithms it may now be possible to model inverse problems faster and more accurately. In order to demonstrate that machine learning can be used in solving inverse problems, we propose a fusion between computational mechanics and machine learning. The forward problems are solved first to create a database. This database is then used to train the machine learning algorithms. The trained algorithm is then used to determine the boundary conditions of a problem from assumed measurements. The proposed method is tested for the linear/non-linear heat conduction, convection–conduction, and natural convection problems in which the boundary conditions are determined by providing three, four, and five temperature measurements. This study demonstrates that the proposed fusion of computational mechanics and machine learning is an effective way of tackling complex inverse problems.},
	language = {en},
	urldate = {2022-06-28},
	journal = {Comput. Methods Appl. Mech. Eng.},
	author = {Tamaddon-Jahromi, Hamid Reza and Chakshu, Neeraj Kavan and Sazonov, Igor and Evans, Llion M. and Thomas, Hywel and Nithiarasu, Perumal},
	month = sep,
	year = {2020},
	keywords = {Machine learning, Computational mechanics, Heat conduction, Heat convection–conduction, Inverse modelling, Natural convection},
	pages = {113217},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/7Y3SKCGQ/S0045782520304023.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/JWSUHU84/S0045782520304023.html:text/html},
}

@article{berrada_multi-objective_1996,
	title = {A multi-objective approach to nurse scheduling with both hard and soft constraints},
	volume = {30},
	issn = {0038-0121},
	url = {https://www.sciencedirect.com/science/article/pii/0038012196000109},
	doi = {10.1016/0038-0121(96)00010-9},
	abstract = {Solving the nurse scheduling problem properly has a great impact on nurses' working conditions which are strongly related to the level of quality of health care and recruiting of qualified personnel. In this paper, a multi-objective approach is shown to be a very flexible tool for modeling this problem. Hence, it could be used in several specific contexts. Three different solution techniques are also summarized. The approach is illustrated with an example, and numerical results are reported.},
	language = {en},
	number = {3},
	urldate = {2022-06-24},
	journal = {Socioecon. Plann. Sci.},
	author = {Berrada, Ilham and Ferland, Jacques A. and Michelon, Philippe},
	month = sep,
	year = {1996},
	pages = {183--193},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/9SXQ27EM/0038012196000109.html:text/html;ScienceDirect Snapshot:/home/hell/Zotero/storage/B68UJVXG/0038012196000109.html:text/html},
}

@article{vaswani_attention_2017-1,
	title = {Attention is all you need},
	volume = {30},
	journal = {Adv. Neural Inf. Process. Syst.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@inproceedings{cao_choose_2021,
	title = {Choose a {Transformer}: {Fourier} or {Galerkin}},
	volume = {34},
	shorttitle = {Choose a {Transformer}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/d0921d442ee91b896ad95059d13df618-Abstract.html},
	abstract = {In this paper, we apply the self-attention from the state-of-the-art Transformer in Attention Is All You Need for the first time to a data-driven operator learning problem related to partial differential equations. An effort is put together to explain the heuristics of, and to improve the efficacy of the attention mechanism. By employing the operator approximation theory in Hilbert spaces, it is demonstrated for the first time that the softmax normalization in the scaled dot-product attention is sufficient but not necessary. Without softmax, the approximation capacity of a linearized Transformer variant can be proved to be comparable to a Petrov-Galerkin projection layer-wise, and the estimate is independent with respect to the sequence length. A new layer normalization scheme mimicking the Petrov-Galerkin projection is proposed to allow a scaling to propagate through attention layers, which helps the model achieve remarkable accuracy in operator learning tasks with unnormalized data. Finally, we present three operator learning experiments, including the viscid Burgers' equation, an interface Darcy flow, and an inverse interface coefficient identification problem. The newly proposed simple attention-based operator learner, Galerkin Transformer, shows significant improvements in both training cost and evaluation accuracy over its softmax-normalized counterparts.},
	urldate = {2022-08-01},
	booktitle = {Adv. Neural Inf. Process. Syst.},
	publisher = {Curran Assoc., Inc.},
	author = {Cao, Shuhao},
	year = {2021},
	pages = {24924--24940},
	file = {Full Text PDF:/home/hell/Zotero/storage/6RDRGWDT/Cao - 2021 - Choose a Transformer Fourier or Galerkin.pdf:application/pdf},
}

@article{gao_physics-informed_2022,
	title = {Physics-informed graph neural {Galerkin} networks: {A} unified framework for solving {PDE}-governed forward and inverse problems},
	volume = {390},
	issn = {0045-7825},
	shorttitle = {Physics-informed graph neural {Galerkin} networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782521007076},
	doi = {10.1016/j.cma.2021.114502},
	abstract = {Despite the great promise of the physics-informed neural networks (PINNs) in solving forward and inverse problems, several technical challenges are present as roadblocks for more complex and realistic applications. First, most existing PINNs are based on point-wise formulation with fully-connected networks to learn continuous functions, which suffer from poor scalability and hard boundary enforcement. Second, the infinite search space over-complicates the non-convex optimization for network training. Third, although the convolutional neural network (CNN)-based discrete learning can significantly improve training efficiency, CNNs struggle to handle irregular geometries with unstructured meshes. To properly address these challenges, we present a novel discrete PINN framework based on graph convolutional network (GCN) and variational structure of PDE to solve forward and inverse partial differential equations (PDEs) in a unified manner. The use of a piecewise polynomial basis can reduce the dimension of search space and facilitate training and convergence. Without the need of tuning penalty parameters in classic PINNs, the proposed method can strictly impose boundary conditions and assimilate sparse data in both forward and inverse settings. The flexibility of GCNs is leveraged for irregular geometries with unstructured meshes. The effectiveness and merit of the proposed method are demonstrated over a variety of forward and inverse computational mechanics problems governed by both linear and nonlinear PDEs.},
	language = {en},
	urldate = {2022-08-01},
	journal = {Comput. Methods Appl. Mech. Eng.},
	author = {Gao, Han and Zahr, Matthew J. and Wang, Jian-Xun},
	month = feb,
	year = {2022},
	keywords = {Partial differential equations, Graph convolutional neural networks, Inverse problem, Mechanics, Physics-informed machine learning},
	pages = {114502},
	file = {ScienceDirect Full Text PDF:/home/hell/Zotero/storage/EPF7FQTV/Gao et al. - 2022 - Physics-informed graph neural Galerkin networks A.pdf:application/pdf},
}

@misc{noauthor_idrl-lab_nodate-1,
	title = {idrl-lab {PINNpapers}},
	url = {https://github.com/idrl-lab/PINNpapers},
}

@incollection{noauthor_notitle_nodate-1,
}

@misc{noauthor_partial_nodate-1,
	title = {Partial {Differential} {Equation} {Toolbox} ({R2022a})},
	url = {https://uk.mathworks.com/products/pde.html},
	abstract = {Partial Differential Equation Toolbox provides functions for solving partial differential equations (PDEs) in 2D, 3D, and time using finite element analysis.},
	language = {en},
	urldate = {2022-05-27},
	file = {Snapshot:/home/hell/Zotero/storage/KYJ937I8/pde.html:text/html},
}

@misc{noauthor_torchsin_nodate-1,
	title = {torch.sin — {PyTorch} 1.11.0 documentation},
	url = {https://pytorch.org/docs/stable/generated/torch.sin.html},
	urldate = {2022-06-03},
	file = {torch.sin — PyTorch 1.11.0 documentation:/home/hell/Zotero/storage/YZCU43H9/torch.sin.html:text/html},
}

@inproceedings{vaswani_attention_2017-2,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-05-27},
	booktitle = {Adv. Neural Inf. Process. Syst.},
	publisher = {Curran Assoc., Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@misc{noauthor_stiff_nodate-1,
	title = {Stiff {Differential} {Equations}},
	url = {https://uk.mathworks.com/company/newsletters/articles/stiff-differential-equations.html},
	abstract = {Stiffness is a subtle, difficult, and important - concept in the numerical solution of ordinary differential equations.},
	language = {en},
	urldate = {2022-05-25},
	file = {Snapshot:/home/hell/Zotero/storage/FGQ3U76V/stiff-differential-equations.html:text/html},
}

@article{vaswani_attention_2017-3,
	title = {Attention is all you need},
	volume = {30},
	journal = {Adv. Neural Inf. Process. Syst.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@article{wu_development_2018,
	title = {Development and {Application} of {Artificial} {Neural} {Network}},
	volume = {102},
	issn = {1572-834X},
	url = {https://doi.org/10.1007/s11277-017-5224-x},
	doi = {10.1007/s11277-017-5224-x},
	abstract = {Artificial neural network is a very important part in the new industry of artificial intelligence. In China, there are many researches on artificial neural network and artificial intelligence are developing rapidly. Therefore, this paper reviews and summarizes artificial neural network, and hopes that readers can get a deeper understanding of artificial neural network. This paper first reviews the development history of artificial neural network and its related theory, and introduces four major characteristics of artificial neural network, such as the non-linear, non-limitative, non-qualitative and non-convex. Then it emphatically analyzes its application in information, medicine, economy, control, transportation and psychology. Finally, the future development trend of artificial neural network is prospected and summarized.},
	language = {en},
	number = {2},
	urldate = {2022-08-01},
	journal = {Wireless Pers. Commun.},
	author = {Wu, Yu-chen and Feng, Jun-wen},
	month = sep,
	year = {2018},
	keywords = {Artificial neural network, Application status analysis, Development history, Future development trend},
	pages = {1645--1656},
	file = {Full Text PDF:/home/hell/Zotero/storage/9TPWSDXT/Wu and Feng - 2018 - Development and Application of Artificial Neural N.pdf:application/pdf},
}

@inproceedings{trehan_non-convex_2020,
	title = {Non-{Convex} {Optimization}: {A} {Review}},
	shorttitle = {Non-{Convex} {Optimization}},
	doi = {10.1109/ICICCS48265.2020.9120874},
	abstract = {With the rapid development in technology, Artificial Intelligence is responsible for giving solution to every new problem in technology. Artificial Intelligence is the combat process of application, implementation and self- correction. The most potential application of Artificial Intelligence is Machine Learning. It is responsible for training the models based on user experience without doing any explicit programming. Optimization Techniques is responsible for maintaining the quality of the given model. This paper focuses upon Non-Convex Optimization Algorithms such as SGD, EM Algorithm, Alternating Minimization and its potential applications in the real world such as Low-Rank Matrix Recovery, Linear Regression, Sparse Dictionary and relatively many others.},
	booktitle = {(ICICCS)},
	author = {Trehan, Dhruv},
	month = may,
	year = {2020},
	keywords = {Machine learning, Machine Learning, Algorithms, Artificial Intelligence, Dictionaries, Linear regression, Machine learning algorithms, Minimization, Non-Convex Optimization, Optimization Techniques, Programming, Real World Application, Training},
	pages = {418--423},
	file = {IEEE Xplore Abstract Record:/home/hell/Zotero/storage/4NVLGNV4/9120874.html:text/html;IEEE Xplore Full Text PDF:/home/hell/Zotero/storage/B8QXVMQD/Trehan - 2020 - Non-Convex Optimization A Review.pdf:application/pdf},
}

@book{molnar_interpretable_2020,
	title = {Interpretable machine learning},
	isbn = {0-244-76852-8},
	publisher = {Lulu. com},
	author = {Molnar, Christoph},
	year = {2020},
}

@article{wang_comprehensive_2022,
	title = {A {Comprehensive} {Survey} of {Loss} {Functions} in {Machine} {Learning}},
	volume = {9},
	issn = {2198-5812},
	url = {https://doi.org/10.1007/s40745-020-00253-5},
	doi = {10.1007/s40745-020-00253-5},
	abstract = {As one of the important research topics in machine learning, loss function plays an important role in the construction of machine learning algorithms and the improvement of their performance, which has been concerned and explored by many researchers. But it still has a big gap to summarize, analyze and compare the classical loss functions. Therefore, this paper summarizes and analyzes 31 classical loss functions in machine learning. Specifically, we describe the loss functions from the aspects of traditional machine learning and deep learning respectively. The former is divided into classification problem, regression problem and unsupervised learning according to the task type. The latter is subdivided according to the application scenario, and here we mainly select object detection and face recognition to introduces their loss functions. In each task or application, in addition to analyzing each loss function from formula, meaning, image and algorithm, the loss functions under the same task or application are also summarized and compared to deepen the understanding and provide help for the selection and improvement of loss function.},
	language = {en},
	number = {2},
	urldate = {2022-08-01},
	journal = {Ann. Data Sci.},
	author = {Wang, Qi and Ma, Yue and Zhao, Kun and Tian, Yingjie},
	month = apr,
	year = {2022},
	keywords = {Machine learning, Deep learning, Loss function, Survey},
	pages = {187--212},
}

@misc{ghojogh_kkt_2021,
	title = {{KKT} {Conditions}, {First}-{Order} and {Second}-{Order} {Optimization}, and {Distributed} {Optimization}: {Tutorial} and {Survey}},
	shorttitle = {{KKT} {Conditions}, {First}-{Order} and {Second}-{Order} {Optimization}, and {Distributed} {Optimization}},
	url = {http://arxiv.org/abs/2110.01858},
	doi = {10.48550/arXiv.2110.01858},
	abstract = {This is a tutorial and survey paper on Karush-Kuhn-Tucker (KKT) conditions, first-order and second-order numerical optimization, and distributed optimization. After a brief review of history of optimization, we start with some preliminaries on properties of sets, norms, functions, and concepts of optimization. Then, we introduce the optimization problem, standard optimization problems (including linear programming, quadratic programming, and semidefinite programming), and convex problems. We also introduce some techniques such as eliminating inequality, equality, and set constraints, adding slack variables, and epigraph form. We introduce Lagrangian function, dual variables, KKT conditions (including primal feasibility, dual feasibility, weak and strong duality, complementary slackness, and stationarity condition), and solving optimization by method of Lagrange multipliers. Then, we cover first-order optimization including gradient descent, line-search, convergence of gradient methods, momentum, steepest descent, and backpropagation. Other first-order methods are explained, such as accelerated gradient method, stochastic gradient descent, mini-batch gradient descent, stochastic average gradient, stochastic variance reduced gradient, AdaGrad, RMSProp, and Adam optimizer, proximal methods (including proximal mapping, proximal point algorithm, and proximal gradient method), and constrained gradient methods (including projected gradient method, projection onto convex sets, and Frank-Wolfe method). We also cover non-smooth and \${\textbackslash}ell\_1\$ optimization methods including lasso regularization, convex conjugate, Huber function, soft-thresholding, coordinate descent, and subgradient methods. Then, we explain second-order methods including Newton's method for unconstrained, equality constrained, and inequality constrained problems....},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
	month = oct,
	year = {2021},
	note = {arXiv:2110.01858 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Computer Science - Distributed, Parallel, and Cluster Computing, Mathematics - Optimization and Control},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/XTQE5LII/2110.html:text/html},
}

@incollection{ketkar_stochastic_2017,
	address = {Berkeley, CA},
	title = {Stochastic {Gradient} {Descent}},
	isbn = {978-1-4842-2766-4},
	url = {https://doi.org/10.1007/978-1-4842-2766-4_8},
	abstract = {This chapter gives a broad overview and a historical context around the subject of deep learning. It also gives the reader a roadmap for navigating the book, the prerequisites, and further reading to dive deeper into the subject matter.},
	language = {en},
	urldate = {2022-08-01},
	booktitle = {Deep {Learning} with {Python}: {A} {Hands}-on {Introduction}},
	publisher = {apress},
	author = {Ketkar, Nikhil},
	editor = {Ketkar, Nikhil},
	year = {2017},
	doi = {10.1007/978-1-4842-2766-4_8},
	keywords = {Gradient Descent, Learning Rate, Loss Function, Saddle Point, Steep Descent},
	pages = {113--132},
}

@misc{kingma_adam_2017-1,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/VNXBQ8G9/1412.html:text/html},
}

@article{elshawi_dlbench_2021,
	title = {{DLBench}: a comprehensive experimental evaluation of deep learning frameworks},
	volume = {24},
	issn = {1573-7543},
	shorttitle = {{DLBench}},
	url = {https://doi.org/10.1007/s10586-021-03240-4},
	doi = {10.1007/s10586-021-03240-4},
	abstract = {Deep Learning (DL) has achieved remarkable progress over the last decade on various tasks such as image recognition, speech recognition, and natural language processing. In general, three main crucial aspects fueled this progress: the increasing availability of large amount of digitized data, the increasing availability of affordable parallel and powerful computing resources (e.g., GPU) and the growing number of open source deep learning frameworks that facilitate and ease the development process of deep learning architectures. In practice, the increasing popularity of deep learning frameworks calls for benchmarking studies that can effectively evaluate and understand the performance characteristics of these systems. In this paper, we conduct an extensive experimental evaluation and analysis of six popular deep learning frameworks, namely, TensorFlow, MXNet, PyTorch, Theano, Chainer, and Keras, using three types of DL architectures Convolutional Neural Networks (CNN), Faster Region-based Convolutional Neural Networks (Faster R-CNN), and Long Short Term Memory (LSTM). Our experimental evaluation considers different aspects for its comparison including accuracy, training time, convergence and resource consumption patterns. Our experiments have been conducted on both CPU and GPU environments using different datasets. We report and analyze the performance characteristics of the studied frameworks. In addition, we report a set of insights and important lessons that we have learned from conducting our experiments.},
	language = {en},
	number = {3},
	urldate = {2022-08-01},
	journal = {Cluster Comput.},
	author = {Elshawi, Radwa and Wahab, Abdul and Barnawi, Ahmed and Sakr, Sherif},
	month = sep,
	year = {2021},
	keywords = {Deep learning, LSTM, CNN, Experimental evaluation},
	pages = {2017--2038},
}

@article{ramm_stable_2001,
	title = {On stable numerical differentiation},
	volume = {70},
	issn = {0025-5718, 1088-6842},
	url = {https://www.ams.org/mcom/2001-70-235/S0025-5718-01-01307-2/},
	doi = {10.1090/S0025-5718-01-01307-2},
	abstract = {A new approach to the construction of finite-difference methods is presented. It is shown how the multi-point differentiators can generate regularizing algorithms with a stepsize h being a regularization parameter. The explicitly computable estimation constants are given. Also an iteratively regularized scheme for solving the numerical differentiation problem in the form of Volterra integral equation is developed.},
	language = {en},
	number = {235},
	urldate = {2022-08-01},
	journal = {Math. Comput.},
	author = {Ramm, Alexander and Smirnova, Alexandra},
	year = {2001},
	keywords = {ill-posed problems, multi-point methods, noisy data, Numerical differentiation, regularization},
	pages = {1131--1153},
}

@book{davenport_computer_1993,
	title = {Computer algebra systems and algorithms for algebraic computation},
	isbn = {0-12-204232-8},
	publisher = {Acad. Press Prof., Inc.},
	author = {Davenport, James Harold and Siret, Yvon and Tournier, Évelyne},
	year = {1993},
}

@article{fang_symbolic_2020,
	title = {Symbolic {Techniques} for {Deep} {Learning}: {Challenges} and {Opportunities}},
	journal = {arXiv preprint arXiv:2010.02727},
	author = {Fang, Belinda and Yang, Elaine and Xie, Fei},
	year = {2020},
}

@article{barros_survey_2021,
	title = {A survey on embedding dynamic graphs},
	volume = {55},
	number = {1},
	journal = {ACM Comput. Surv. (CSUR)},
	author = {Barros, Claudio DT and Mendonça, Matheus RF and Vieira, Alex B. and Ziviani, Artur},
	year = {2021},
	note = {ISBN: 0360-0300
Publisher: ACM New York, NY},
	pages = {1--37},
}

@article{giles_extended_2008,
	title = {An extended collection of matrix derivative results for forward and reverse mode automatic differentiation},
	author = {Giles, Mike},
	year = {2008},
}

@article{mathias_chain_1996,
	title = {A chain rule for matrix functions and applications},
	volume = {17},
	number = {3},
	journal = {SIAM J. Matrix Anal. Appl.},
	author = {Mathias, Roy},
	year = {1996},
	note = {ISBN: 0895-4798
Publisher: SIAM},
	pages = {610--620},
}

@misc{lewkowycz_how_2021,
	title = {How to decay your learning rate},
	url = {http://arxiv.org/abs/2103.12682},
	doi = {10.48550/arXiv.2103.12682},
	abstract = {Complex learning rate schedules have become an integral part of deep learning. We find empirically that common fine-tuned schedules decay the learning rate after the weight norm bounces. This leads to the proposal of ABEL: an automatic scheduler which decays the learning rate by keeping track of the weight norm. ABEL's performance matches that of tuned schedules and is more robust with respect to its parameters. Through extensive experiments in vision, NLP, and RL, we show that if the weight norm does not bounce, we can simplify schedules even further with no loss in performance. In such cases, a complex schedule has similar performance to a constant learning rate with a decay at the end of training.},
	urldate = {2022-08-25},
	publisher = {arXiv},
	author = {Lewkowycz, Aitor},
	month = mar,
	year = {2021},
	note = {arXiv:2103.12682 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/JKLUK24U/2103.html:text/html},
}

@book{ozisik_inverse_2020,
	address = {New York},
	title = {Inverse {Heat} {Transfer}: {Fundamentals} and {Applications}},
	isbn = {978-0-203-74978-4},
	shorttitle = {Inverse {Heat} {Transfer}},
	abstract = {This book introduces the fundamental concepts of inverse heat transfer problems. It presents in detail the basic steps of four techniques of inverse heat transfer protocol, as a parameter estimation approach and as a function estimation approach. These techniques are then applied to the solution of the problems of practical engineering interest involving conduction, convection, and radiation. The text also introduces a formulation based on generalized coordinates for the solution of inverse heat conduction problems in two-dimensional regions.},
	publisher = {Routledge},
	author = {Ozisik, M. Necat},
	month = sep,
	year = {2020},
	doi = {10.1201/9780203749784},
}

@book{ozisik_inverse_2018,
	edition = {1},
	title = {Inverse {Heat} {Transfer}: {Fundamentals} and {Applications}},
	isbn = {978-0-203-74978-4},
	shorttitle = {Inverse {Heat} {Transfer}},
	url = {https://www.taylorfrancis.com/books/9781351436403},
	language = {en},
	urldate = {2023-01-31},
	publisher = {Routledge},
	author = {Özisik, M. Necati and Orlande, Helcio R. B.},
	month = may,
	year = {2018},
	doi = {10.1201/9780203749784},
}

@book{tikhonov_numerical_1995,
	title = {Numerical {Methods} for the {Solution} of {Ill}-{Posed} {Problems}},
	isbn = {978-0-7923-3583-2},
	abstract = {Many problems in science, technology and engineering are posed in the form of operator equations of the first kind, with the operator and RHS approximately known. But such problems often turn out to be ill-posed, having no solution, or a non-unique solution, and/or an unstable solution. Non-existence and non-uniqueness can usually be overcome by settling for `generalised' solutions, leading to the need to develop regularising algorithms.  The theory of ill-posed problems has advanced greatly since A. N. Tikhonov laid its foundations, the Russian original of this book (1990) rapidly becoming a classical monograph on the topic. The present edition has been completely updated to consider linear ill-posed problems with or without a priori constraints (non-negativity, monotonicity, convexity, etc.).  Besides the theoretical material, the book also contains a FORTRAN program library.  Audience: Postgraduate students of physics, mathematics, chemistry, economics, engineering. Engineers and scientists interested in data processing and the theory of ill-posed problems.},
	language = {en},
	publisher = {Springer Sci. Bus. Media},
	author = {Tikhonov, A. N. and Goncharsky, A. and Stepanov, V. V. and Yagola, Anatoly G.},
	month = jun,
	year = {1995},
	note = {Google-Books-ID: rpdAzBsOSMgC},
	keywords = {Computers / Programming / Algorithms, Mathematics / Applied, Mathematics / Calculus, Mathematics / Counting \& Numeration, Mathematics / Functional Analysis, Mathematics / Mathematical Analysis, Mathematics / Numerical Analysis, Mathematics / Optimization, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@book{engl_regularization_1996,
	title = {Regularization of {Inverse} {Problems}},
	isbn = {978-0-7923-4157-4},
	abstract = {In the last two decades, the field of inverse problems has certainly been one of the fastest growing areas in applied mathematics. This growth has largely been driven by the needs of applications both in other sciences and in industry. In Chapter 1, we will give a short overview over some classes of inverse problems of practical interest. Like everything in this book, this overview is far from being complete and quite subjective. As will be shown, inverse problems typically lead to mathematical models that are not well-posed in the sense of Hadamard, i.e., to ill-posed problems. This means especially that their solution is unstable under data perturbations. Numerical meth ods that can cope with this problem are the so-called regularization methods. This book is devoted to the mathematical theory of regularization methods. For linear problems, this theory can be considered to be relatively complete and will be de scribed in Chapters 2 - 8. For nonlinear problems, the theory is so far developed to a much lesser extent. We give an account of some of the currently available results, as far as they might be of lasting value, in Chapters 10 and 11. Although the main emphasis of the book is on a functional analytic treatment in the context of operator equations, we include, for linear problems, also some information on numerical aspects in Chapter 9.},
	language = {en},
	publisher = {Springer Sci. Bus. Media},
	author = {Engl, Heinz Werner and Hanke, Martin and Neubauer, A.},
	month = jul,
	year = {1996},
	note = {Google-Books-ID: 2bzgmMv5EVcC},
	keywords = {Mathematics / Calculus, Mathematics / Counting \& Numeration, Mathematics / Mathematical Analysis, Mathematics / Numerical Analysis, Mathematics / Probability \& Statistics / Stochastic Processes, Mathematics / Differential Equations / General, Science / Chemistry / Physical \& Theoretical, Technology \& Engineering / Materials Science},
}

@article{fellows_local_2012,
	series = {In {Commemoration} of {Amir} {Pnueli}},
	title = {Local search: {Is} brute-force avoidable?},
	volume = {78},
	issn = {0022-0000},
	shorttitle = {Local search},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000011001097},
	doi = {10.1016/j.jcss.2011.10.003},
	abstract = {Many local search algorithms are based on searching in the k-exchange neighborhood. This is the set of solutions that can be obtained from the current solution by exchanging at most k elements. As a rule of thumb, the larger k is, the better are the chances of finding an improved solution. However, for inputs of size n, a naïve brute-force search of the k-exchange neighborhood requires nO(k) time, which is not practical even for very small values of k. We show that for several classes of sparse graphs, including planar graphs, graphs of bounded vertex degree and graphs excluding some fixed graph as a minor, an improved solution in the k-exchange neighborhood for many problems can be found much more efficiently. Our algorithms run in time O(τ(k)⋅nc), where τ is a function depending only on k and c is a constant independent of k and n. We demonstrate the applicability of this approach on a variety of problems including r-Center, Vertex Cover, Odd Cycle Transversal, Max-Cut, and Min-Bisection. In particular, on planar graphs, all our algorithms searching for a k-local improvement run in time O(2O(k)⋅n2), which is polynomial for k=O(logn). We complement these fixed-parameter tractable algorithms for k-local search with parameterized intractability results indicating that brute-force search is unavoidable in more general classes of graphs.},
	language = {en},
	number = {3},
	urldate = {2023-01-31},
	journal = {J. Comput. Syst. Sci.},
	author = {Fellows, Michael R. and Fomin, Fedor V. and Lokshtanov, Daniel and Rosamond, Frances and Saurabh, Saket and Villanger, Yngve},
	month = may,
	year = {2012},
	keywords = {Local search, Local treewidth, Parameterized complexity},
	pages = {707--719},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/ADKUQ9RK/S0022000011001097.html:text/html},
}

@incollection{ippoliti_reasoning_2015,
	address = {Cham},
	series = {Studies in {Applied} {Philosophy}, {Epistemology} and {Rational} {Ethics}},
	title = {Reasoning at the {Frontier} of {Knowledge}: {Introductory} {Essay}},
	isbn = {978-3-319-09159-4},
	shorttitle = {Reasoning at the {Frontier} of {Knowledge}},
	url = {https://doi.org/10.1007/978-3-319-09159-4_1},
	abstract = {The advancement of knowledge is the big goal in human understanding. To get it, we often have to push beyond the frontier of knowledge, where our understanding dissolves and where new, strange entities appear. These require bold explorations and the consequent discoveries are not idle mind games, but crucial tools for our future life. And to have a method for carrying out these explorations is essential.},
	language = {en},
	urldate = {2023-01-31},
	booktitle = {Heuristic {Reasoning}},
	publisher = {Springer Int. Pub.},
	author = {Ippoliti, Emiliano},
	editor = {Ippoliti, Emiliano},
	year = {2015},
	doi = {10.1007/978-3-319-09159-4_1},
	keywords = {Abductive Reasoning, Orthodox View, Rational Heuristic, Scientific Revolution, Stock Market Price},
	pages = {1--10},
}

@misc{noauthor_heuristics_nodate,
	title = {Heuristics: intelligent search strategies for computer problem solving},
	shorttitle = {Heuristics},
	url = {https://dl.acm.org/doi/abs/10.5555/525},
	language = {EN},
	urldate = {2023-01-31},
	journal = {Guide Books},
	doi = {10.5555/525},
	note = {Archive Location: world},
	file = {Snapshot:/home/hell/Zotero/storage/Y7N537KX/525.html:text/html},
}

@article{dechter_generalized_1985,
	title = {Generalized best-first search strategies and the optimality of {A}*},
	volume = {32},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/3828.3830},
	doi = {10.1145/3828.3830},
	abstract = {This paper reports several properties of heuristic best-first search strategies whose scoring functions ƒ depend on all the information available from each candidate path, not merely on the current cost
              g
              and the estimated completion cost
              h
              . It is shown that several known properties of A* retain their form (with the minmax of
              f
              playing the role of the optimal cost), which helps establish general tests of admissibility and general conditions for node expansion for these strategies. On the basis of this framework the computational optimality of A*, in the sense of never expanding a node that can be skipped by some other algorithm having access to the same heuristic information that A* uses, is examined. A hierarchy of four optimality types is defined and three classes of algorithms and four domains of problem instances are considered. Computational performances relative to these algorithms and domains are appraised. For each class-domain combination, we then identify the strongest type of optimality that exists and the algorithm for achieving it. The main results of this paper relate to the class of algorithms that, like A*, return optimal solutions (i.e., admissible) when all cost estimates are optimistic (i.e.,
              h
              ≤
              h
              *). On this class, A* is shown to be not optimal and it is also shown that no optimal algorithm exists, but if the performance tests are confirmed to cases in which the estimates are also consistent, then A* is indeed optimal. Additionally, A* is also shown to be optimal over a subset of the latter class containing all
              best-first
              algorithms that are guided by path-dependent evaluation functions.},
	language = {en},
	number = {3},
	urldate = {2023-01-31},
	journal = {J. ACM},
	author = {Dechter, Rina and Pearl, Judea},
	month = jul,
	year = {1985},
	pages = {505--536},
}

@article{jaluria_solution_2019,
	title = {Solution of {Inverse} {Problems} in {Thermal} {Systems}},
	volume = {12},
	issn = {1948-5085},
	url = {https://doi.org/10.1115/1.4042353},
	doi = {10.1115/1.4042353},
	abstract = {A common occurrence in many practical systems is that the desired result is known or given, but the conditions needed for achieving this result are not known. This situation leads to inverse problems, which are of particular interest in thermal processes. For instance, the temperature cycle to which a component must be subjected in order to obtain desired characteristics in a manufacturing system, such as heat treatment or plastic thermoforming, is prescribed. However, the necessary boundary and initial conditions are not known and must be determined by solving the inverse problem. Similarly, an inverse solution may be needed to complete a given physical problem by determining the unknown boundary conditions. Solutions thus obtained are not unique and optimization is generally needed to obtain results within a small region of uncertainty. This review paper discusses several inverse problems that arise in a variety of practical processes and presents some of the approaches that may be used to solve them and obtain acceptable and realistic results. Optimization methods that may be used for reducing the error are presented. A few examples are given to illustrate the applicability of these methods and the challenges that must be addressed in solving inverse problems. These examples include the heat treatment process, unknown wall temperature distribution in a furnace, and transport in a plume or jet involving the determination of the strength and location of the heat source by employing a few selected data points downstream. Optimization of the positioning of the data points is used to minimize the number of samples needed for accurate predictions.},
	number = {1},
	urldate = {2023-01-31},
	journal = {J. Therm. Sci. Eng. Appl.},
	author = {Jaluria, Yogesh},
	month = sep,
	year = {2019},
	file = {Snapshot:/home/hell/Zotero/storage/QHY4D39A/Solution-of-Inverse-Problems-in-Thermal-Systems.html:text/html},
}

@article{bangian-tabrizi_optimization_2018,
	title = {An optimization strategy for the inverse solution of a convection heat transfer problem},
	volume = {124},
	issn = {0017-9310},
	url = {https://www.sciencedirect.com/science/article/pii/S0017931017344617},
	doi = {10.1016/j.ijheatmasstransfer.2018.04.053},
	abstract = {In many practical situations, the desired results are given, but the conditions needed for achieving these are unknown. This circumstance leads to inverse problems, which are of particular interest in thermal processes. For instance, the temperature cycle to which a component must be subjected in order to obtain desired transformations in a manufacturing system are known or prescribed. However, the boundary and initial conditions, in terms of heat input, pressure, flow rate and temperature, are not known and must be determined by solving the inverse problem. A method based on a search and optimization approach is developed to solve the inverse natural convection problem of a two-dimensional heat source on a vertical flat plate. This problem is of interest in fires and electronic systems. The inverse problem involves determination of the strength and location of the heat source, which is taken as a fixed-length region of the wall with an isothermal or isoflux condition, by employing a few selected data points downstream. This is achieved by numerical simulations of the region at differing source strengths and locations, thus obtaining relevant temperature interpolation functions of source location and strength for selected data points. A search based optimization method, particle swarm optimization (PSO), is then applied to find the best pair of vertical locations for input of data. The system of equations based on their respective relations is solved to obtain solution to the inverse problem. The goal of this method is to reduce the uncertainty and approach essentially unique solutions. The error of the method is found to be acceptable for both source strength and location.},
	language = {en},
	urldate = {2023-01-31},
	journal = {Int. J. Heat Mass Transfer},
	author = {Bangian-Tabrizi, Ardeshir and Jaluria, Yogesh},
	month = sep,
	year = {2018},
	pages = {1147--1155},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/7UX5YEPN/S0017931017344617.html:text/html},
}

@article{yaman_survey_2013,
	title = {A {Survey} on {Inverse} {Problems} for {Applied} {Sciences}},
	volume = {2013},
	issn = {1024-123X},
	url = {https://www.hindawi.com/journals/mpe/2013/976837/},
	doi = {10.1155/2013/976837},
	abstract = {The aim of this paper is to introduce inversion-based engineering applications and to investigate some of the important ones from mathematical point of view. To do this we employ acoustic, electromagnetic, and elastic waves for presenting different types of inverse problems. More specifically, we first study location, shape, and boundary parameter reconstruction algorithms for the inaccessible targets in acoustics. The inverse problems for the time-dependent differential equations of isotropic and anisotropic elasticity are reviewed in the following section of the paper. These problems were the objects of the study by many authors in the last several decades. The physical interpretations for almost all of these problems are given, and the geophysical applications for some of them are described. In our last section, an introduction with many links into the literature is given for modern algorithms which combine techniques from classical inverse problems with stochastic tools into ensemble methods both for data assimilation as well as for forecasting.},
	language = {en},
	urldate = {2023-01-31},
	journal = {Math. Probl. Eng.},
	author = {Yaman, Fatih and Yakhno, Valery G. and Potthast, Roland},
	month = jul,
	year = {2013},
	note = {Publisher: Hindawi},
	pages = {e976837},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	language = {en},
	number = {5},
	urldate = {2023-01-31},
	journal = {Neural Netw.},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	keywords = {Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	pages = {359--366},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/AGD9FDIS/0893608089900208.html:text/html},
}

@article{hornik_approximation_1991,
	title = {Approximation capabilities of multilayer feedforward networks},
	volume = {4},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/089360809190009T},
	doi = {10.1016/0893-6080(91)90009-T},
	abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	language = {en},
	number = {2},
	urldate = {2023-01-31},
	journal = {Neural Netw.},
	author = {Hornik, Kurt},
	month = jan,
	year = {1991},
	keywords = {() approximation, Activation function, Input environment measure, Multilayer feedforward networks, Smooth approximation, Sobolev spaces, Uniform approximation, Universal approximation capabilities},
	pages = {251--257},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/NAZ5ZSIL/089360809190009T.html:text/html},
}

@article{yang_artificial_2008,
	title = {Artificial {Neural} {Networks} ({ANNs}): {A} {New} {Paradigm} for {Thermal} {Science} and {Engineering}},
	volume = {130},
	issn = {0022-1481},
	shorttitle = {Artificial {Neural} {Networks} ({ANNs})},
	url = {https://doi.org/10.1115/1.2944238},
	doi = {10.1115/1.2944238},
	abstract = {The use of artificial neural network (ANN), as one of the artificial intelligence methodologies, in a variety of real-world applications has been around for some time. However, the application of ANN to thermal science and engineering is still relatively new, but is receiving ever-increasing attention in recent published literature. Such attention is due essentially to special requirement and needs of the field of thermal science and engineering in terms of its increasing complexity and the recognition that it is not always feasible to deal with many critical problems in this field by the use of traditional analysis. The purpose of the present review is to point out the recent advances in ANN and its successes in dealing with a variety of important thermal problems. Some current ANN shortcomings, the development of recent advances in ANN-based hybrid analysis, and its future prospects will also be indicated.},
	number = {9},
	urldate = {2023-01-31},
	journal = {J. Heat Transfer},
	author = {Yang, Kwang-Tzu},
	month = jul,
	year = {2008},
	file = {Snapshot:/home/hell/Zotero/storage/LXX3X3NH/Artificial-Neural-Networks-ANNs-A-New-Paradigm-for.html:text/html},
}

@inproceedings{guo_convolutional_2016,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {Convolutional {Neural} {Networks} for {Steady} {Flow} {Approximation}},
	isbn = {978-1-4503-4232-2},
	url = {https://doi.org/10.1145/2939672.2939738},
	doi = {10.1145/2939672.2939738},
	abstract = {In aerodynamics related design, analysis and optimization problems, flow fields are simulated using computational fluid dynamics (CFD) solvers. However, CFD simulation is usually a computationally expensive, memory demanding and time consuming iterative process. These drawbacks of CFD limit opportunities for design space exploration and forbid interactive design. We propose a general and flexible approximation model for real-time prediction of non-uniform steady laminar flow in a 2D or 3D domain based on convolutional neural networks (CNNs). We explored alternatives for the geometry representation and the network architecture of CNNs. We show that convolutional neural networks can estimate the velocity field two orders of magnitude faster than a GPU-accelerated CFD solver and four orders of magnitude faster than a CPU-based CFD solver at a cost of a low error rate. This approach can provide immediate feedback for real-time design iterations at the early stage of design. Compared with existing approximation models in the aerodynamics domain, CNNs enable an efficient estimation for the entire velocity field. Furthermore, designers and engineers can directly apply the CNN approximation model in their design space exploration algorithms without training extra lower-dimensional surrogate models.},
	urldate = {2023-01-31},
	booktitle = {Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.},
	publisher = {ACM},
	author = {Guo, Xiaoxiao and Li, Wei and Iorio, Francesco},
	month = aug,
	year = {2016},
	keywords = {machine learning, computational fluid dynamics, convolutional neural networks, surrogate models},
	pages = {481--490},
}

@article{sharma_stiff-pdes_2023,
	title = {Stiff-{PDEs} and {Physics}-{Informed} {Neural} {Networks}},
	issn = {1886-1784},
	url = {https://doi.org/10.1007/s11831-023-09890-4},
	doi = {10.1007/s11831-023-09890-4},
	abstract = {In recent years, physics-informed neural networks (PINN) have been used to solve stiff-PDEs mostly in the 1D and 2D spatial domain. PINNs still experience issues solving 3D problems, especially, problems with conflicting boundary conditions at adjacent edges and corners. These problems have discontinuous solutions at edges and corners that are difficult to learn for neural networks with a continuous activation function. In this review paper, we have investigated various PINN frameworks that are designed to solve stiff-PDEs. We took two heat conduction problems (2D and 3D) with a discontinuous solution at corners as test cases. We investigated these problems with a number of PINN frameworks, discussed and analysed the results against the FEM solution. It appears that PINNs provide a more general platform for parameterisation compared to conventional solvers. Thus, we have investigated the 2D heat conduction problem with parametric conductivity and geometry separately. We also discuss the challenges associated with PINNs and identify areas for further investigation.},
	language = {en},
	urldate = {2023-02-08},
	journal = {Arch. Comput. Methods Eng.},
	author = {Sharma, Prakhar and Evans, Llion and Tindall, Michelle and Nithiarasu, Perumal},
	month = feb,
	year = {2023},
}

@article{xu_transfer_2023,
	title = {Transfer learning based physics-informed neural networks for solving inverse problems in engineering structures under different loading scenarios},
	volume = {405},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782522008088},
	doi = {10.1016/j.cma.2022.115852},
	abstract = {Recently, a class of machine learning methods called physics-informed neural networks (PINNs) has been proposed and gained prevalence in solving various scientific computing problems. This approach enables the solution of partial differential equations (PDEs) via embedding physical laws into the loss function. Many inverse problems can be tackled by simply combining the data from real life scenarios with existing PINN algorithms. In this paper, we present a multi-task learning method using uncertainty weighting to improve the training efficiency and accuracy of PINNs for inverse problems in linear elasticity and hyperelasticity. Furthermore, we demonstrate an application of PINNs to a practical inverse problem in structural analysis: prediction of external loads of diverse engineering structures based on limited displacement monitoring points. To this end, we first determine a simplified loading scenario at the offline stage. By setting unknown boundary conditions as learnable parameters, PINNs can predict the external loads with the support of measured data. When it comes to the online stage in real engineering projects, transfer learning is employed to fine-tune the pre-trained model from offline stage. Our results show that, even with noisy gappy data, satisfactory results can still be obtained from the PINN model due to the dual regularization of physics laws and prior knowledge, which exhibits better robustness compared to traditional analysis methods. Our approach is capable of bridging the gap between various structures with geometric scaling and under different loading scenarios, and the convergence of training is also greatly accelerated through not only the layer freezing but also the multi-task weight inheritance from pre-trained models, thus making it possible to be applied as surrogate models in actual engineering projects.},
	language = {en},
	urldate = {2023-03-07},
	journal = {Comput. Methods Appl. Mech. Eng.},
	author = {Xu, Chen and Cao, Ba Trung and Yuan, Yong and Meschke, Günther},
	month = feb,
	year = {2023},
	keywords = {Transfer learning, Multi-task learning, Inverse analysis, Physics-informed neural networks (PINNs), Tunnel engineering},
	pages = {115852},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/NJEYAITN/S0045782522008088.html:text/html},
}

@article{jones_characterising_2020,
	title = {Characterising the {Digital} {Twin}: {A} systematic literature review},
	volume = {29},
	issn = {1755-5817},
	shorttitle = {Characterising the {Digital} {Twin}},
	url = {https://www.sciencedirect.com/science/article/pii/S1755581720300110},
	doi = {10.1016/j.cirpj.2020.02.002},
	abstract = {While there has been a recent growth of interest in the Digital Twin, a variety of definitions employed across industry and academia remain. There is a need to consolidate research such to maintain a common understanding of the topic and ensure future research efforts are to be based on solid foundations. Through a systematic literature review and a thematic analysis of 92 Digital Twin publications from the last ten years, this paper provides a characterisation of the Digital Twin, identification of gaps in knowledge, and required areas of future research. In characterising the Digital Twin, the state of the concept, key terminology, and associated processes are identified, discussed, and consolidated to produce 13 characteristics (Physical Entity/Twin; Virtual Entity/Twin; Physical Environment; Virtual Environment; State; Realisation; Metrology; Twinning; Twinning Rate; Physical-to-Virtual Connection/Twinning; Virtual-to-Physical Connection/Twinning; Physical Processes; and Virtual Processes) and a complete framework of the Digital Twin and its process of operation. Following this characterisation, seven knowledge gaps and topics for future research focus are identified: Perceived Benefits; Digital Twin across the Product Life-Cycle; Use-Cases; Technical Implementations; Levels of Fidelity; Data Ownership; and Integration between Virtual Entities; each of which are required to realise the Digital Twin.},
	language = {en},
	urldate = {2023-03-13},
	journal = {CIRP J. Manuf. Sci. Technol.},
	author = {Jones, David and Snider, Chris and Nassehi, Aydin and Yon, Jason and Hicks, Ben},
	month = may,
	year = {2020},
	keywords = {Digital Twin, Virtual Twin},
	pages = {36--52},
	file = {ScienceDirect Snapshot:/home/hell/Zotero/storage/K87MEGWP/S1755581720300110.html:text/html},
}

@article{mojtabi_one-dimensional_2015,
	title = {One-dimensional linear advection–diffusion equation: {Analytical} and finite element solutions},
	volume = {107},
	issn = {0045-7930},
	shorttitle = {One-dimensional linear advection–diffusion equation},
	url = {https://www.sciencedirect.com/science/article/pii/S0045793014004289},
	doi = {10.1016/j.compfluid.2014.11.006},
	abstract = {In this paper, a time dependent one-dimensional linear advection–diffusion equation with Dirichlet homogeneous boundary conditions and an initial sine function is solved analytically by separation of variables and numerically by the finite element method. It is observed that when the advection becomes dominant, the analytical solution becomes ill-behaved and harder to evaluate. Therefore another approach is designed where the solution is decomposed in a simple wave solution and a viscous perturbation. It is shown that an exponential layer builds up close to the downstream boundary. Discussion and comparison of both solutions are carried out extensively offering the numericist a new test model for the numerical integration of the Navier–Stokes equation.},
	language = {en},
	urldate = {2023-03-28},
	journal = {Comput. Fluids},
	author = {Mojtabi, Abdelkader and Deville, Michel O.},
	month = jan,
	year = {2015},
	keywords = {Asymptotic development, Exponential layer, Finite element method, Separation of variables, Symbolic computation},
	pages = {189--195},
}

@misc{klambauer_self-normalizing_2017,
	title = {Self-{Normalizing} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.02515},
	doi = {10.48550/arXiv.1706.02515},
	abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
	month = sep,
	year = {2017},
	note = {arXiv:1706.02515 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/U5EMN46W/1706.html:text/html},
}

@misc{hasebrook_why_2022,
	title = {Why {Do} {Machine} {Learning} {Practitioners} {Still} {Use} {Manual} {Tuning}? {A} {Qualitative} {Study}},
	shorttitle = {Why {Do} {Machine} {Learning} {Practitioners} {Still} {Use} {Manual} {Tuning}?},
	url = {http://arxiv.org/abs/2203.01717},
	doi = {10.48550/arXiv.2203.01717},
	abstract = {Current advanced hyperparameter optimization (HPO) methods, such as Bayesian optimization, have high sampling efficiency and facilitate replicability. Nonetheless, machine learning (ML) practitioners (e.g., engineers, scientists) mostly apply less advanced HPO methods, which can increase resource consumption during HPO or lead to underoptimized ML models. Therefore, we suspect that practitioners choose their HPO method to achieve different goals, such as decrease practitioner effort and target audience compliance. To develop HPO methods that align with such goals, the reasons why practitioners decide for specific HPO methods must be unveiled and thoroughly understood. Because qualitative research is most suitable to uncover such reasons and find potential explanations for them, we conducted semi-structured interviews to explain why practitioners choose different HPO methods. The interviews revealed six principal practitioner goals (e.g., increasing model comprehension), and eleven key factors that impact decisions for HPO methods (e.g., available computing resources). We deepen the understanding about why practitioners decide for different HPO methods and outline recommendations for improvements of HPO methods by aligning them with practitioner goals.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Hasebrook, Niklas and Morsbach, Felix and Kannengießer, Niclas and Franke, Jörg and Hutter, Frank and Sunyaev, Ali},
	month = mar,
	year = {2022},
	note = {arXiv:2203.01717 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/J7YRWUMV/2203.html:text/html},
}

@article{hasebrook_why_2022-1,
	title = {Why {Do} {Machine} {Learning} {Practitioners} {Still} {Use} {Manual} {Tuning}? {A} {Qualitative} {Study}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Why {Do} {Machine} {Learning} {Practitioners} {Still} {Use} {Manual} {Tuning}?},
	url = {https://arxiv.org/abs/2203.01717},
	doi = {10.48550/ARXIV.2203.01717},
	abstract = {Current advanced hyperparameter optimization (HPO) methods, such as Bayesian optimization, have high sampling efficiency and facilitate replicability. Nonetheless, machine learning (ML) practitioners (e.g., engineers, scientists) mostly apply less advanced HPO methods, which can increase resource consumption during HPO or lead to underoptimized ML models. Therefore, we suspect that practitioners choose their HPO method to achieve different goals, such as decrease practitioner effort and target audience compliance. To develop HPO methods that align with such goals, the reasons why practitioners decide for specific HPO methods must be unveiled and thoroughly understood. Because qualitative research is most suitable to uncover such reasons and find potential explanations for them, we conducted semi-structured interviews to explain why practitioners choose different HPO methods. The interviews revealed six principal practitioner goals (e.g., increasing model comprehension), and eleven key factors that impact decisions for HPO methods (e.g., available computing resources). We deepen the understanding about why practitioners decide for different HPO methods and outline recommendations for improvements of HPO methods by aligning them with practitioner goals.},
	urldate = {2023-05-02},
	author = {Hasebrook, Niklas and Morsbach, Felix and Kannengießer, Niclas and Franke, Jörg and Hutter, Frank and Sunyaev, Ali},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@inproceedings{bertoin_numerical_2021,
	title = {Numerical influence of {ReLU}’(0) on backpropagation},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/043ab21fc5a1607b381ac3896176dac6-Paper.pdf},
	booktitle = {Adv. Neural Inf. Process. Syst.},
	publisher = {Curran Assoc., Inc.},
	author = {Bertoin, David and Bolte, Jérôme and Gerchinovitz, Sébastien and Pauwels, Edouard},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {468--479},
}

@article{nishikawa_hyperbolic_2018,
	title = {On hyperbolic method for diffusion with discontinuous coefficients},
	volume = {367},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999118302444},
	doi = {10.1016/j.jcp.2018.04.027},
	language = {en},
	urldate = {2023-07-25},
	journal = {J. Comput. Phys.},
	author = {Nishikawa, Hiroaki},
	month = aug,
	year = {2018},
	pages = {102--108},
}

@article{shashkov_solving_1996,
	title = {Solving {Diffusion} {Equations} with {Rough} {Coefficients} in {Rough} {Grids}},
	volume = {129},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999196902570},
	doi = {10.1006/jcph.1996.0257},
	language = {en},
	number = {2},
	urldate = {2023-07-25},
	journal = {J. Comput. Phys.},
	author = {Shashkov, Mikhail and Steinberg, Stanly},
	month = dec,
	year = {1996},
	pages = {383--405},
}

@article{shin_convergence_2020,
	title = {On the {Convergence} of {Physics} {Informed} {Neural} {Networks} for {Linear} {Second}-{Order} {Elliptic} and {Parabolic} {Type} {PDEs}},
	volume = {28},
	issn = {1815-2406, 1991-7120},
	url = {http://global-sci.org/intro/article_detail/cicp/18404.html},
	doi = {10.4208/cicp.OA-2020-0193},
	number = {5},
	urldate = {2023-07-25},
	journal = {Comm. Comput. Phys.},
	author = {Shin, Yeonjong},
	month = jun,
	year = {2020},
	pages = {2042--2074},
}

@article{pantidis_error_2023,
	title = {Error convergence and engineering-guided hyperparameter search of {PINNs}: {Towards} optimized {I}-{FENN} performance},
	volume = {414},
	issn = {00457825},
	shorttitle = {Error convergence and engineering-guided hyperparameter search of {PINNs}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782523002840},
	doi = {10.1016/j.cma.2023.116160},
	language = {en},
	urldate = {2023-07-25},
	journal = {Comput. Methods Appl. Mech. Eng.},
	author = {Pantidis, Panos and Eldababy, Habiba and Tagle, Christopher Miguel and Mobasher, Mostafa E.},
	month = sep,
	year = {2023},
	pages = {116160},
}

@article{wang_auto-pinn_2022,
	title = {Auto-{PINN}: {Understanding} and {Optimizing} {Physics}-{Informed} {Neural} {Architecture}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Auto-{PINN}},
	url = {https://arxiv.org/abs/2205.13748},
	doi = {10.48550/ARXIV.2205.13748},
	abstract = {Physics-informed neural networks (PINNs) are revolutionizing science and engineering practice by bringing together the power of deep learning to bear on scientific computation. In forward modeling problems, PINNs are meshless partial differential equation (PDE) solvers that can handle irregular, high-dimensional physical domains. Naturally, the neural architecture hyperparameters have a large impact on the efficiency and accuracy of the PINN solver. However, this remains an open and challenging problem because of the large search space and the difficulty of identifying a proper search objective for PDEs. Here, we propose Auto-PINN, the first systematic, automated hyperparameter optimization approach for PINNs, which employs Neural Architecture Search (NAS) techniques to PINN design. Auto-PINN avoids manually or exhaustively searching the hyperparameter space associated with PINNs. A comprehensive set of pre-experiments using standard PDE benchmarks allows us to probe the structure-performance relationship in PINNs. We find that the different hyperparameters can be decoupled, and that the training loss function of PINNs is a good search objective. Comparison experiments with baseline methods demonstrate that Auto-PINN produces neural architectures with superior stability and accuracy over alternative baselines.},
	urldate = {2023-07-25},
	author = {Wang, Yicheng and Han, Xiaotian and Chang, Chia-Yuan and Zha, Daochen and Braga-Neto, Ulisses and Hu, Xia},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{markidis_old_2021,
	title = {The {Old} and the {New}: {Can} {Physics}-{Informed} {Deep}-{Learning} {Replace} {Traditional} {Linear} {Solvers}?},
	volume = {4},
	issn = {2624-909X},
	shorttitle = {The {Old} and the {New}},
	url = {https://www.frontiersin.org/articles/10.3389/fdata.2021.669097/full},
	doi = {10.3389/fdata.2021.669097},
	abstract = {Physics-Informed Neural Networks (PINN) are neural networks encoding the problem governing equations, such as Partial Differential Equations (PDE), as a part of the neural network. PINNs have emerged as a new essential tool to solve various challenging problems, including computing linear systems arising from PDEs, a task for which several traditional methods exist. In this work, we focus first on evaluating the potential of PINNs as linear solvers in the case of the Poisson equation, an omnipresent equation in scientific computing. We characterize PINN linear solvers in terms of accuracy and performance under different network configurations (depth, activation functions, input data set distribution). We highlight the critical role of transfer learning. Our results show that low-frequency components of the solution converge quickly as an effect of the F-principle. In contrast, an accurate solution of the high frequencies requires an exceedingly long time. To address this limitation, we propose integrating PINNs into traditional linear solvers. We show that this integration leads to the development of new solvers whose performance is on par with other high-performance solvers, such as PETSc conjugate gradient linear solvers, in terms of performance and accuracy. Overall, while the accuracy and computational performance are still a limiting factor for the direct use of PINN linear solvers, hybrid strategies combining old traditional linear solver approaches with new emerging deep-learning techniques are among the most promising methods for developing a new class of linear solvers.},
	urldate = {2023-07-25},
	journal = {Front. Big Data},
	author = {Markidis, Stefano},
	month = nov,
	year = {2021},
	pages = {669097},
}

@article{escapil-inchauspe_hyper-parameter_2022,
	title = {Hyper-parameter tuning of physics-informed neural networks: {Application} to {Helmholtz} problems},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Hyper-parameter tuning of physics-informed neural networks},
	url = {https://arxiv.org/abs/2205.06704},
	doi = {10.48550/ARXIV.2205.06704},
	abstract = {We consider physics-informed neural networks (PINNs) [Raissi et al., J.{\textasciitilde}Comput. Phys. 278 (2019) 686-707] for forward physical problems. In order to find optimal PINNs configuration, we introduce a hyper-parameter optimization (HPO) procedure via Gaussian processes-based Bayesian optimization. We apply the HPO to Helmholtz equation for bounded domains and conduct a thorough study, focusing on: (i) performance, (ii) the collocation points density \$r\$ and (iii) the frequency \$κ\$, confirming the applicability and necessity of the method. Numerical experiments are performed in two and three dimensions, including comparison to finite element methods.},
	urldate = {2023-07-25},
	author = {Escapil-Inchauspé, Paul and Ruz, Gonzalo A.},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Artificial Intelligence (cs.AI), FOS: Mathematics, Numerical Analysis (math.NA)},
}

@article{cho_lstm-pinn_2022,
	title = {An {LSTM}-{PINN} {Hybrid} {Method} to {Estimate} {Lithium}-{Ion} {Battery} {Pack} {Temperature}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9895422/},
	doi = {10.1109/ACCESS.2022.3208103},
	urldate = {2023-07-25},
	journal = {IEEE Access},
	author = {Cho, Gyouho and Zhu, Di and Campbell, Jeffrey Joseph and Wang, Mengqi},
	year = {2022},
	pages = {100594--100604},
}

@article{wen_dynamics_2022,
	title = {Dynamics of diverse data-driven solitons for the three-component coupled nonlinear {Schrödinger} model by the {MPS}-{PINN} method},
	volume = {109},
	issn = {0924-090X, 1573-269X},
	url = {https://link.springer.com/10.1007/s11071-022-07583-4},
	doi = {10.1007/s11071-022-07583-4},
	language = {en},
	number = {4},
	urldate = {2023-07-25},
	journal = {Nonlinear Dyn.},
	author = {Wen, Xue-Kun and Wu, Gang-Zhou and Liu, Wei and Dai, Chao-Qing},
	month = sep,
	year = {2022},
	pages = {3041--3050},
}

@article{wei_indoor_2023,
	title = {Indoor airflow field reconstruction using physics-informed neural network},
	volume = {242},
	issn = {03601323},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0360132323005905},
	doi = {10.1016/j.buildenv.2023.110563},
	language = {en},
	urldate = {2023-07-25},
	journal = {Build. Environ.},
	author = {Wei, Chenghao and Ooka, Ryozo},
	month = aug,
	year = {2023},
	pages = {110563},
}

@misc{noauthor_regularizing_nodate,
	title = {Regularizing deep networks with prior knowledge: {A} constraint-based approach - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121002525},
	urldate = {2023-08-09},
	file = {Regularizing deep networks with prior knowledge\: A constraint-based approach - ScienceDirect:/home/hell/Zotero/storage/FS6HRXMN/S0950705121002525.html:text/html},
}

@article{roychowdhury_regularizing_2021,
	title = {Regularizing deep networks with prior knowledge: {A} constraint-based approach},
	volume = {222},
	issn = {09507051},
	shorttitle = {Regularizing deep networks with prior knowledge},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705121002525},
	doi = {10.1016/j.knosys.2021.106989},
	language = {en},
	urldate = {2023-08-09},
	journal = {Knowl. Based Syst.},
	author = {Roychowdhury, Soumali and Diligenti, Michelangelo and Gori, Marco},
	month = jun,
	year = {2021},
	pages = {106989},
}

@misc{kervadec_constrained_2022,
	title = {Constrained {Deep} {Networks}: {Lagrangian} {Optimization} via {Log}-{Barrier} {Extensions}},
	shorttitle = {Constrained {Deep} {Networks}},
	url = {http://arxiv.org/abs/1904.04205},
	abstract = {This study investigates imposing hard inequality constraints on the outputs of convolutional neural networks (CNN) during training. Several recent works showed that the theoretical and practical advantages of Lagrangian optimization over simple penalties do not materialize in practice when dealing with modern CNNs involving millions of parameters. Therefore, constrained CNNs are typically handled with penalties. We propose *log-barrier extensions*, which approximate Lagrangian optimization of constrained-CNN problems with a sequence of unconstrained losses. Unlike standard interior-point and log-barrier methods, our formulation does not need an initial feasible solution. The proposed extension yields an upper bound on the duality gap -- generalizing the result of standard log-barriers -- and yielding sub-optimality certificates for feasible solutions. While sub-optimality is not guaranteed for non-convex problems, this result shows that log-barrier extensions are a principled way to approximate Lagrangian optimization for constrained CNNs via implicit dual variables. We report weakly supervised image segmentation experiments, with various constraints, showing that our formulation outperforms substantially the existing constrained-CNN methods, in terms of accuracy, constraint satisfaction and training stability, more so when dealing with a large number of constraints.},
	urldate = {2023-08-09},
	publisher = {arXiv},
	author = {Kervadec, Hoel and Dolz, Jose and Yuan, Jing and Desrosiers, Christian and Granger, Eric and Ayed, Ismail Ben},
	month = mar,
	year = {2022},
	note = {arXiv:1904.04205 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/hell/Zotero/storage/Q84W9QAX/1904.html:text/html},
}

@inproceedings{gnecco_learning_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning with {Hard} {Constraints}},
	isbn = {978-3-642-40728-4},
	doi = {10.1007/978-3-642-40728-4_19},
	abstract = {A learning paradigm is proposed, in which one has both classical supervised examples and constraints that cannot be violated, called here “hard constraints”, such as those enforcing the probabilistic normalization of a density function or imposing coherent decisions of the classifiers acting on different views of the same pattern. In contrast, supervised examples can be violated at the cost of some penalization (quantified by the choice of a suitable loss function) and so play the roles of “soft constraints”. Constrained variational calculus is exploited to derive a representation theorem which provides a description of the “optimal body of the agent”, i.e. the functional structure of the solution to the proposed learning problem. It is shown that the solution can be represented in terms of a set of “support constraints”, thus extending the well-known notion of “support vectors”.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2013},
	publisher = {springer},
	author = {Gnecco, Giorgio and Gori, Marco and Melacci, Stefano and Sanguineti, Marcello},
	editor = {Mladenov, Valeri and Koprinkova-Hristova, Petia and Palm, Günther and Villa, Alessandro E. P. and Appollini, Bruno and Kasabov, Nikola},
	year = {2013},
	keywords = {constrained variational calculus, Learning from constraints, learning with prior knowledge, multi-task learning, support constraints},
	pages = {146--153},
}

@article{lu_physics-informed_2021-1,
	title = {Physics-{Informed} {Neural} {Networks} with {Hard} {Constraints} for {Inverse} {Design}},
	volume = {43},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/21M1397908},
	doi = {10.1137/21M1397908},
	language = {en},
	number = {6},
	urldate = {2023-08-09},
	journal = {SIAM J. Sci. Comput.},
	author = {Lu, Lu and Pestourie, Raphaël and Yao, Wenjie and Wang, Zhicheng and Verdugo, Francesc and Johnson, Steven G.},
	month = jan,
	year = {2021},
	pages = {B1105--B1132},
}

@article{chuang_experience_2022,
	title = {Experience report of physics-informed neural networks in fluid simulations: pitfalls and frustration},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Experience report of physics-informed neural networks in fluid simulations},
	url = {https://arxiv.org/abs/2205.14249},
	doi = {10.48550/ARXIV.2205.14249},
	abstract = {Though PINNs (physics-informed neural networks) are now deemed as a complement to traditional CFD (computational fluid dynamics) solvers rather than a replacement, their ability to solve the Navier-Stokes equations without given data is still of great interest. This report presents our not-so-successful experiments of solving the Navier-Stokes equations with PINN as a replacement for traditional solvers. We aim to, with our experiments, prepare readers for the challenges they may face if they are interested in data-free PINN. In this work, we used two standard flow problems: 2D Taylor-Green vortex at Re=100 and 2D cylinder flow at Re=200. The PINN method solved the 2D Taylor-Green vortex problem with acceptable results, and we used this flow as an accuracy and performance benchmark. About 32 hours of training were required for the PINN method's accuracy to match the accuracy of a 16x16 finite-difference simulation, which took less than 20 seconds. The 2D cylinder flow, on the other hand, did not produce a physical solution. The PINN method behaved like a steady-flow solver and did not capture the vortex shedding phenomenon. By sharing our experience, we would like to emphasize that the PINN method is still a work-in-progress, especially in terms of solving flow problems without any given data. More work is needed to make PINN feasible for real-world problems in such applications.},
	urldate = {2023-08-30},
	author = {Chuang, Pi-Yueh and Barba, Lorena A.},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Fluid Dynamics (physics.flu-dyn), FOS: Physical sciences},
}
